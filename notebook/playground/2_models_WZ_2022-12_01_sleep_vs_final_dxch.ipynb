{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f5d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import export_text\n",
    "import mglearn\n",
    "from dashboard_one import *\n",
    "from dash_model_two import *\n",
    "from feature_selection import *\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b74b94",
   "metadata": {},
   "source": [
    "### sleep_____VS_____final diagnosischanges of each patient in each phase\n",
    "\n",
    "\n",
    "#### sleep_brain_finaldxch.csv\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef6c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_col = ['Phase', 'RID', 'VISCODE','PTID','RID_Phase']\n",
    "target = 'final_dxch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769bd041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Phase</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>RID_Phase</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>...</th>\n",
       "      <th>ratio_PTAU_bl</th>\n",
       "      <th>Ventricles_reduction_per_year</th>\n",
       "      <th>Hippocampus_reduction_per_year</th>\n",
       "      <th>wholebrain_reduction_per_year</th>\n",
       "      <th>Entorhinal_reduction_per_year</th>\n",
       "      <th>Fusiform_reduction_per_year</th>\n",
       "      <th>ICV_reduction_per_year</th>\n",
       "      <th>ABETA_reduction_per_year</th>\n",
       "      <th>TAU_reduction_per_year</th>\n",
       "      <th>PTAU_reduction_per_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>v06</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>2_ADNI2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>v11</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>2_ADNI2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>v21</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>2_ADNI2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>v41</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>2_ADNI2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>v51</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>2_ADNI2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14740</th>\n",
       "      <td>6978</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>y1</td>\n",
       "      <td>021_S_6978</td>\n",
       "      <td>6978_ADNI3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14741</th>\n",
       "      <td>6999</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>m12</td>\n",
       "      <td>035_S_6999</td>\n",
       "      <td>6999_ADNI3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14742</th>\n",
       "      <td>6999</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>y1</td>\n",
       "      <td>035_S_6999</td>\n",
       "      <td>6999_ADNI3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14743</th>\n",
       "      <td>7000</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>m12</td>\n",
       "      <td>035_S_7000</td>\n",
       "      <td>7000_ADNI3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14744</th>\n",
       "      <td>7000</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>y1</td>\n",
       "      <td>035_S_7000</td>\n",
       "      <td>7000_ADNI3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14745 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RID  Phase VISCODE        PTID   RID_Phase  NPIK1  NPIK2  NPIK3  \\\n",
       "0         2  ADNI2     v06  011_S_0002     2_ADNI2    NaN    NaN    NaN   \n",
       "1         2  ADNI2     v11  011_S_0002     2_ADNI2    NaN    NaN    NaN   \n",
       "2         2  ADNI2     v21  011_S_0002     2_ADNI2    NaN    NaN    NaN   \n",
       "3         2  ADNI2     v41  011_S_0002     2_ADNI2    NaN    NaN    NaN   \n",
       "4         2  ADNI2     v51  011_S_0002     2_ADNI2    NaN    NaN    NaN   \n",
       "...     ...    ...     ...         ...         ...    ...    ...    ...   \n",
       "14740  6978  ADNI3      y1  021_S_6978  6978_ADNI3    NaN    NaN    NaN   \n",
       "14741  6999  ADNI3     m12  035_S_6999  6999_ADNI3    NaN    NaN    NaN   \n",
       "14742  6999  ADNI3      y1  035_S_6999  6999_ADNI3    NaN    NaN    NaN   \n",
       "14743  7000  ADNI3     m12  035_S_7000  7000_ADNI3    NaN    NaN    NaN   \n",
       "14744  7000  ADNI3      y1  035_S_7000  7000_ADNI3    NaN    NaN    NaN   \n",
       "\n",
       "       NPIK4  NPIK5  ...  ratio_PTAU_bl  Ventricles_reduction_per_year  \\\n",
       "0        NaN    NaN  ...            NaN                            NaN   \n",
       "1        NaN    NaN  ...            NaN                            NaN   \n",
       "2        NaN    NaN  ...            NaN                            NaN   \n",
       "3        NaN    NaN  ...            NaN                            NaN   \n",
       "4        NaN    NaN  ...            NaN                            NaN   \n",
       "...      ...    ...  ...            ...                            ...   \n",
       "14740    NaN    NaN  ...            NaN                            NaN   \n",
       "14741    NaN    NaN  ...            NaN                            NaN   \n",
       "14742    NaN    NaN  ...            NaN                            NaN   \n",
       "14743    NaN    NaN  ...            NaN                            NaN   \n",
       "14744    NaN    NaN  ...            NaN                            NaN   \n",
       "\n",
       "       Hippocampus_reduction_per_year  wholebrain_reduction_per_year  \\\n",
       "0                                 NaN                            NaN   \n",
       "1                                 NaN                            NaN   \n",
       "2                                 NaN                            NaN   \n",
       "3                                 NaN                            NaN   \n",
       "4                                 NaN                            NaN   \n",
       "...                               ...                            ...   \n",
       "14740                             NaN                            NaN   \n",
       "14741                             NaN                            NaN   \n",
       "14742                             NaN                            NaN   \n",
       "14743                             NaN                            NaN   \n",
       "14744                             NaN                            NaN   \n",
       "\n",
       "       Entorhinal_reduction_per_year  Fusiform_reduction_per_year  \\\n",
       "0                                NaN                          NaN   \n",
       "1                                NaN                          NaN   \n",
       "2                                NaN                          NaN   \n",
       "3                                NaN                          NaN   \n",
       "4                                NaN                          NaN   \n",
       "...                              ...                          ...   \n",
       "14740                            NaN                          NaN   \n",
       "14741                            NaN                          NaN   \n",
       "14742                            NaN                          NaN   \n",
       "14743                            NaN                          NaN   \n",
       "14744                            NaN                          NaN   \n",
       "\n",
       "       ICV_reduction_per_year  ABETA_reduction_per_year  \\\n",
       "0                         NaN                       NaN   \n",
       "1                         NaN                       NaN   \n",
       "2                         NaN                       NaN   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "...                       ...                       ...   \n",
       "14740                     NaN                       NaN   \n",
       "14741                     NaN                       NaN   \n",
       "14742                     NaN                       NaN   \n",
       "14743                     NaN                       NaN   \n",
       "14744                     NaN                       NaN   \n",
       "\n",
       "       TAU_reduction_per_year PTAU_reduction_per_year  \n",
       "0                         NaN                     NaN  \n",
       "1                         NaN                     NaN  \n",
       "2                         NaN                     NaN  \n",
       "3                         NaN                     NaN  \n",
       "4                         NaN                     NaN  \n",
       "...                       ...                     ...  \n",
       "14740                     NaN                     NaN  \n",
       "14741                     NaN                     NaN  \n",
       "14742                     NaN                     NaN  \n",
       "14743                     NaN                     NaN  \n",
       "14744                     NaN                     NaN  \n",
       "\n",
       "[14745 rows x 39 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_brain_finaldxch = pd.read_csv('sleep_brain_finaldxch.csv').iloc[:,1:].drop(['NPIKSEV'],axis=1)\n",
    "sleep_brain_finaldxch = sleep_brain_finaldxch[sleep_brain_finaldxch['final_dxch'].notna()].reset_index().drop(['index'],axis=1)   # keep the rows where DXCHANGE is not nan\n",
    "sleep_brain_finaldxch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ce40a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14745 entries, 0 to 14744\n",
      "Data columns (total 39 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   RID                             14745 non-null  int64  \n",
      " 1   Phase                           14745 non-null  object \n",
      " 2   VISCODE                         13649 non-null  object \n",
      " 3   PTID                            14745 non-null  object \n",
      " 4   RID_Phase                       14745 non-null  object \n",
      " 5   NPIK1                           884 non-null    float64\n",
      " 6   NPIK2                           883 non-null    float64\n",
      " 7   NPIK3                           882 non-null    float64\n",
      " 8   NPIK4                           882 non-null    float64\n",
      " 9   NPIK5                           882 non-null    float64\n",
      " 10  NPIK6                           883 non-null    float64\n",
      " 11  NPIK7                           883 non-null    float64\n",
      " 12  NPIK8                           882 non-null    float64\n",
      " 13  NPIK9A                          885 non-null    float64\n",
      " 14  NPIK9B                          884 non-null    float64\n",
      " 15  NPIK9C                          885 non-null    float64\n",
      " 16  NPIKTOT                         884 non-null    float64\n",
      " 17  insomnia                        9018 non-null   float64\n",
      " 18  OSA                             14745 non-null  float64\n",
      " 19  final_dxch                      14745 non-null  object \n",
      " 20  DXCHANGE                        14745 non-null  object \n",
      " 21  ratio_Ventricles_bl             5663 non-null   float64\n",
      " 22  ratio_Hippocampus_bl            4912 non-null   float64\n",
      " 23  ratio_WholeBrain_bl             5895 non-null   float64\n",
      " 24  ratio_Entorhinal_bl             4628 non-null   float64\n",
      " 25  ratio_Fusiform_bl               4628 non-null   float64\n",
      " 26  ratio_ICV_bl                    6212 non-null   float64\n",
      " 27  ratio_ABETA_bl                  838 non-null    float64\n",
      " 28  ratio_TAU_bl                    1029 non-null   float64\n",
      " 29  ratio_PTAU_bl                   717 non-null    float64\n",
      " 30  Ventricles_reduction_per_year   5663 non-null   float64\n",
      " 31  Hippocampus_reduction_per_year  4912 non-null   float64\n",
      " 32  wholebrain_reduction_per_year   5895 non-null   float64\n",
      " 33  Entorhinal_reduction_per_year   4628 non-null   float64\n",
      " 34  Fusiform_reduction_per_year     4628 non-null   float64\n",
      " 35  ICV_reduction_per_year          6212 non-null   float64\n",
      " 36  ABETA_reduction_per_year        838 non-null    float64\n",
      " 37  TAU_reduction_per_year          1029 non-null   float64\n",
      " 38  PTAU_reduction_per_year         717 non-null    float64\n",
      "dtypes: float64(32), int64(1), object(6)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "sleep_brain_finaldxch.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81696451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RID', 'Phase', 'VISCODE', 'PTID', 'RID_Phase', 'NPIK1', 'NPIK2',\n",
       "       'NPIK3', 'NPIK4', 'NPIK5', 'NPIK6', 'NPIK7', 'NPIK8', 'NPIK9A',\n",
       "       'NPIK9B', 'NPIK9C', 'NPIKTOT', 'insomnia', 'OSA', 'final_dxch',\n",
       "       'DXCHANGE', 'ratio_Ventricles_bl', 'ratio_Hippocampus_bl',\n",
       "       'ratio_WholeBrain_bl', 'ratio_Entorhinal_bl', 'ratio_Fusiform_bl',\n",
       "       'ratio_ICV_bl', 'ratio_ABETA_bl', 'ratio_TAU_bl', 'ratio_PTAU_bl',\n",
       "       'Ventricles_reduction_per_year', 'Hippocampus_reduction_per_year',\n",
       "       'wholebrain_reduction_per_year', 'Entorhinal_reduction_per_year',\n",
       "       'Fusiform_reduction_per_year', 'ICV_reduction_per_year',\n",
       "       'ABETA_reduction_per_year', 'TAU_reduction_per_year',\n",
       "       'PTAU_reduction_per_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_brain_finaldxch.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b7f3e",
   "metadata": {},
   "source": [
    "### brain_biomarker______VS______final_dxch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b829bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col_lst = [ 'final_dxch','NPIK1', 'NPIK2',\n",
    "       'NPIK3', 'NPIK4', 'NPIK5', 'NPIK6', 'NPIK7', 'NPIK8', 'NPIK9A',\n",
    "       'NPIK9B', 'NPIK9C', 'NPIKTOT', 'insomnia', 'OSA']\n",
    "bio_lst = [ 'final_dxch','ratio_ABETA_bl', 'ratio_TAU_bl','ratio_PTAU_bl']\n",
    "sleep_dxch = sleep_brain_finaldxch[com_col + col_lst].set_index(com_col).dropna(how='any',axis=0).reset_index()\n",
    "#biomarkers to dxch\n",
    "bio_dxch = sleep_brain_finaldxch[com_col + bio_lst].set_index(com_col).dropna(how='any',axis=0).reset_index()\n",
    "df = sleep_dxch.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75224ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phase         0\n",
       "RID           0\n",
       "VISCODE       0\n",
       "PTID          0\n",
       "RID_Phase     0\n",
       "final_dxch    0\n",
       "NPIK1         0\n",
       "NPIK2         0\n",
       "NPIK3         0\n",
       "NPIK4         0\n",
       "NPIK5         0\n",
       "NPIK6         0\n",
       "NPIK7         0\n",
       "NPIK8         0\n",
       "NPIK9A        0\n",
       "NPIK9B        0\n",
       "NPIK9C        0\n",
       "NPIKTOT       0\n",
       "insomnia      0\n",
       "OSA           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.isna())   # check nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af84a781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase</th>\n",
       "      <th>RID</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>RID_Phase</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>NPIK6</th>\n",
       "      <th>NPIK7</th>\n",
       "      <th>NPIK8</th>\n",
       "      <th>NPIK9A</th>\n",
       "      <th>NPIK9B</th>\n",
       "      <th>NPIK9C</th>\n",
       "      <th>NPIKTOT</th>\n",
       "      <th>insomnia</th>\n",
       "      <th>OSA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final_dxch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AD-AD</th>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AD-MCI</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-AD</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-CN</th>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-MCI</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-AD</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-CN</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-MCI</th>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Phase  RID  VISCODE  PTID  RID_Phase  NPIK1  NPIK2  NPIK3  NPIK4  \\\n",
       "final_dxch                                                                     \n",
       "AD-AD         185  185      185   185        185    185    185    185    185   \n",
       "AD-MCI          6    6        6     6          6      6      6      6      6   \n",
       "CN-AD           2    2        2     2          2      2      2      2      2   \n",
       "CN-CN         186  186      186   186        186    186    186    186    186   \n",
       "CN-MCI         31   31       31    31         31     31     31     31     31   \n",
       "MCI-AD         96   96       96    96         96     96     96     96     96   \n",
       "MCI-CN         32   32       32    32         32     32     32     32     32   \n",
       "MCI-MCI       339  339      339   339        339    339    339    339    339   \n",
       "\n",
       "            NPIK5  NPIK6  NPIK7  NPIK8  NPIK9A  NPIK9B  NPIK9C  NPIKTOT  \\\n",
       "final_dxch                                                                \n",
       "AD-AD         185    185    185    185     185     185     185      185   \n",
       "AD-MCI          6      6      6      6       6       6       6        6   \n",
       "CN-AD           2      2      2      2       2       2       2        2   \n",
       "CN-CN         186    186    186    186     186     186     186      186   \n",
       "CN-MCI         31     31     31     31      31      31      31       31   \n",
       "MCI-AD         96     96     96     96      96      96      96       96   \n",
       "MCI-CN         32     32     32     32      32      32      32       32   \n",
       "MCI-MCI       339    339    339    339     339     339     339      339   \n",
       "\n",
       "            insomnia  OSA  \n",
       "final_dxch                 \n",
       "AD-AD            185  185  \n",
       "AD-MCI             6    6  \n",
       "CN-AD              2    2  \n",
       "CN-CN            186  186  \n",
       "CN-MCI            31   31  \n",
       "MCI-AD            96   96  \n",
       "MCI-CN            32   32  \n",
       "MCI-MCI          339  339  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(target).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae317f9",
   "metadata": {},
   "source": [
    "- unselect the labels with too little data: AD-MCI, CN-AD, CN-MCI, MCI-CN\n",
    "- the possible groups: ['CN-CN','MCI-AD', 'MCI-MCI','AD-AD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9a4caaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2g = df.loc[df[target].isin(['MCI-AD', 'MCI-CN'])].reset_index().drop(['index'],axis=1)\n",
    "df_2g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7432df5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3g = df.loc[df[target].isin(['MCI-AD', 'MCI-CN','MCI-MCI'])].reset_index().drop(['index'],axis=1)\n",
    "df_3g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39783bf",
   "metadata": {},
   "source": [
    "### undersampling and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd62ca",
   "metadata": {},
   "source": [
    "- functions\n",
    "    - models(df,drop_lst,target) : under sampling, split, scale, pca, models\n",
    "    - cv_models(df,drop_lst,target,k): under sampling, NOT SPLIT, scale, pca, models with cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c356c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_lst = ['Phase', 'RID', 'VISCODE', 'PTID','RID_Phase',target]\n",
    "k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916eb3f2",
   "metadata": {},
   "source": [
    "# - 'MCI-AD': 32, 'MCI-CN': 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895d717",
   "metadata": {},
   "source": [
    "- 'MCI-AD': 32, 'MCI-CN': 32}\n",
    "- #original dataset: random forest 55trees. average weighted f1-score of 10-cross validation:0.645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcc70e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 64 ; Resampled dataset shape Counter({'MCI-AD': 32, 'MCI-CN': 32})\n",
      "\n",
      "8 principle components are needed to explain 90% of the data\n",
      "\n",
      "Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.668, Test set f1-score: 0.544\n",
      "          - saga_L1, Training set f1-score:0.675, Test set f1-score: 0.632\n",
      "          - newton-cg_L2, Training set f1-score:0.668, Test set f1-score: 0.544\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.775, Test set f1-score: 0.538\n",
      "          - saga_L1, Training set f1-score:0.658, Test set f1-score: 0.700\n",
      "          - newton-cg_L2, Training set f1-score:0.775, Test set f1-score: 0.538\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.826, Test set f1-score: 0.620\n",
      "          - saga_L1, Training set f1-score:0.678, Test set f1-score: 0.544\n",
      "          - newton-cg_L2, Training set f1-score:0.826, Test set f1-score: 0.620\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.826, Test set f1-score: 0.620\n",
      "          - saga_L1, Training set f1-score:0.826, Test set f1-score: 0.620\n",
      "          - newton-cg_L2, Training set f1-score:0.826, Test set f1-score: 0.620\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.844, Test set f1-score: 0.620\n",
      "          - saga_L1, Training set f1-score:0.824, Test set f1-score: 0.704\n",
      "          - newton-cg_L2, Training set f1-score:0.844, Test set f1-score: 0.620\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.804, Test set f1-score: 0.646\n",
      "          - saga_L1, Training set f1-score:0.804, Test set f1-score: 0.646\n",
      "          - newton-cg_L2, Training set f1-score:0.804, Test set f1-score: 0.646\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.824, Test set f1-score: 0.704\n",
      "          - saga_L1, Training set f1-score:0.804, Test set f1-score: 0.646\n",
      "          - newton-cg_L2, Training set f1-score:0.824, Test set f1-score: 0.704\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.672 f1-score on test data: 0.704\n",
      "          - tree depth: 2.000. f1-score on training data: 0.751 f1-score on test data: 0.620\n",
      "          - tree depth: 3.000. f1-score on training data: 0.807 f1-score on test data: 0.692\n",
      "          - tree depth: 4.000. f1-score on training data: 0.864 f1-score on test data: 0.778\n",
      "          - tree depth: 5.000. f1-score on training data: 0.883 f1-score on test data: 0.778\n",
      "          - tree depth: 6.000. f1-score on training data: 0.922 f1-score on test data: 0.692\n",
      "          - tree depth: 7.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - tree depth: 8.000. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - tree depth: 9.000. f1-score on training data: 0.941 f1-score on test data: 0.769\n",
      "          - tree depth: 10.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - tree depth: 11.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - tree depth: 12.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - tree depth: 13.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - tree depth: 14.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.863 f1-score on test data: 0.769\n",
      "          - 10trees. f1-score on training data: 0.902 f1-score on test data: 0.692\n",
      "          - 15trees. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - 20trees. f1-score on training data: 0.941 f1-score on test data: 0.704\n",
      "          - 25trees. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - 30trees. f1-score on training data: 0.941 f1-score on test data: 0.646\n",
      "          - 35trees. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - 40trees. f1-score on training data: 0.941 f1-score on test data: 0.704\n",
      "          - 45trees. f1-score on training data: 0.941 f1-score on test data: 0.704\n",
      "          - 50trees. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - 55trees. f1-score on training data: 0.941 f1-score on test data: 0.704\n",
      "          - 60trees. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - 65trees. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - 70trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 75trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 80trees. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - 85trees. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - 90trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 95trees. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.941 f1-score on test data: 0.696\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.701, Test set f1-score: 0.481\n",
      "          - saga_L1, Training set f1-score:0.675, Test set f1-score: 0.632\n",
      "          - newton-cg_L2, Training set f1-score:0.701, Test set f1-score: 0.481\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.771, Test set f1-score: 0.538\n",
      "          - saga_L1, Training set f1-score:0.675, Test set f1-score: 0.632\n",
      "          - newton-cg_L2, Training set f1-score:0.771, Test set f1-score: 0.538\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.844, Test set f1-score: 0.538\n",
      "          - saga_L1, Training set f1-score:0.675, Test set f1-score: 0.632\n",
      "          - newton-cg_L2, Training set f1-score:0.844, Test set f1-score: 0.538\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.844, Test set f1-score: 0.620\n",
      "          - saga_L1, Training set f1-score:0.807, Test set f1-score: 0.620\n",
      "          - newton-cg_L2, Training set f1-score:0.844, Test set f1-score: 0.620\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.804, Test set f1-score: 0.704\n",
      "          - saga_L1, Training set f1-score:0.804, Test set f1-score: 0.646\n",
      "          - newton-cg_L2, Training set f1-score:0.804, Test set f1-score: 0.704\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.824, Test set f1-score: 0.704\n",
      "          - saga_L1, Training set f1-score:0.824, Test set f1-score: 0.704\n",
      "          - newton-cg_L2, Training set f1-score:0.824, Test set f1-score: 0.704\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.824, Test set f1-score: 0.704\n",
      "          - saga_L1, Training set f1-score:0.824, Test set f1-score: 0.704\n",
      "          - newton-cg_L2, Training set f1-score:0.824, Test set f1-score: 0.704\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.672 f1-score on test data: 0.704\n",
      "          - tree depth: 2.000. f1-score on training data: 0.751 f1-score on test data: 0.620\n",
      "          - tree depth: 3.000. f1-score on training data: 0.807 f1-score on test data: 0.692\n",
      "          - tree depth: 4.000. f1-score on training data: 0.864 f1-score on test data: 0.778\n",
      "          - tree depth: 5.000. f1-score on training data: 0.883 f1-score on test data: 0.778\n",
      "          - tree depth: 6.000. f1-score on training data: 0.922 f1-score on test data: 0.692\n",
      "          - tree depth: 7.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - tree depth: 8.000. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - tree depth: 9.000. f1-score on training data: 0.941 f1-score on test data: 0.769\n",
      "          - tree depth: 10.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - tree depth: 11.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - tree depth: 12.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - tree depth: 13.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - tree depth: 14.000. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.863 f1-score on test data: 0.769\n",
      "          - 10trees. f1-score on training data: 0.902 f1-score on test data: 0.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 15trees. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - 20trees. f1-score on training data: 0.941 f1-score on test data: 0.704\n",
      "          - 25trees. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - 30trees. f1-score on training data: 0.941 f1-score on test data: 0.646\n",
      "          - 35trees. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - 40trees. f1-score on training data: 0.941 f1-score on test data: 0.704\n",
      "          - 45trees. f1-score on training data: 0.941 f1-score on test data: 0.704\n",
      "          - 50trees. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - 55trees. f1-score on training data: 0.941 f1-score on test data: 0.704\n",
      "          - 60trees. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - 65trees. f1-score on training data: 0.941 f1-score on test data: 0.772\n",
      "          - 70trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 75trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 80trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 85trees. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - 90trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 95trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.941 f1-score on test data: 0.615\n",
      "- Using 8 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.701, Test set f1-score: 0.481\n",
      "          - saga_L1, Training set f1-score:0.675, Test set f1-score: 0.632\n",
      "          - newton-cg_L2, Training set f1-score:0.701, Test set f1-score: 0.481\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.754, Test set f1-score: 0.538\n",
      "          - saga_L1, Training set f1-score:0.675, Test set f1-score: 0.632\n",
      "          - newton-cg_L2, Training set f1-score:0.754, Test set f1-score: 0.538\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.826, Test set f1-score: 0.538\n",
      "          - saga_L1, Training set f1-score:0.769, Test set f1-score: 0.462\n",
      "          - newton-cg_L2, Training set f1-score:0.826, Test set f1-score: 0.538\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "          - saga_L1, Training set f1-score:0.785, Test set f1-score: 0.468\n",
      "          - newton-cg_L2, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "          - saga_L1, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "          - newton-cg_L2, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "          - saga_L1, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "          - newton-cg_L2, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "          - saga_L1, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "          - newton-cg_L2, Training set f1-score:0.824, Test set f1-score: 0.468\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.785 f1-score on test data: 0.538\n",
      "          - tree depth: 2.000. f1-score on training data: 0.844 f1-score on test data: 0.620\n",
      "          - tree depth: 3.000. f1-score on training data: 0.882 f1-score on test data: 0.620\n",
      "          - tree depth: 4.000. f1-score on training data: 0.922 f1-score on test data: 0.692\n",
      "          - tree depth: 5.000. f1-score on training data: 0.941 f1-score on test data: 0.462\n",
      "          - tree depth: 6.000. f1-score on training data: 0.941 f1-score on test data: 0.615\n",
      "          - tree depth: 7.000. f1-score on training data: 0.941 f1-score on test data: 0.462\n",
      "          - tree depth: 8.000. f1-score on training data: 0.941 f1-score on test data: 0.462\n",
      "          - tree depth: 9.000. f1-score on training data: 0.941 f1-score on test data: 0.462\n",
      "          - tree depth: 10.000. f1-score on training data: 0.941 f1-score on test data: 0.462\n",
      "          - tree depth: 11.000. f1-score on training data: 0.941 f1-score on test data: 0.462\n",
      "          - tree depth: 12.000. f1-score on training data: 0.941 f1-score on test data: 0.462\n",
      "          - tree depth: 13.000. f1-score on training data: 0.941 f1-score on test data: 0.462\n",
      "          - tree depth: 14.000. f1-score on training data: 0.941 f1-score on test data: 0.462\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.902 f1-score on test data: 0.481\n",
      "          - 10trees. f1-score on training data: 0.922 f1-score on test data: 0.615\n",
      "          - 15trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 20trees. f1-score on training data: 0.941 f1-score on test data: 0.615\n",
      "          - 25trees. f1-score on training data: 0.941 f1-score on test data: 0.615\n",
      "          - 30trees. f1-score on training data: 0.941 f1-score on test data: 0.615\n",
      "          - 35trees. f1-score on training data: 0.941 f1-score on test data: 0.615\n",
      "          - 40trees. f1-score on training data: 0.941 f1-score on test data: 0.615\n",
      "          - 45trees. f1-score on training data: 0.941 f1-score on test data: 0.538\n",
      "          - 50trees. f1-score on training data: 0.941 f1-score on test data: 0.538\n",
      "          - 55trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 60trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 65trees. f1-score on training data: 0.941 f1-score on test data: 0.615\n",
      "          - 70trees. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - 75trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 80trees. f1-score on training data: 0.941 f1-score on test data: 0.620\n",
      "          - 85trees. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - 90trees. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - 95trees. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.941 f1-score on test data: 0.692\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.941 f1-score on test data: 0.696\n"
     ]
    }
   ],
   "source": [
    "models(df_2g,drop_lst,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43e5725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 64 ; Resampled dataset shape Counter({'MCI-AD': 32, 'MCI-CN': 32})\n",
      "\n",
      "8 principle components are needed to explain 90% of the data\n",
      "\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.575\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.335\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.575\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.679\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.335\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.679\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.657\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.550\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.657\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.624\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.604\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.624\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.627\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.582\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.627\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.593\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.598\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.593\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.628\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.598\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.628\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.455\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.515\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.472\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.501\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.589\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.560\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.543\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.543\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.512\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.483\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.512\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.512\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.512\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.512\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.559\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.614\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.615\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.577\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.608\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.573\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.560\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.545\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.579\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.594\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.645\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.645\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.630\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.645\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.615\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.615\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.595\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.611\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.592\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.509\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.428\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.532\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.319\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.532\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.640\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.319\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.640\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.620\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.273\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.620\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.659\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.623\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.659\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.647\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.593\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.647\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.612\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.628\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.612\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.628\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.628\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.628\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.455\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.515\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.472\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.501\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.589\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.560\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.543\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.543\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.512\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.483\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.512\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.512\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.512\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.512\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.559\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.614\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.615\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 25trees. average weighted f1-score of 10-cross validation:0.608\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.573\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.560\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.545\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.579\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.594\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.645\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.645\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.630\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.645\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.615\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.615\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.595\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.611\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.592\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.515\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.452\n",
      "- Using 8 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.506\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.350\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.506\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.640\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.319\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.640\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.591\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.661\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.591\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.586\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.575\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.586\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.586\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.586\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.586\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.586\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.586\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.586\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.586\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.586\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.586\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.673\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.589\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.569\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.560\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.521\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.538\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.508\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.520\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.535\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.550\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.550\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.550\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.550\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.550\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.529\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.473\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.532\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.589\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.574\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.588\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.596\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.607\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.589\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.589\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.590\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.608\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.591\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.591\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.591\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.608\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.591\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.608\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.624\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.502\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.531\n"
     ]
    }
   ],
   "source": [
    "cv_models(df_2g,drop_lst,target,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e0322c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 64 ; Resampled dataset shape Counter({'MCI-AD': 32, 'MCI-CN': 32})\n",
      "\n",
      "8 principle components are needed to explain 90% of the data\n",
      "\n",
      "Features sorted by their score for each estimator \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_importance</th>\n",
       "      <th>importance_1</th>\n",
       "      <th>importance_2</th>\n",
       "      <th>importance_3</th>\n",
       "      <th>importance_4</th>\n",
       "      <th>importance_5</th>\n",
       "      <th>importance_6</th>\n",
       "      <th>importance_7</th>\n",
       "      <th>importance_8</th>\n",
       "      <th>importance_9</th>\n",
       "      <th>importance_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NPIK9C</th>\n",
       "      <td>0.169285</td>\n",
       "      <td>0.175372</td>\n",
       "      <td>0.176845</td>\n",
       "      <td>0.178429</td>\n",
       "      <td>0.188315</td>\n",
       "      <td>0.152009</td>\n",
       "      <td>0.162420</td>\n",
       "      <td>0.172689</td>\n",
       "      <td>0.156833</td>\n",
       "      <td>0.168890</td>\n",
       "      <td>0.161042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIKTOT</th>\n",
       "      <td>0.164782</td>\n",
       "      <td>0.138031</td>\n",
       "      <td>0.169764</td>\n",
       "      <td>0.166604</td>\n",
       "      <td>0.181928</td>\n",
       "      <td>0.164134</td>\n",
       "      <td>0.174648</td>\n",
       "      <td>0.140933</td>\n",
       "      <td>0.175871</td>\n",
       "      <td>0.166405</td>\n",
       "      <td>0.169502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIK9A</th>\n",
       "      <td>0.125314</td>\n",
       "      <td>0.117180</td>\n",
       "      <td>0.113772</td>\n",
       "      <td>0.144927</td>\n",
       "      <td>0.116643</td>\n",
       "      <td>0.108415</td>\n",
       "      <td>0.150398</td>\n",
       "      <td>0.125842</td>\n",
       "      <td>0.146206</td>\n",
       "      <td>0.118190</td>\n",
       "      <td>0.111563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIK7</th>\n",
       "      <td>0.097935</td>\n",
       "      <td>0.099356</td>\n",
       "      <td>0.081024</td>\n",
       "      <td>0.091218</td>\n",
       "      <td>0.108890</td>\n",
       "      <td>0.106521</td>\n",
       "      <td>0.088514</td>\n",
       "      <td>0.120594</td>\n",
       "      <td>0.086539</td>\n",
       "      <td>0.100130</td>\n",
       "      <td>0.096567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIK4</th>\n",
       "      <td>0.073968</td>\n",
       "      <td>0.079721</td>\n",
       "      <td>0.072308</td>\n",
       "      <td>0.077496</td>\n",
       "      <td>0.034353</td>\n",
       "      <td>0.106709</td>\n",
       "      <td>0.051238</td>\n",
       "      <td>0.091312</td>\n",
       "      <td>0.082136</td>\n",
       "      <td>0.067289</td>\n",
       "      <td>0.077113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIK2</th>\n",
       "      <td>0.070770</td>\n",
       "      <td>0.067885</td>\n",
       "      <td>0.057345</td>\n",
       "      <td>0.064606</td>\n",
       "      <td>0.059795</td>\n",
       "      <td>0.077553</td>\n",
       "      <td>0.085834</td>\n",
       "      <td>0.058295</td>\n",
       "      <td>0.078725</td>\n",
       "      <td>0.083895</td>\n",
       "      <td>0.073771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OSA</th>\n",
       "      <td>0.065909</td>\n",
       "      <td>0.062323</td>\n",
       "      <td>0.080417</td>\n",
       "      <td>0.054939</td>\n",
       "      <td>0.076364</td>\n",
       "      <td>0.063528</td>\n",
       "      <td>0.069786</td>\n",
       "      <td>0.071539</td>\n",
       "      <td>0.045402</td>\n",
       "      <td>0.057305</td>\n",
       "      <td>0.077493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIK1</th>\n",
       "      <td>0.063909</td>\n",
       "      <td>0.068700</td>\n",
       "      <td>0.067979</td>\n",
       "      <td>0.063788</td>\n",
       "      <td>0.074105</td>\n",
       "      <td>0.062933</td>\n",
       "      <td>0.052686</td>\n",
       "      <td>0.065329</td>\n",
       "      <td>0.054913</td>\n",
       "      <td>0.067617</td>\n",
       "      <td>0.061044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIK9B</th>\n",
       "      <td>0.056478</td>\n",
       "      <td>0.054347</td>\n",
       "      <td>0.054403</td>\n",
       "      <td>0.066054</td>\n",
       "      <td>0.061393</td>\n",
       "      <td>0.060827</td>\n",
       "      <td>0.050738</td>\n",
       "      <td>0.045611</td>\n",
       "      <td>0.054602</td>\n",
       "      <td>0.054097</td>\n",
       "      <td>0.062707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIK6</th>\n",
       "      <td>0.051542</td>\n",
       "      <td>0.060569</td>\n",
       "      <td>0.048002</td>\n",
       "      <td>0.044408</td>\n",
       "      <td>0.047796</td>\n",
       "      <td>0.057962</td>\n",
       "      <td>0.056895</td>\n",
       "      <td>0.043298</td>\n",
       "      <td>0.056001</td>\n",
       "      <td>0.047143</td>\n",
       "      <td>0.053345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insomnia</th>\n",
       "      <td>0.026551</td>\n",
       "      <td>0.031282</td>\n",
       "      <td>0.030695</td>\n",
       "      <td>0.016003</td>\n",
       "      <td>0.027672</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.034530</td>\n",
       "      <td>0.023982</td>\n",
       "      <td>0.031404</td>\n",
       "      <td>0.029131</td>\n",
       "      <td>0.027181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIK8</th>\n",
       "      <td>0.021849</td>\n",
       "      <td>0.033265</td>\n",
       "      <td>0.027018</td>\n",
       "      <td>0.018820</td>\n",
       "      <td>0.009456</td>\n",
       "      <td>0.014098</td>\n",
       "      <td>0.022313</td>\n",
       "      <td>0.026023</td>\n",
       "      <td>0.017463</td>\n",
       "      <td>0.028164</td>\n",
       "      <td>0.021876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIK5</th>\n",
       "      <td>0.011707</td>\n",
       "      <td>0.011967</td>\n",
       "      <td>0.020429</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>0.013288</td>\n",
       "      <td>0.011681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>0.011744</td>\n",
       "      <td>0.006795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPIK3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          avg_importance  importance_1  importance_2  importance_3  \\\n",
       "NPIK9C          0.169285      0.175372      0.176845      0.178429   \n",
       "NPIKTOT         0.164782      0.138031      0.169764      0.166604   \n",
       "NPIK9A          0.125314      0.117180      0.113772      0.144927   \n",
       "NPIK7           0.097935      0.099356      0.081024      0.091218   \n",
       "NPIK4           0.073968      0.079721      0.072308      0.077496   \n",
       "NPIK2           0.070770      0.067885      0.057345      0.064606   \n",
       "OSA             0.065909      0.062323      0.080417      0.054939   \n",
       "NPIK1           0.063909      0.068700      0.067979      0.063788   \n",
       "NPIK9B          0.056478      0.054347      0.054403      0.066054   \n",
       "NPIK6           0.051542      0.060569      0.048002      0.044408   \n",
       "insomnia        0.026551      0.031282      0.030695      0.016003   \n",
       "NPIK8           0.021849      0.033265      0.027018      0.018820   \n",
       "NPIK5           0.011707      0.011967      0.020429      0.012707   \n",
       "NPIK3           0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "          importance_4  importance_5  importance_6  importance_7  \\\n",
       "NPIK9C        0.188315      0.152009      0.162420      0.172689   \n",
       "NPIKTOT       0.181928      0.164134      0.174648      0.140933   \n",
       "NPIK9A        0.116643      0.108415      0.150398      0.125842   \n",
       "NPIK7         0.108890      0.106521      0.088514      0.120594   \n",
       "NPIK4         0.034353      0.106709      0.051238      0.091312   \n",
       "NPIK2         0.059795      0.077553      0.085834      0.058295   \n",
       "OSA           0.076364      0.063528      0.069786      0.071539   \n",
       "NPIK1         0.074105      0.062933      0.052686      0.065329   \n",
       "NPIK9B        0.061393      0.060827      0.050738      0.045611   \n",
       "NPIK6         0.047796      0.057962      0.056895      0.043298   \n",
       "insomnia      0.027672      0.013631      0.034530      0.023982   \n",
       "NPIK8         0.009456      0.014098      0.022313      0.026023   \n",
       "NPIK5         0.013288      0.011681      0.000000      0.014553   \n",
       "NPIK3         0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "          importance_8  importance_9  importance_10  \n",
       "NPIK9C        0.156833      0.168890       0.161042  \n",
       "NPIKTOT       0.175871      0.166405       0.169502  \n",
       "NPIK9A        0.146206      0.118190       0.111563  \n",
       "NPIK7         0.086539      0.100130       0.096567  \n",
       "NPIK4         0.082136      0.067289       0.077113  \n",
       "NPIK2         0.078725      0.083895       0.073771  \n",
       "OSA           0.045402      0.057305       0.077493  \n",
       "NPIK1         0.054913      0.067617       0.061044  \n",
       "NPIK9B        0.054602      0.054097       0.062707  \n",
       "NPIK6         0.056001      0.047143       0.053345  \n",
       "insomnia      0.031404      0.029131       0.027181  \n",
       "NPIK8         0.017463      0.028164       0.021876  \n",
       "NPIK5         0.013907      0.011744       0.006795  \n",
       "NPIK3         0.000000      0.000000       0.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = usampling_scale_data(df_2g,drop_lst,target)     \n",
    "X = res[0]\n",
    "y = res[3]\n",
    "clf = RandomForestClassifier(n_estimators =55, random_state = 5862)\n",
    "title_label = '10-fold crossvalidation of random forest with (55 trees)'\n",
    "feature_importance(X,y,clf,10,title_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625eea53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e32ef963",
   "metadata": {},
   "source": [
    "MCI-AD': 32, 'MCI-CN': 32, 'MCI-MCI': 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a0d9049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 96 ; Resampled dataset shape Counter({'MCI-AD': 32, 'MCI-CN': 32, 'MCI-MCI': 32})\n",
      "\n",
      "9 principle components are needed to explain 90% of the data\n",
      "\n",
      "Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.575, Test set f1-score: 0.478\n",
      "          - saga_L1, Training set f1-score:0.524, Test set f1-score: 0.400\n",
      "          - newton-cg_L2, Training set f1-score:0.575, Test set f1-score: 0.538\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.562, Test set f1-score: 0.601\n",
      "          - saga_L1, Training set f1-score:0.524, Test set f1-score: 0.400\n",
      "          - newton-cg_L2, Training set f1-score:0.562, Test set f1-score: 0.601\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.584, Test set f1-score: 0.601\n",
      "          - saga_L1, Training set f1-score:0.522, Test set f1-score: 0.478\n",
      "          - newton-cg_L2, Training set f1-score:0.584, Test set f1-score: 0.601\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.606, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.591, Test set f1-score: 0.601\n",
      "          - newton-cg_L2, Training set f1-score:0.606, Test set f1-score: 0.600\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.588, Test set f1-score: 0.601\n",
      "          - saga_L1, Training set f1-score:0.587, Test set f1-score: 0.600\n",
      "          - newton-cg_L2, Training set f1-score:0.588, Test set f1-score: 0.601\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "          - newton-cg_L2, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "          - newton-cg_L2, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.523 f1-score on test data: 0.400\n",
      "          - tree depth: 2.000. f1-score on training data: 0.563 f1-score on test data: 0.524\n",
      "          - tree depth: 3.000. f1-score on training data: 0.595 f1-score on test data: 0.524\n",
      "          - tree depth: 4.000. f1-score on training data: 0.646 f1-score on test data: 0.524\n",
      "          - tree depth: 5.000. f1-score on training data: 0.697 f1-score on test data: 0.524\n",
      "          - tree depth: 6.000. f1-score on training data: 0.776 f1-score on test data: 0.536\n",
      "          - tree depth: 7.000. f1-score on training data: 0.818 f1-score on test data: 0.531\n",
      "          - tree depth: 8.000. f1-score on training data: 0.855 f1-score on test data: 0.531\n",
      "          - tree depth: 9.000. f1-score on training data: 0.868 f1-score on test data: 0.531\n",
      "          - tree depth: 10.000. f1-score on training data: 0.882 f1-score on test data: 0.394\n",
      "          - tree depth: 11.000. f1-score on training data: 0.895 f1-score on test data: 0.461\n",
      "          - tree depth: 12.000. f1-score on training data: 0.896 f1-score on test data: 0.466\n",
      "          - tree depth: 13.000. f1-score on training data: 0.896 f1-score on test data: 0.466\n",
      "          - tree depth: 14.000. f1-score on training data: 0.896 f1-score on test data: 0.466\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.843 f1-score on test data: 0.569\n",
      "          - 10trees. f1-score on training data: 0.869 f1-score on test data: 0.554\n",
      "          - 15trees. f1-score on training data: 0.894 f1-score on test data: 0.548\n",
      "          - 20trees. f1-score on training data: 0.894 f1-score on test data: 0.442\n",
      "          - 25trees. f1-score on training data: 0.894 f1-score on test data: 0.489\n",
      "          - 30trees. f1-score on training data: 0.894 f1-score on test data: 0.489\n",
      "          - 35trees. f1-score on training data: 0.894 f1-score on test data: 0.489\n",
      "          - 40trees. f1-score on training data: 0.894 f1-score on test data: 0.489\n",
      "          - 45trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 50trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 55trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 60trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 65trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 70trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 75trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 80trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 85trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 90trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 95trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.894 f1-score on test data: 0.500\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.895 f1-score on test data: 0.229\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.573, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.524, Test set f1-score: 0.400\n",
      "          - newton-cg_L2, Training set f1-score:0.573, Test set f1-score: 0.600\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.573, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.524, Test set f1-score: 0.400\n",
      "          - newton-cg_L2, Training set f1-score:0.573, Test set f1-score: 0.600\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.568, Test set f1-score: 0.596\n",
      "          - saga_L1, Training set f1-score:0.524, Test set f1-score: 0.400\n",
      "          - newton-cg_L2, Training set f1-score:0.568, Test set f1-score: 0.596\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.587, Test set f1-score: 0.596\n",
      "          - saga_L1, Training set f1-score:0.547, Test set f1-score: 0.596\n",
      "          - newton-cg_L2, Training set f1-score:0.587, Test set f1-score: 0.596\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.603, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.581, Test set f1-score: 0.600\n",
      "          - newton-cg_L2, Training set f1-score:0.603, Test set f1-score: 0.600\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "          - newton-cg_L2, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "          - newton-cg_L2, Training set f1-score:0.570, Test set f1-score: 0.600\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.523 f1-score on test data: 0.400\n",
      "          - tree depth: 2.000. f1-score on training data: 0.563 f1-score on test data: 0.524\n",
      "          - tree depth: 3.000. f1-score on training data: 0.595 f1-score on test data: 0.524\n",
      "          - tree depth: 4.000. f1-score on training data: 0.646 f1-score on test data: 0.524\n",
      "          - tree depth: 5.000. f1-score on training data: 0.697 f1-score on test data: 0.524\n",
      "          - tree depth: 6.000. f1-score on training data: 0.776 f1-score on test data: 0.536\n",
      "          - tree depth: 7.000. f1-score on training data: 0.818 f1-score on test data: 0.531\n",
      "          - tree depth: 8.000. f1-score on training data: 0.855 f1-score on test data: 0.531\n",
      "          - tree depth: 9.000. f1-score on training data: 0.868 f1-score on test data: 0.531\n",
      "          - tree depth: 10.000. f1-score on training data: 0.882 f1-score on test data: 0.394\n",
      "          - tree depth: 11.000. f1-score on training data: 0.895 f1-score on test data: 0.461\n",
      "          - tree depth: 12.000. f1-score on training data: 0.896 f1-score on test data: 0.466\n",
      "          - tree depth: 13.000. f1-score on training data: 0.896 f1-score on test data: 0.466\n",
      "          - tree depth: 14.000. f1-score on training data: 0.896 f1-score on test data: 0.466\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.843 f1-score on test data: 0.569\n",
      "          - 10trees. f1-score on training data: 0.869 f1-score on test data: 0.557\n",
      "          - 15trees. f1-score on training data: 0.894 f1-score on test data: 0.548\n",
      "          - 20trees. f1-score on training data: 0.894 f1-score on test data: 0.446\n",
      "          - 25trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 30trees. f1-score on training data: 0.894 f1-score on test data: 0.489\n",
      "          - 35trees. f1-score on training data: 0.894 f1-score on test data: 0.543\n",
      "          - 40trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 45trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 50trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 55trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 60trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 65trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 70trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 75trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 80trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 85trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 90trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "          - 95trees. f1-score on training data: 0.894 f1-score on test data: 0.495\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.895 f1-score on test data: 0.394\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.895 f1-score on test data: 0.399\n",
      "- Using 9 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.558, Test set f1-score: 0.601\n",
      "          - saga_L1, Training set f1-score:0.524, Test set f1-score: 0.400\n",
      "          - newton-cg_L2, Training set f1-score:0.558, Test set f1-score: 0.601\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.559, Test set f1-score: 0.601\n",
      "          - saga_L1, Training set f1-score:0.524, Test set f1-score: 0.400\n",
      "          - newton-cg_L2, Training set f1-score:0.559, Test set f1-score: 0.601\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.539, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.559, Test set f1-score: 0.601\n",
      "          - newton-cg_L2, Training set f1-score:0.539, Test set f1-score: 0.600\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.539, Test set f1-score: 0.599\n",
      "          - newton-cg_L2, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "          - newton-cg_L2, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "          - newton-cg_L2, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "          - saga_L1, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "          - newton-cg_L2, Training set f1-score:0.534, Test set f1-score: 0.600\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.600 f1-score on test data: 0.600\n",
      "          - tree depth: 2.000. f1-score on training data: 0.660 f1-score on test data: 0.600\n",
      "          - tree depth: 3.000. f1-score on training data: 0.672 f1-score on test data: 0.600\n",
      "          - tree depth: 4.000. f1-score on training data: 0.701 f1-score on test data: 0.458\n",
      "          - tree depth: 5.000. f1-score on training data: 0.722 f1-score on test data: 0.597\n",
      "          - tree depth: 6.000. f1-score on training data: 0.775 f1-score on test data: 0.551\n",
      "          - tree depth: 7.000. f1-score on training data: 0.820 f1-score on test data: 0.582\n",
      "          - tree depth: 8.000. f1-score on training data: 0.856 f1-score on test data: 0.637\n",
      "          - tree depth: 9.000. f1-score on training data: 0.884 f1-score on test data: 0.573\n",
      "          - tree depth: 10.000. f1-score on training data: 0.884 f1-score on test data: 0.511\n",
      "          - tree depth: 11.000. f1-score on training data: 0.895 f1-score on test data: 0.511\n",
      "          - tree depth: 12.000. f1-score on training data: 0.896 f1-score on test data: 0.511\n",
      "          - tree depth: 13.000. f1-score on training data: 0.896 f1-score on test data: 0.511\n",
      "          - tree depth: 14.000. f1-score on training data: 0.896 f1-score on test data: 0.511\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.806 f1-score on test data: 0.262\n",
      "          - 10trees. f1-score on training data: 0.882 f1-score on test data: 0.511\n",
      "          - 15trees. f1-score on training data: 0.882 f1-score on test data: 0.425\n",
      "          - 20trees. f1-score on training data: 0.895 f1-score on test data: 0.545\n",
      "          - 25trees. f1-score on training data: 0.894 f1-score on test data: 0.425\n",
      "          - 30trees. f1-score on training data: 0.894 f1-score on test data: 0.453\n",
      "          - 35trees. f1-score on training data: 0.894 f1-score on test data: 0.500\n",
      "          - 40trees. f1-score on training data: 0.894 f1-score on test data: 0.500\n",
      "          - 45trees. f1-score on training data: 0.894 f1-score on test data: 0.461\n",
      "          - 50trees. f1-score on training data: 0.894 f1-score on test data: 0.429\n",
      "          - 55trees. f1-score on training data: 0.894 f1-score on test data: 0.429\n",
      "          - 60trees. f1-score on training data: 0.894 f1-score on test data: 0.363\n",
      "          - 65trees. f1-score on training data: 0.894 f1-score on test data: 0.429\n",
      "          - 70trees. f1-score on training data: 0.894 f1-score on test data: 0.461\n",
      "          - 75trees. f1-score on training data: 0.894 f1-score on test data: 0.461\n",
      "          - 80trees. f1-score on training data: 0.894 f1-score on test data: 0.461\n",
      "          - 85trees. f1-score on training data: 0.894 f1-score on test data: 0.461\n",
      "          - 90trees. f1-score on training data: 0.894 f1-score on test data: 0.406\n",
      "          - 95trees. f1-score on training data: 0.894 f1-score on test data: 0.406\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.895 f1-score on test data: 0.399\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.895 f1-score on test data: 0.341\n"
     ]
    }
   ],
   "source": [
    "models(df_3g,drop_lst,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6843adcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 96 ; Resampled dataset shape Counter({'MCI-AD': 32, 'MCI-CN': 32, 'MCI-MCI': 32})\n",
      "\n",
      "9 principle components are needed to explain 90% of the data\n",
      "\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.300\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.159\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.300\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.356\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.159\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.356\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.327\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.274\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.327\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.330\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.351\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.330\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.335\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.335\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.335\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.335\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.335\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.335\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.335\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.325\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.335\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.233\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.316\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.373\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.373\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.422\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.377\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.361\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.344\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.371\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.340\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.404\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.354\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.351\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.334\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.336\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.404\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.398\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.374\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.404\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.392\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.395\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.388\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.396\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.395\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.383\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.395\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.396\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.395\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.395\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.382\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.382\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.375\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.395\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.326\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.282\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.315\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.159\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.315\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.350\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.177\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.350\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.312\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.273\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.312\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.328\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.321\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.328\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.335\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.325\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.359\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.335\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.344\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.335\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.233\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.316\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.373\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.373\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.422\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.377\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.361\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.344\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.371\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.340\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.404\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.354\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.351\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.334\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.329\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.378\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.374\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.374\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.397\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.385\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.379\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.396\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.396\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.384\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.382\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.394\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.404\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.395\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.395\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.382\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.382\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.375\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.392\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.301\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.277\n",
      "- Using 9 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.272\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.177\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.272\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.326\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.159\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.326\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.310\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.309\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.310\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.310\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.315\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.310\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.312\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.312\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.312\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.313\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.312\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.305\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.312\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.381\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.384\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.357\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.343\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.369\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.332\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.374\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.365\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.333\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.321\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.339\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.348\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.352\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.352\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.348\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.331\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.305\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.277\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.309\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.361\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.379\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.341\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.345\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.318\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.336\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.339\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.331\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.322\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.366\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.359\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.359\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.345\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.347\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.413\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.353\n"
     ]
    }
   ],
   "source": [
    "cv_models(df_3g,drop_lst,target,k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
