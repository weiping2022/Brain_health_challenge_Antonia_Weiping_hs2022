{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "20f5d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import export_text\n",
    "import mglearn\n",
    "from dashboard_one import *\n",
    "from dash_model_two import *\n",
    "from feature_selection import *\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "feb5850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## models(df,drop_lst,target)\n",
    "\n",
    "def usampling_split_scale_data(df,drop_lst,target):\n",
    "    '''\n",
    "    undersampling data, split data (4:1),data scaling, pca components which could explains 90% data\n",
    "    ----------------------------------\n",
    "    df: the full dataframe\n",
    "    target: the target feature name\n",
    "    -----------\n",
    "    Outputs: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
    "       '''\n",
    "    # undersampling \n",
    "    X = df.copy()\n",
    "    y = X[target]\n",
    "    rus = RandomUnderSampler(random_state=432)\n",
    "    X_undersampled, y_unsampled = rus.fit_resample(X, y)\n",
    "    print('After undersampling data size is',len(X_undersampled),'; Resampled dataset shape %s' % Counter(y_unsampled))\n",
    "    # split data\n",
    "    train, test = train_test_split(X_undersampled,test_size=0.2)   \n",
    "    X_train = train.drop(drop_lst,axis=1)\n",
    "    y_train = train[target]\n",
    "    X_test = test.drop(drop_lst,axis=1)\n",
    "    y_test = test[target]\n",
    "    # data scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # pca components\n",
    "    pca = PCA(n_components=X_train_scaled.shape[1]) # keep all n principal components \n",
    "    pca.fit(X_train_scaled) # fit PCA model with scaled data\n",
    "    X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "    ex_ratio = pca.explained_variance_ratio_\n",
    "    cum_sum = 0\n",
    "    for i in range(len(ex_ratio)):\n",
    "        cum_sum += ex_ratio[i]\n",
    "        if cum_sum >= 0.9:  # if it could explain 90% of the data, then stop\n",
    "            break\n",
    "    n_com = i         \n",
    "         # PCA with first n_com components\n",
    "    pca = PCA(n_com)\n",
    "    pca.fit(X_train_scaled)\n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "       #plot \n",
    "    plt.bar(range(1,len(pca.explained_variance_ratio_ )+1),pca.explained_variance_ratio_ )\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Components')\n",
    "    plt.plot(range(1,len(pca.explained_variance_ )+1),\n",
    "             np.cumsum(pca.explained_variance_ratio_),\n",
    "             c='red',\n",
    "             label=\"Cumulative Explained Variance ratio\")\n",
    "    plt.legend(loc='upper left')\n",
    "    print('\\n{} principle components are needed to explain 90% of the data\\n'.format(n_com))  \n",
    "    print('Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test')\n",
    "    X_labels = ['original dataset','scaled dataset','%s pca-components'%n_com]\n",
    "    return X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test,X_labels  \n",
    "\n",
    "def models(df,drop_lst,target):\n",
    "    '''\n",
    "    for splitted data\n",
    "    '''\n",
    "    res = usampling_split_scale_data(df,drop_lst,target)\n",
    "    y_train = res[6]\n",
    "    y_test = res[7]\n",
    "    X_labels = res[8]\n",
    "    for i in range(len(X_labels)):\n",
    "        print('- Using {}:'.format(X_labels[i]))\n",
    "        X_train = res[2*i]\n",
    "        X_test = res[2*i+1]\n",
    "        # logistic regression\n",
    "        C_lst = [0.001,0.01,0.1,1,10,100,1000]\n",
    "        print('    - Logistic regression')\n",
    "        for i in range(len(C_lst)):\n",
    "            print('       - C = {}'.format(C_lst[i]))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='lbfgs',multi_class='auto',penalty='l2',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - lbfgs_L2, Training set f1-score:{:.3f}, Test set f1-score: {:.3f}'\n",
    "                  .format(f1_score(logreg.predict(X_train),y_train,average='weighted'),f1_score(logreg.predict(X_test),y_test,average='weighted')))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='saga',multi_class='auto',penalty='l1',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - saga_L1, Training set f1-score:{:.3f}, Test set f1-score: {:.3f}'\n",
    "                  .format(f1_score(logreg.predict(X_train),y_train,average='weighted'),f1_score(logreg.predict(X_test),y_test,average='weighted')))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='newton-cg',multi_class='auto',penalty='l2',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - newton-cg_L2, Training set f1-score:{:.3f}, Test set f1-score: {:.3f}'\n",
    "                  .format(f1_score(logreg.predict(X_train),y_train,average='weighted'),f1_score(logreg.predict(X_test),y_test,average='weighted')))\n",
    "\n",
    "        # decision tree\n",
    "        print('    - Decision tree')\n",
    "        for i in range(1,15):\n",
    "            dtree = DecisionTreeClassifier(random_state=0,max_depth=i,criterion='gini')\n",
    "            dtree.fit(X_train,y_train)\n",
    "            print('          - tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(dtree.predict(X_train),y_train,average='weighted'),f1_score(dtree.predict(X_test),y_test,average='weighted')))\n",
    "        # random forest\n",
    "        print('    - Random forest')\n",
    "        for i in range(1,20):   \n",
    "            m= 5*i\n",
    "            forest = RandomForestClassifier(n_estimators=m,random_state=5862)\n",
    "            forest.fit(X_train,y_train)\n",
    "            print('          - {}trees. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(m,f1_score(forest.predict(X_train),y_train,average='weighted'),f1_score(forest.predict(X_test),y_test,average='weighted')))\n",
    "        # MLP  \n",
    "        print('    - MLP')\n",
    "        hls = [[50,50],[20,20],[100,100],[50,50,50]] # hidden layer size\n",
    "        for i in range(len(hls)):\n",
    "            mlp = MLPClassifier(solver='lbfgs',random_state=460,hidden_layer_sizes = hls[i],max_iter=20000)\n",
    "            mlp.fit(X_train,y_train)\n",
    "            print('          - hidden layer size{}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'.format(hls[i],f1_score(mlp.predict(X_train),y_train,average='weighted'),f1_score(mlp.predict(X_test),y_test,average='weighted'))) \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "##### cv_models(df,drop_lst,target,k)\n",
    "\n",
    "def usampling_scale_data(df,drop_lst,target):\n",
    "    '''\n",
    "    undersampling data, NOT SPLIT data (later use CROSS VALIDATION),data scaling, pca components which could explains 90% data\n",
    "    ----------------------------------\n",
    "    df: the full dataframe\n",
    "    drop_lst: drop the features which are not gonna be used in modeling, e.g. RID,...\n",
    "    target: the target feature name\n",
    "    -----------\n",
    "    Outputs: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
    "       '''\n",
    "    # undersampling \n",
    "    y_output = df[target]\n",
    "    rus = RandomUnderSampler(random_state=432)\n",
    "    X_undersampled, y_unsampled = rus.fit_resample(df, y_output)\n",
    "    print('After undersampling data size is',len(X_undersampled),'; Resampled dataset shape %s' % Counter(y_unsampled)) \n",
    "    # feature list for the X \n",
    "    # normal input output data\n",
    "    X = X_undersampled.drop(drop_lst,axis=1)\n",
    "    y = X_undersampled[target]\n",
    "    # data scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    # pca components\n",
    "    pca = PCA(n_components=X_scaled.shape[1]) # keep all n principal components \n",
    "    pca.fit(X_scaled) # fit PCA model with scaled data\n",
    "    X_pca = pca.transform(X_scaled)  #transform data onto the first two principal components\n",
    "    ex_ratio = pca.explained_variance_ratio_\n",
    "    cum_sum = 0\n",
    "    for i in range(len(ex_ratio)):\n",
    "        cum_sum += ex_ratio[i]\n",
    "        if cum_sum >= 0.9:  # if it could explain 90% of the data, then stop\n",
    "            break\n",
    "    n_com = i         \n",
    "         # PCA with first n_com components\n",
    "    pca = PCA(n_com)\n",
    "    pca.fit(X_scaled)\n",
    "    X_pca = pca.transform(X_scaled)\n",
    "       #plot \n",
    "    plt.bar(range(1,len(pca.explained_variance_ratio_ )+1),pca.explained_variance_ratio_ )\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Components')\n",
    "    plt.plot(range(1,len(pca.explained_variance_ )+1),\n",
    "             np.cumsum(pca.explained_variance_ratio_),\n",
    "             c='red',\n",
    "             label=\"Cumulative Explained Variance ratio\")\n",
    "    plt.legend(loc='upper left')\n",
    "    print('\\n{} principle components are needed to explain 90% of the data\\n'.format(n_com))  \n",
    "    #print('Output dataframes sequence: X_,X_scaled,X_pca,y_')\n",
    "    X_labels = ['original dataset','scaled dataset','%s pca-components'%n_com]\n",
    "    return X,X_scaled,X_pca,y,X_labels \n",
    "\n",
    "def cv_models(df,drop_lst,target,k):\n",
    "    '''\n",
    "    df: full dataframe.\n",
    "    drop_lst: drop the features which are not gonna be used in modeling, e.g. RID,...\n",
    "    target: the target feature name\n",
    "    k: folds of cross-validation\n",
    "    '''\n",
    "    res = usampling_scale_data(df,drop_lst,target) # Output dataframes sequence: X_,X_scaled,X_pca,y_\n",
    "    \n",
    "    y = res[3]\n",
    "    X_labels = res[4]\n",
    "    for i in range(3):\n",
    "        X = res[i]\n",
    "        print('- Using {}:'.format(X_labels[i]))\n",
    "        # logistic regression\n",
    "        C_lst = [0.001,0.01,0.1,1,10,100,1000]\n",
    "        print('    - Logistic regression')\n",
    "        for i in range(len(C_lst)):\n",
    "            print('       - C = {}'.format(C_lst[i]))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='lbfgs',multi_class='auto',penalty='l2',max_iter=10000).fit(X,y)\n",
    "            print('          - lbfgs_L2, average weighted f1-score of {}-cross validation:{:.3f}'.format(k,cross_val_score(logreg, X, y, cv = k,scoring='f1_weighted').mean()))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='saga',multi_class='auto',penalty='l1',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - saga_L1, average weighted f1-score of {}-cross validation:{:.3f}'.format(k,cross_val_score(logreg, X, y, cv = k,scoring='f1_weighted').mean()))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='newton-cg',multi_class='auto',penalty='l2',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - newton-cg_L2, average weighted f1-score of {}-cross validation:{:.3f}'.format(k,cross_val_score(logreg, X, y, cv = k,scoring='f1_weighted').mean()))\n",
    "\n",
    "        # decision tree\n",
    "        print('    - Decision tree')\n",
    "        for i in range(1,15):\n",
    "            dtree = DecisionTreeClassifier(random_state=0,max_depth=i,criterion='gini')\n",
    "            dtree.fit(X,y)\n",
    "            print('          - tree depth: {:.3f}. average weighted f1-score of {}-cross validation:{:.3f}'\n",
    "                .format(i,k,cross_val_score(dtree, X, y, cv = k,scoring='f1_weighted').mean()))\n",
    "\n",
    "        # random forest\n",
    "        print('    - Random forest')\n",
    "        for i in range(1,20):   \n",
    "            m= 5*i\n",
    "            forest = RandomForestClassifier(n_estimators=m,random_state=5862)\n",
    "            forest.fit(X,y)\n",
    "            print('          - {}trees. average weighted f1-score of {}-cross validation:{:.3f}'\n",
    "                .format(m,k,cross_val_score(forest, X, y, cv = k,scoring='f1_weighted').mean()))\n",
    "        # MLP \n",
    "        print('    - MLP')\n",
    "        hls = [[50,50],[20,20],[100,100],[50,50,50]] # hidden layer size\n",
    "        for i in range(len(hls)):\n",
    "            mlp = MLPClassifier(solver='lbfgs',random_state=460,hidden_layer_sizes = hls[i],max_iter=20000).fit(X,y)\n",
    "            mlp.fit(X,y)\n",
    "            print('          - hidden layer size{}. average weighted f1-score of {}-cross validation:{:.3f}'.format(hls[i],k,cross_val_score(mlp, X, y, cv = k,scoring='f1_weighted').mean()))          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b74b94",
   "metadata": {},
   "source": [
    "### sleep, brain_volume_ratio_to_baseline_____VS_____diagnosischanges from every visit\n",
    "\n",
    "\n",
    "#### sleep_brain_dxch.csv\n",
    "#### drop column 'NPIKSEV', otherwise we get no samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "10f4b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_col = ['Phase', 'RID', 'VISCODE','PTID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "67519c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase</th>\n",
       "      <th>RID</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>DXCHANGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m06</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m12</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m18</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m24</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m30</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m36</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m42</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m48</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m54</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m60</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m66</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>m72</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>ADNI1</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>ADNIGO</td>\n",
       "      <td>61</td>\n",
       "      <td>m60</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>61</td>\n",
       "      <td>v06</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>61</td>\n",
       "      <td>v11</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>CN-MCI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>61</td>\n",
       "      <td>v21</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>MCI-AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>61</td>\n",
       "      <td>v31</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>MCI-AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>61</td>\n",
       "      <td>v41</td>\n",
       "      <td>023_S_0061</td>\n",
       "      <td>AD-AD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Phase  RID VISCODE        PTID DXCHANGE\n",
       "370   ADNI1   61     m06  023_S_0061    CN-CN\n",
       "371   ADNI1   61     m12  023_S_0061    CN-CN\n",
       "372   ADNI1   61     m18  023_S_0061    CN-CN\n",
       "373   ADNI1   61     m24  023_S_0061    CN-CN\n",
       "374   ADNI1   61     m30  023_S_0061    CN-CN\n",
       "375   ADNI1   61     m36  023_S_0061    CN-CN\n",
       "376   ADNI1   61     m42  023_S_0061    CN-CN\n",
       "377   ADNI1   61     m48  023_S_0061    CN-CN\n",
       "378   ADNI1   61     m54  023_S_0061    CN-CN\n",
       "379   ADNI1   61     m60  023_S_0061    CN-CN\n",
       "380   ADNI1   61     m66  023_S_0061    CN-CN\n",
       "381   ADNI1   61     m72  023_S_0061    CN-CN\n",
       "382   ADNI1   61     NaN  023_S_0061    CN-AD\n",
       "383   ADNI1   61     NaN  023_S_0061    CN-AD\n",
       "384   ADNI1   61     NaN  023_S_0061    CN-AD\n",
       "385   ADNI1   61     NaN  023_S_0061   CN-MCI\n",
       "386  ADNIGO   61     m60  023_S_0061    CN-CN\n",
       "387   ADNI2   61     v06  023_S_0061    CN-CN\n",
       "388   ADNI2   61     v11  023_S_0061   CN-MCI\n",
       "389   ADNI2   61     v21  023_S_0061   MCI-AD\n",
       "390   ADNI2   61     v31  023_S_0061   MCI-AD\n",
       "391   ADNI2   61     v41  023_S_0061    AD-AD"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_brain_dxch[sleep_brain_dxch['RID']==61][com_col + ['DXCHANGE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "769bd041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Phase</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>NPIK6</th>\n",
       "      <th>...</th>\n",
       "      <th>ratio_PTAU_bl</th>\n",
       "      <th>Ventricles_reduction_per_year</th>\n",
       "      <th>Hippocampus_reduction_per_year</th>\n",
       "      <th>wholebrain_reduction_per_year</th>\n",
       "      <th>Entorhinal_reduction_per_year</th>\n",
       "      <th>Fusiform_reduction_per_year</th>\n",
       "      <th>ICV_reduction_per_year</th>\n",
       "      <th>ABETA_reduction_per_year</th>\n",
       "      <th>TAU_reduction_per_year</th>\n",
       "      <th>PTAU_reduction_per_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m06</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m36</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m60</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m66</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m72</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19432</th>\n",
       "      <td>7083</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>126_S_7083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19433</th>\n",
       "      <td>7085</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>941_S_7085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19434</th>\n",
       "      <td>7088</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>033_S_7088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19435</th>\n",
       "      <td>7092</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>021_S_7092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19436</th>\n",
       "      <td>7100</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>033_S_7100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19437 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RID  Phase VISCODE        PTID  NPIK1  NPIK2  NPIK3  NPIK4  NPIK5  \\\n",
       "0         2  ADNI1     m06  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "1         2  ADNI1     m36  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "2         2  ADNI1     m60  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "3         2  ADNI1     m66  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "4         2  ADNI1     m72  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "...     ...    ...     ...         ...    ...    ...    ...    ...    ...   \n",
       "19432  7083  ADNI3      sc  126_S_7083    NaN    NaN    NaN    NaN    NaN   \n",
       "19433  7085  ADNI3      sc  941_S_7085    NaN    NaN    NaN    NaN    NaN   \n",
       "19434  7088  ADNI3      sc  033_S_7088    NaN    NaN    NaN    NaN    NaN   \n",
       "19435  7092  ADNI3      sc  021_S_7092    NaN    NaN    NaN    NaN    NaN   \n",
       "19436  7100  ADNI3      sc  033_S_7100    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "       NPIK6  ...  ratio_PTAU_bl  Ventricles_reduction_per_year  \\\n",
       "0        NaN  ...            NaN                            NaN   \n",
       "1        NaN  ...            NaN                            NaN   \n",
       "2        NaN  ...            NaN                            NaN   \n",
       "3        NaN  ...            NaN                            NaN   \n",
       "4        NaN  ...            NaN                            NaN   \n",
       "...      ...  ...            ...                            ...   \n",
       "19432    NaN  ...            NaN                            NaN   \n",
       "19433    NaN  ...            NaN                            NaN   \n",
       "19434    NaN  ...            NaN                            NaN   \n",
       "19435    NaN  ...            NaN                            NaN   \n",
       "19436    NaN  ...            NaN                            NaN   \n",
       "\n",
       "       Hippocampus_reduction_per_year  wholebrain_reduction_per_year  \\\n",
       "0                                 NaN                            NaN   \n",
       "1                                 NaN                            NaN   \n",
       "2                                 NaN                            NaN   \n",
       "3                                 NaN                            NaN   \n",
       "4                                 NaN                            NaN   \n",
       "...                               ...                            ...   \n",
       "19432                             NaN                            NaN   \n",
       "19433                             NaN                            NaN   \n",
       "19434                             NaN                            NaN   \n",
       "19435                             NaN                            NaN   \n",
       "19436                             NaN                            NaN   \n",
       "\n",
       "       Entorhinal_reduction_per_year  Fusiform_reduction_per_year  \\\n",
       "0                                NaN                          NaN   \n",
       "1                                NaN                          NaN   \n",
       "2                                NaN                          NaN   \n",
       "3                                NaN                          NaN   \n",
       "4                                NaN                          NaN   \n",
       "...                              ...                          ...   \n",
       "19432                            NaN                          NaN   \n",
       "19433                            NaN                          NaN   \n",
       "19434                            NaN                          NaN   \n",
       "19435                            NaN                          NaN   \n",
       "19436                            NaN                          NaN   \n",
       "\n",
       "       ICV_reduction_per_year  ABETA_reduction_per_year  \\\n",
       "0                         NaN                       NaN   \n",
       "1                         NaN                       NaN   \n",
       "2                         NaN                       NaN   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "...                       ...                       ...   \n",
       "19432                     NaN                       NaN   \n",
       "19433                     NaN                       NaN   \n",
       "19434                     NaN                       NaN   \n",
       "19435                     NaN                       NaN   \n",
       "19436                     NaN                       NaN   \n",
       "\n",
       "      TAU_reduction_per_year  PTAU_reduction_per_year  \n",
       "0                        NaN                      NaN  \n",
       "1                        NaN                      NaN  \n",
       "2                        NaN                      NaN  \n",
       "3                        NaN                      NaN  \n",
       "4                        NaN                      NaN  \n",
       "...                      ...                      ...  \n",
       "19432                    NaN                      NaN  \n",
       "19433                    NaN                      NaN  \n",
       "19434                    NaN                      NaN  \n",
       "19435                    NaN                      NaN  \n",
       "19436                    NaN                      NaN  \n",
       "\n",
       "[19437 rows x 37 columns]"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_brain_dxch = pd.read_csv('sleep_brain_dxch.csv').iloc[:,1:].drop(['NPIKSEV'],axis=1)\n",
    "sleep_brain_dxch = sleep_brain_dxch[sleep_brain_dxch['DXCHANGE'].notna()].reset_index().drop(['index'],axis=1)   # keep the rows where DXCHANGE is not nan\n",
    "sleep_brain_dxch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b7f3e",
   "metadata": {},
   "source": [
    "### sleep______VS______DXCHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "b9b829bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase</th>\n",
       "      <th>RID</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>DXCHANGE</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>NPIK6</th>\n",
       "      <th>NPIK7</th>\n",
       "      <th>NPIK8</th>\n",
       "      <th>NPIK9A</th>\n",
       "      <th>NPIK9B</th>\n",
       "      <th>NPIK9C</th>\n",
       "      <th>NPIKTOT</th>\n",
       "      <th>insomnia</th>\n",
       "      <th>OSA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>8</td>\n",
       "      <td>v06</td>\n",
       "      <td>011_S_0008</td>\n",
       "      <td>CN-CN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>8</td>\n",
       "      <td>v41</td>\n",
       "      <td>011_S_0008</td>\n",
       "      <td>CN-MCI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>31</td>\n",
       "      <td>v06</td>\n",
       "      <td>023_S_0031</td>\n",
       "      <td>CN-CN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>31</td>\n",
       "      <td>v11</td>\n",
       "      <td>023_S_0031</td>\n",
       "      <td>CN-CN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>31</td>\n",
       "      <td>v21</td>\n",
       "      <td>023_S_0031</td>\n",
       "      <td>CN-CN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>ADNI3</td>\n",
       "      <td>6890</td>\n",
       "      <td>y1</td>\n",
       "      <td>021_S_6890</td>\n",
       "      <td>MCI-MCI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>ADNI3</td>\n",
       "      <td>6891</td>\n",
       "      <td>y1</td>\n",
       "      <td>123_S_6891</td>\n",
       "      <td>AD-AD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>ADNI3</td>\n",
       "      <td>6897</td>\n",
       "      <td>y1</td>\n",
       "      <td>036_S_6897</td>\n",
       "      <td>MCI-MCI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>ADNI3</td>\n",
       "      <td>6947</td>\n",
       "      <td>y1</td>\n",
       "      <td>035_S_6947</td>\n",
       "      <td>MCI-MCI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>ADNI3</td>\n",
       "      <td>6962</td>\n",
       "      <td>y1</td>\n",
       "      <td>941_S_6962</td>\n",
       "      <td>AD-AD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Phase   RID VISCODE        PTID DXCHANGE  NPIK1  NPIK2  NPIK3  NPIK4  \\\n",
       "0    ADNI2     8     v06  011_S_0008    CN-CN    1.0    0.0    0.0    0.0   \n",
       "1    ADNI2     8     v41  011_S_0008   CN-MCI    0.0    0.0    0.0    0.0   \n",
       "2    ADNI2    31     v06  023_S_0031    CN-CN    0.0    1.0    0.0    0.0   \n",
       "3    ADNI2    31     v11  023_S_0031    CN-CN    0.0    1.0    0.0    0.0   \n",
       "4    ADNI2    31     v21  023_S_0031    CN-CN    0.0    0.0    0.0    1.0   \n",
       "..     ...   ...     ...         ...      ...    ...    ...    ...    ...   \n",
       "902  ADNI3  6890      y1  021_S_6890  MCI-MCI    0.0    1.0    0.0    0.0   \n",
       "903  ADNI3  6891      y1  123_S_6891    AD-AD    1.0    1.0    0.0    1.0   \n",
       "904  ADNI3  6897      y1  036_S_6897  MCI-MCI    0.0    0.0    0.0    1.0   \n",
       "905  ADNI3  6947      y1  035_S_6947  MCI-MCI    0.0    1.0    0.0    0.0   \n",
       "906  ADNI3  6962      y1  941_S_6962    AD-AD    0.0    1.0    0.0    0.0   \n",
       "\n",
       "     NPIK5  NPIK6  NPIK7  NPIK8  NPIK9A  NPIK9B  NPIK9C  NPIKTOT  insomnia  \\\n",
       "0      0.0    0.0    0.0    0.0     3.0     2.0     0.0      6.0       1.0   \n",
       "1      0.0    0.0    1.0    0.0     4.0     2.0     1.0      8.0       1.0   \n",
       "2      0.0    1.0    0.0    0.0     2.0     1.0     0.0      2.0       1.0   \n",
       "3      0.0    1.0    0.0    0.0     2.0     1.0     0.0      2.0       1.0   \n",
       "4      0.0    0.0    0.0    0.0     3.0     1.0     1.0      3.0       1.0   \n",
       "..     ...    ...    ...    ...     ...     ...     ...      ...       ...   \n",
       "902    0.0    0.0    0.0    0.0     4.0     1.0     1.0      4.0       1.0   \n",
       "903    0.0    0.0    1.0    0.0     4.0     2.0     3.0      8.0       1.0   \n",
       "904    0.0    0.0    0.0    1.0     1.0     1.0     1.0      1.0       1.0   \n",
       "905    0.0    0.0    1.0    0.0     1.0     1.0     1.0      1.0       1.0   \n",
       "906    0.0    0.0    0.0    1.0     3.0     2.0     2.0      6.0       1.0   \n",
       "\n",
       "     OSA  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  \n",
       "..   ...  \n",
       "902  1.0  \n",
       "903  0.0  \n",
       "904  0.0  \n",
       "905  0.0  \n",
       "906  0.0  \n",
       "\n",
       "[907 rows x 19 columns]"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_lst = [ 'DXCHANGE','NPIK1', 'NPIK2', 'NPIK3', 'NPIK4', 'NPIK5', 'NPIK6', 'NPIK7', 'NPIK8',\n",
    "       'NPIK9A', 'NPIK9B', 'NPIK9C', 'NPIKTOT',  'insomnia','OSA']\n",
    "sleep_dxch = sleep_brain_dxch[com_col + col_lst].set_index(['Phase', 'RID', 'VISCODE','PTID']).dropna(how='any',axis=0).reset_index()\n",
    "sleep_dxch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "c75224ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phase       0\n",
       "RID         0\n",
       "VISCODE     0\n",
       "PTID        0\n",
       "DXCHANGE    0\n",
       "NPIK1       0\n",
       "NPIK2       0\n",
       "NPIK3       0\n",
       "NPIK4       0\n",
       "NPIK5       0\n",
       "NPIK6       0\n",
       "NPIK7       0\n",
       "NPIK8       0\n",
       "NPIK9A      0\n",
       "NPIK9B      0\n",
       "NPIK9C      0\n",
       "NPIKTOT     0\n",
       "insomnia    0\n",
       "OSA         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sleep_dxch.isna())   # check nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "af84a781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase</th>\n",
       "      <th>RID</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>NPIK6</th>\n",
       "      <th>NPIK7</th>\n",
       "      <th>NPIK8</th>\n",
       "      <th>NPIK9A</th>\n",
       "      <th>NPIK9B</th>\n",
       "      <th>NPIK9C</th>\n",
       "      <th>NPIKTOT</th>\n",
       "      <th>insomnia</th>\n",
       "      <th>OSA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DXCHANGE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AD-AD</th>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AD-MCI</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-AD</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-CN</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-MCI</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-AD</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-CN</th>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-MCI</th>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Phase  RID  VISCODE  PTID  NPIK1  NPIK2  NPIK3  NPIK4  NPIK5  NPIK6  \\\n",
       "DXCHANGE                                                                        \n",
       "AD-AD       146  146      146   146    146    146    146    146    146    146   \n",
       "AD-MCI        2    2        2     2      2      2      2      2      2      2   \n",
       "CN-AD         2    2        2     2      2      2      2      2      2      2   \n",
       "CN-CN       201  201      201   201    201    201    201    201    201    201   \n",
       "CN-MCI       28   28       28    28     28     28     28     28     28     28   \n",
       "MCI-AD       77   77       77    77     77     77     77     77     77     77   \n",
       "MCI-CN       41   41       41    41     41     41     41     41     41     41   \n",
       "MCI-MCI     410  410      410   410    410    410    410    410    410    410   \n",
       "\n",
       "          NPIK7  NPIK8  NPIK9A  NPIK9B  NPIK9C  NPIKTOT  insomnia  OSA  \n",
       "DXCHANGE                                                                \n",
       "AD-AD       146    146     146     146     146      146       146  146  \n",
       "AD-MCI        2      2       2       2       2        2         2    2  \n",
       "CN-AD         2      2       2       2       2        2         2    2  \n",
       "CN-CN       201    201     201     201     201      201       201  201  \n",
       "CN-MCI       28     28      28      28      28       28        28   28  \n",
       "MCI-AD       77     77      77      77      77       77        77   77  \n",
       "MCI-CN       41     41      41      41      41       41        41   41  \n",
       "MCI-MCI     410    410     410     410     410      410       410  410  "
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_dxch.groupby('DXCHANGE').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae317f9",
   "metadata": {},
   "source": [
    "- select only the MCI-AD, MCI-MCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "cb2e3309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487, 19)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_dxch_2g = sleep_dxch.loc[sleep_dxch['DXCHANGE'].isin(['MCI-AD','MCI-MCI'])].reset_index().drop(['index'],axis=1)\n",
    "sleep_dxch_2g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "40b9cf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528, 19)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_dxch_3g = sleep_dxch.loc[sleep_dxch['DXCHANGE'].isin(['MCI-AD','MCI-MCI','MCI-CN'])].reset_index().drop(['index'],axis=1)\n",
    "sleep_dxch_3g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522d6ff",
   "metadata": {},
   "source": [
    "- drop DXCHANGE labels 'AD-MCI','CN-AD','CN-CN' then undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "23461b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832, 19)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_dxch_6g = sleep_dxch[(sleep_dxch['DXCHANGE'].isin(['CN-MCI', 'AD-AD', 'MCI-MCI', 'MCI-AD', 'MCI-CN']))].reset_index().drop(['index'],axis=1)\n",
    "sleep_dxch_6g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39783bf",
   "metadata": {},
   "source": [
    "### oversampling and undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd62ca",
   "metadata": {},
   "source": [
    "- functions\n",
    "    - models(df,drop_lst,target) : under sampling, split, scale, pca, models\n",
    "    - cv_models(df,drop_lst,target,k): under sampling, NOT SPLIT, scale, pca, models with cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "e94f2878",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_lst = ['Phase', 'RID', 'VISCODE', 'PTID','DXCHANGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "216b25f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 154 ; Resampled dataset shape Counter({'MCI-AD': 77, 'MCI-MCI': 77})\n",
      "\n",
      "10 principle components are needed to explain 90% of the data\n",
      "\n",
      "Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.654, Test set f1-score: 0.423\n",
      "          - saga_L1, Training set f1-score:0.648, Test set f1-score: 0.735\n",
      "          - newton-cg_L2, Training set f1-score:0.654, Test set f1-score: 0.423\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.581, Test set f1-score: 0.492\n",
      "          - saga_L1, Training set f1-score:0.648, Test set f1-score: 0.735\n",
      "          - newton-cg_L2, Training set f1-score:0.581, Test set f1-score: 0.492\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.606, Test set f1-score: 0.555\n",
      "          - saga_L1, Training set f1-score:0.578, Test set f1-score: 0.443\n",
      "          - newton-cg_L2, Training set f1-score:0.606, Test set f1-score: 0.555\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.653, Test set f1-score: 0.519\n",
      "          - saga_L1, Training set f1-score:0.647, Test set f1-score: 0.519\n",
      "          - newton-cg_L2, Training set f1-score:0.653, Test set f1-score: 0.519\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.685, Test set f1-score: 0.519\n",
      "          - saga_L1, Training set f1-score:0.685, Test set f1-score: 0.519\n",
      "          - newton-cg_L2, Training set f1-score:0.685, Test set f1-score: 0.519\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.684, Test set f1-score: 0.484\n",
      "          - saga_L1, Training set f1-score:0.684, Test set f1-score: 0.519\n",
      "          - newton-cg_L2, Training set f1-score:0.684, Test set f1-score: 0.484\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.684, Test set f1-score: 0.484\n",
      "          - saga_L1, Training set f1-score:0.676, Test set f1-score: 0.519\n",
      "          - newton-cg_L2, Training set f1-score:0.684, Test set f1-score: 0.484\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.678 f1-score on test data: 0.452\n",
      "          - tree depth: 2.000. f1-score on training data: 0.706 f1-score on test data: 0.452\n",
      "          - tree depth: 3.000. f1-score on training data: 0.715 f1-score on test data: 0.452\n",
      "          - tree depth: 4.000. f1-score on training data: 0.724 f1-score on test data: 0.613\n",
      "          - tree depth: 5.000. f1-score on training data: 0.768 f1-score on test data: 0.578\n",
      "          - tree depth: 6.000. f1-score on training data: 0.823 f1-score on test data: 0.546\n",
      "          - tree depth: 7.000. f1-score on training data: 0.847 f1-score on test data: 0.578\n",
      "          - tree depth: 8.000. f1-score on training data: 0.927 f1-score on test data: 0.484\n",
      "          - tree depth: 9.000. f1-score on training data: 0.935 f1-score on test data: 0.546\n",
      "          - tree depth: 10.000. f1-score on training data: 0.959 f1-score on test data: 0.583\n",
      "          - tree depth: 11.000. f1-score on training data: 0.959 f1-score on test data: 0.546\n",
      "          - tree depth: 12.000. f1-score on training data: 0.968 f1-score on test data: 0.546\n",
      "          - tree depth: 13.000. f1-score on training data: 0.968 f1-score on test data: 0.546\n",
      "          - tree depth: 14.000. f1-score on training data: 0.976 f1-score on test data: 0.546\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.919 f1-score on test data: 0.514\n",
      "          - 10trees. f1-score on training data: 0.919 f1-score on test data: 0.448\n",
      "          - 15trees. f1-score on training data: 0.951 f1-score on test data: 0.448\n",
      "          - 20trees. f1-score on training data: 0.951 f1-score on test data: 0.514\n",
      "          - 25trees. f1-score on training data: 0.959 f1-score on test data: 0.514\n",
      "          - 30trees. f1-score on training data: 0.968 f1-score on test data: 0.514\n",
      "          - 35trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 40trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 45trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 50trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 55trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 60trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 65trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 70trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 75trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 80trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 85trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 90trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 95trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.976 f1-score on test data: 0.613\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.976 f1-score on test data: 0.482\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.976 f1-score on test data: 0.546\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.976 f1-score on test data: 0.513\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.682, Test set f1-score: 0.591\n",
      "          - saga_L1, Training set f1-score:0.684, Test set f1-score: 0.591\n",
      "          - newton-cg_L2, Training set f1-score:0.682, Test set f1-score: 0.591\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.635, Test set f1-score: 0.519\n",
      "          - saga_L1, Training set f1-score:0.648, Test set f1-score: 0.735\n",
      "          - newton-cg_L2, Training set f1-score:0.635, Test set f1-score: 0.519\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.612, Test set f1-score: 0.519\n",
      "          - saga_L1, Training set f1-score:0.653, Test set f1-score: 0.482\n",
      "          - newton-cg_L2, Training set f1-score:0.612, Test set f1-score: 0.519\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.685, Test set f1-score: 0.519\n",
      "          - saga_L1, Training set f1-score:0.669, Test set f1-score: 0.519\n",
      "          - newton-cg_L2, Training set f1-score:0.685, Test set f1-score: 0.519\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.692, Test set f1-score: 0.519\n",
      "          - saga_L1, Training set f1-score:0.685, Test set f1-score: 0.519\n",
      "          - newton-cg_L2, Training set f1-score:0.692, Test set f1-score: 0.519\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.676, Test set f1-score: 0.484\n",
      "          - saga_L1, Training set f1-score:0.676, Test set f1-score: 0.484\n",
      "          - newton-cg_L2, Training set f1-score:0.676, Test set f1-score: 0.484\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.684, Test set f1-score: 0.484\n",
      "          - saga_L1, Training set f1-score:0.684, Test set f1-score: 0.484\n",
      "          - newton-cg_L2, Training set f1-score:0.684, Test set f1-score: 0.484\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.678 f1-score on test data: 0.452\n",
      "          - tree depth: 2.000. f1-score on training data: 0.706 f1-score on test data: 0.452\n",
      "          - tree depth: 3.000. f1-score on training data: 0.715 f1-score on test data: 0.452\n",
      "          - tree depth: 4.000. f1-score on training data: 0.724 f1-score on test data: 0.613\n",
      "          - tree depth: 5.000. f1-score on training data: 0.768 f1-score on test data: 0.578\n",
      "          - tree depth: 6.000. f1-score on training data: 0.823 f1-score on test data: 0.546\n",
      "          - tree depth: 7.000. f1-score on training data: 0.847 f1-score on test data: 0.578\n",
      "          - tree depth: 8.000. f1-score on training data: 0.927 f1-score on test data: 0.484\n",
      "          - tree depth: 9.000. f1-score on training data: 0.935 f1-score on test data: 0.546\n",
      "          - tree depth: 10.000. f1-score on training data: 0.959 f1-score on test data: 0.583\n",
      "          - tree depth: 11.000. f1-score on training data: 0.959 f1-score on test data: 0.546\n",
      "          - tree depth: 12.000. f1-score on training data: 0.968 f1-score on test data: 0.546\n",
      "          - tree depth: 13.000. f1-score on training data: 0.968 f1-score on test data: 0.546\n",
      "          - tree depth: 14.000. f1-score on training data: 0.976 f1-score on test data: 0.546\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.919 f1-score on test data: 0.546\n",
      "          - 10trees. f1-score on training data: 0.919 f1-score on test data: 0.448\n",
      "          - 15trees. f1-score on training data: 0.951 f1-score on test data: 0.448\n",
      "          - 20trees. f1-score on training data: 0.951 f1-score on test data: 0.514\n",
      "          - 25trees. f1-score on training data: 0.959 f1-score on test data: 0.514\n",
      "          - 30trees. f1-score on training data: 0.968 f1-score on test data: 0.514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 35trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 40trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 45trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 50trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 55trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 60trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 65trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 70trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 75trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 80trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 85trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 90trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "          - 95trees. f1-score on training data: 0.976 f1-score on test data: 0.514\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.976 f1-score on test data: 0.519\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.976 f1-score on test data: 0.449\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.976 f1-score on test data: 0.546\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.976 f1-score on test data: 0.419\n",
      "- Using 10 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.681, Test set f1-score: 0.591\n",
      "          - saga_L1, Training set f1-score:0.648, Test set f1-score: 0.735\n",
      "          - newton-cg_L2, Training set f1-score:0.681, Test set f1-score: 0.591\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.601, Test set f1-score: 0.484\n",
      "          - saga_L1, Training set f1-score:0.648, Test set f1-score: 0.735\n",
      "          - newton-cg_L2, Training set f1-score:0.601, Test set f1-score: 0.484\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.629, Test set f1-score: 0.592\n",
      "          - saga_L1, Training set f1-score:0.548, Test set f1-score: 0.519\n",
      "          - newton-cg_L2, Training set f1-score:0.629, Test set f1-score: 0.592\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "          - saga_L1, Training set f1-score:0.628, Test set f1-score: 0.592\n",
      "          - newton-cg_L2, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "          - saga_L1, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "          - newton-cg_L2, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "          - saga_L1, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "          - newton-cg_L2, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "          - saga_L1, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "          - newton-cg_L2, Training set f1-score:0.578, Test set f1-score: 0.592\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.694 f1-score on test data: 0.371\n",
      "          - tree depth: 2.000. f1-score on training data: 0.721 f1-score on test data: 0.371\n",
      "          - tree depth: 3.000. f1-score on training data: 0.805 f1-score on test data: 0.416\n",
      "          - tree depth: 4.000. f1-score on training data: 0.862 f1-score on test data: 0.417\n",
      "          - tree depth: 5.000. f1-score on training data: 0.919 f1-score on test data: 0.448\n",
      "          - tree depth: 6.000. f1-score on training data: 0.943 f1-score on test data: 0.351\n",
      "          - tree depth: 7.000. f1-score on training data: 0.951 f1-score on test data: 0.387\n",
      "          - tree depth: 8.000. f1-score on training data: 0.968 f1-score on test data: 0.352\n",
      "          - tree depth: 9.000. f1-score on training data: 0.976 f1-score on test data: 0.287\n",
      "          - tree depth: 10.000. f1-score on training data: 0.976 f1-score on test data: 0.287\n",
      "          - tree depth: 11.000. f1-score on training data: 0.976 f1-score on test data: 0.287\n",
      "          - tree depth: 12.000. f1-score on training data: 0.976 f1-score on test data: 0.287\n",
      "          - tree depth: 13.000. f1-score on training data: 0.976 f1-score on test data: 0.287\n",
      "          - tree depth: 14.000. f1-score on training data: 0.976 f1-score on test data: 0.287\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.935 f1-score on test data: 0.514\n",
      "          - 10trees. f1-score on training data: 0.959 f1-score on test data: 0.481\n",
      "          - 15trees. f1-score on training data: 0.959 f1-score on test data: 0.546\n",
      "          - 20trees. f1-score on training data: 0.967 f1-score on test data: 0.514\n",
      "          - 25trees. f1-score on training data: 0.967 f1-score on test data: 0.579\n",
      "          - 30trees. f1-score on training data: 0.976 f1-score on test data: 0.513\n",
      "          - 35trees. f1-score on training data: 0.976 f1-score on test data: 0.546\n",
      "          - 40trees. f1-score on training data: 0.976 f1-score on test data: 0.449\n",
      "          - 45trees. f1-score on training data: 0.976 f1-score on test data: 0.481\n",
      "          - 50trees. f1-score on training data: 0.976 f1-score on test data: 0.513\n",
      "          - 55trees. f1-score on training data: 0.976 f1-score on test data: 0.513\n",
      "          - 60trees. f1-score on training data: 0.976 f1-score on test data: 0.481\n",
      "          - 65trees. f1-score on training data: 0.976 f1-score on test data: 0.481\n",
      "          - 70trees. f1-score on training data: 0.976 f1-score on test data: 0.449\n",
      "          - 75trees. f1-score on training data: 0.976 f1-score on test data: 0.481\n",
      "          - 80trees. f1-score on training data: 0.976 f1-score on test data: 0.449\n",
      "          - 85trees. f1-score on training data: 0.976 f1-score on test data: 0.481\n",
      "          - 90trees. f1-score on training data: 0.976 f1-score on test data: 0.449\n",
      "          - 95trees. f1-score on training data: 0.976 f1-score on test data: 0.481\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.976 f1-score on test data: 0.448\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.976 f1-score on test data: 0.615\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.976 f1-score on test data: 0.547\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.976 f1-score on test data: 0.610\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models(sleep_dxch_2g,drop_lst,'DXCHANGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "2f10aae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 154 ; Resampled dataset shape Counter({'MCI-AD': 77, 'MCI-MCI': 77})\n",
      "\n",
      "10 principle components are needed to explain 90% of the data\n",
      "\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.490\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.326\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.490\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.534\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.326\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.534\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.591\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.483\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.591\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.560\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.539\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.560\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.566\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.566\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.566\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.559\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.559\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.559\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.559\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.559\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.559\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.519\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.553\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.581\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.565\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.491\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.493\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.473\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.466\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.494\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.423\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.463\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.443\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.443\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.451\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.522\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.501\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.506\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.514\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.528\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.529\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.534\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.523\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.536\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.527\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.532\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.520\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.519\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.520\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.507\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.514\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.514\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.514\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.519\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.472\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.463\n",
      "          - hidden layer size[100, 100]. average weighted f1-score of 10-cross validation:0.518\n",
      "          - hidden layer size[50, 50, 50]. average weighted f1-score of 10-cross validation:0.454\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.509\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.334\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.509\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.532\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.326\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.532\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.563\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.500\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.563\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.572\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.565\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.572\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.559\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.566\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.559\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.559\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.559\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.559\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.559\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.559\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.559\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.519\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.553\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.581\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.565\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.491\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.493\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.473\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.466\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.494\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.423\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.463\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.443\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.443\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.451\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.522\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 15trees. average weighted f1-score of 10-cross validation:0.499\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.508\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.514\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.523\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.515\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.510\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.523\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.508\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.514\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.514\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.507\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.514\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.507\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.508\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.508\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.508\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.508\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.509\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.487\n",
      "          - hidden layer size[100, 100]. average weighted f1-score of 10-cross validation:0.491\n",
      "          - hidden layer size[50, 50, 50]. average weighted f1-score of 10-cross validation:0.458\n",
      "- Using 10 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.550\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.326\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.550\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.544\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.341\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.544\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.531\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.522\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.531\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.532\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.539\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.532\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.539\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.539\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.539\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.539\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.539\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.539\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.539\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.539\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.539\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.499\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.426\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.519\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.454\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.496\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.462\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.495\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.505\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.513\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.504\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.478\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.486\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.486\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.480\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.523\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.559\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.592\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.622\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.600\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.598\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.595\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.594\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.576\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.562\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.552\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.552\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.545\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.563\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.550\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.561\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.576\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.576\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.584\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.553\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.506\n",
      "          - hidden layer size[100, 100]. average weighted f1-score of 10-cross validation:0.532\n",
      "          - hidden layer size[50, 50, 50]. average weighted f1-score of 10-cross validation:0.540\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_models(sleep_dxch_2g,drop_lst,'DXCHANGE',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "80eed256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 123 ; Resampled dataset shape Counter({'MCI-AD': 41, 'MCI-CN': 41, 'MCI-MCI': 41})\n",
      "\n",
      "9 principle components are needed to explain 90% of the data\n",
      "\n",
      "Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.621, Test set f1-score: 0.447\n",
      "          - saga_L1, Training set f1-score:0.504, Test set f1-score: 0.485\n",
      "          - newton-cg_L2, Training set f1-score:0.621, Test set f1-score: 0.447\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.588, Test set f1-score: 0.396\n",
      "          - saga_L1, Training set f1-score:0.504, Test set f1-score: 0.485\n",
      "          - newton-cg_L2, Training set f1-score:0.588, Test set f1-score: 0.396\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.595, Test set f1-score: 0.440\n",
      "          - saga_L1, Training set f1-score:0.534, Test set f1-score: 0.482\n",
      "          - newton-cg_L2, Training set f1-score:0.595, Test set f1-score: 0.440\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.629, Test set f1-score: 0.457\n",
      "          - saga_L1, Training set f1-score:0.610, Test set f1-score: 0.458\n",
      "          - newton-cg_L2, Training set f1-score:0.629, Test set f1-score: 0.457\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.629, Test set f1-score: 0.416\n",
      "          - saga_L1, Training set f1-score:0.629, Test set f1-score: 0.416\n",
      "          - newton-cg_L2, Training set f1-score:0.629, Test set f1-score: 0.416\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.618, Test set f1-score: 0.413\n",
      "          - saga_L1, Training set f1-score:0.627, Test set f1-score: 0.416\n",
      "          - newton-cg_L2, Training set f1-score:0.618, Test set f1-score: 0.413\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.618, Test set f1-score: 0.413\n",
      "          - saga_L1, Training set f1-score:0.627, Test set f1-score: 0.413\n",
      "          - newton-cg_L2, Training set f1-score:0.618, Test set f1-score: 0.413\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.646 f1-score on test data: 0.440\n",
      "          - tree depth: 2.000. f1-score on training data: 0.625 f1-score on test data: 0.440\n",
      "          - tree depth: 3.000. f1-score on training data: 0.633 f1-score on test data: 0.369\n",
      "          - tree depth: 4.000. f1-score on training data: 0.700 f1-score on test data: 0.465\n",
      "          - tree depth: 5.000. f1-score on training data: 0.790 f1-score on test data: 0.359\n",
      "          - tree depth: 6.000. f1-score on training data: 0.837 f1-score on test data: 0.328\n",
      "          - tree depth: 7.000. f1-score on training data: 0.877 f1-score on test data: 0.367\n",
      "          - tree depth: 8.000. f1-score on training data: 0.917 f1-score on test data: 0.319\n",
      "          - tree depth: 9.000. f1-score on training data: 0.917 f1-score on test data: 0.272\n",
      "          - tree depth: 10.000. f1-score on training data: 0.938 f1-score on test data: 0.320\n",
      "          - tree depth: 11.000. f1-score on training data: 0.938 f1-score on test data: 0.280\n",
      "          - tree depth: 12.000. f1-score on training data: 0.938 f1-score on test data: 0.275\n",
      "          - tree depth: 13.000. f1-score on training data: 0.939 f1-score on test data: 0.327\n",
      "          - tree depth: 14.000. f1-score on training data: 0.939 f1-score on test data: 0.321\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.898 f1-score on test data: 0.220\n",
      "          - 10trees. f1-score on training data: 0.938 f1-score on test data: 0.301\n",
      "          - 15trees. f1-score on training data: 0.928 f1-score on test data: 0.337\n",
      "          - 20trees. f1-score on training data: 0.938 f1-score on test data: 0.374\n",
      "          - 25trees. f1-score on training data: 0.938 f1-score on test data: 0.339\n",
      "          - 30trees. f1-score on training data: 0.938 f1-score on test data: 0.366\n",
      "          - 35trees. f1-score on training data: 0.938 f1-score on test data: 0.337\n",
      "          - 40trees. f1-score on training data: 0.938 f1-score on test data: 0.367\n",
      "          - 45trees. f1-score on training data: 0.938 f1-score on test data: 0.367\n",
      "          - 50trees. f1-score on training data: 0.938 f1-score on test data: 0.372\n",
      "          - 55trees. f1-score on training data: 0.938 f1-score on test data: 0.335\n",
      "          - 60trees. f1-score on training data: 0.938 f1-score on test data: 0.335\n",
      "          - 65trees. f1-score on training data: 0.938 f1-score on test data: 0.372\n",
      "          - 70trees. f1-score on training data: 0.938 f1-score on test data: 0.372\n",
      "          - 75trees. f1-score on training data: 0.938 f1-score on test data: 0.372\n",
      "          - 80trees. f1-score on training data: 0.938 f1-score on test data: 0.344\n",
      "          - 85trees. f1-score on training data: 0.938 f1-score on test data: 0.344\n",
      "          - 90trees. f1-score on training data: 0.938 f1-score on test data: 0.345\n",
      "          - 95trees. f1-score on training data: 0.938 f1-score on test data: 0.344\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.939 f1-score on test data: 0.396\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.939 f1-score on test data: 0.288\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.938 f1-score on test data: 0.412\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.939 f1-score on test data: 0.440\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.620, Test set f1-score: 0.588\n",
      "          - saga_L1, Training set f1-score:0.504, Test set f1-score: 0.485\n",
      "          - newton-cg_L2, Training set f1-score:0.620, Test set f1-score: 0.588\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.593, Test set f1-score: 0.529\n",
      "          - saga_L1, Training set f1-score:0.504, Test set f1-score: 0.485\n",
      "          - newton-cg_L2, Training set f1-score:0.593, Test set f1-score: 0.529\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.657, Test set f1-score: 0.434\n",
      "          - saga_L1, Training set f1-score:0.601, Test set f1-score: 0.448\n",
      "          - newton-cg_L2, Training set f1-score:0.657, Test set f1-score: 0.434\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.637, Test set f1-score: 0.457\n",
      "          - saga_L1, Training set f1-score:0.635, Test set f1-score: 0.457\n",
      "          - newton-cg_L2, Training set f1-score:0.637, Test set f1-score: 0.457\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.629, Test set f1-score: 0.416\n",
      "          - saga_L1, Training set f1-score:0.627, Test set f1-score: 0.416\n",
      "          - newton-cg_L2, Training set f1-score:0.629, Test set f1-score: 0.416\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.618, Test set f1-score: 0.413\n",
      "          - saga_L1, Training set f1-score:0.618, Test set f1-score: 0.413\n",
      "          - newton-cg_L2, Training set f1-score:0.618, Test set f1-score: 0.413\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.618, Test set f1-score: 0.413\n",
      "          - saga_L1, Training set f1-score:0.618, Test set f1-score: 0.413\n",
      "          - newton-cg_L2, Training set f1-score:0.618, Test set f1-score: 0.413\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.646 f1-score on test data: 0.440\n",
      "          - tree depth: 2.000. f1-score on training data: 0.625 f1-score on test data: 0.440\n",
      "          - tree depth: 3.000. f1-score on training data: 0.633 f1-score on test data: 0.369\n",
      "          - tree depth: 4.000. f1-score on training data: 0.700 f1-score on test data: 0.465\n",
      "          - tree depth: 5.000. f1-score on training data: 0.790 f1-score on test data: 0.359\n",
      "          - tree depth: 6.000. f1-score on training data: 0.837 f1-score on test data: 0.328\n",
      "          - tree depth: 7.000. f1-score on training data: 0.877 f1-score on test data: 0.367\n",
      "          - tree depth: 8.000. f1-score on training data: 0.917 f1-score on test data: 0.319\n",
      "          - tree depth: 9.000. f1-score on training data: 0.917 f1-score on test data: 0.272\n",
      "          - tree depth: 10.000. f1-score on training data: 0.938 f1-score on test data: 0.320\n",
      "          - tree depth: 11.000. f1-score on training data: 0.938 f1-score on test data: 0.280\n",
      "          - tree depth: 12.000. f1-score on training data: 0.938 f1-score on test data: 0.275\n",
      "          - tree depth: 13.000. f1-score on training data: 0.939 f1-score on test data: 0.327\n",
      "          - tree depth: 14.000. f1-score on training data: 0.939 f1-score on test data: 0.321\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.898 f1-score on test data: 0.272\n",
      "          - 10trees. f1-score on training data: 0.938 f1-score on test data: 0.301\n",
      "          - 15trees. f1-score on training data: 0.928 f1-score on test data: 0.337\n",
      "          - 20trees. f1-score on training data: 0.938 f1-score on test data: 0.374\n",
      "          - 25trees. f1-score on training data: 0.938 f1-score on test data: 0.339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 30trees. f1-score on training data: 0.938 f1-score on test data: 0.366\n",
      "          - 35trees. f1-score on training data: 0.938 f1-score on test data: 0.337\n",
      "          - 40trees. f1-score on training data: 0.938 f1-score on test data: 0.367\n",
      "          - 45trees. f1-score on training data: 0.938 f1-score on test data: 0.367\n",
      "          - 50trees. f1-score on training data: 0.938 f1-score on test data: 0.372\n",
      "          - 55trees. f1-score on training data: 0.938 f1-score on test data: 0.335\n",
      "          - 60trees. f1-score on training data: 0.938 f1-score on test data: 0.335\n",
      "          - 65trees. f1-score on training data: 0.938 f1-score on test data: 0.372\n",
      "          - 70trees. f1-score on training data: 0.938 f1-score on test data: 0.372\n",
      "          - 75trees. f1-score on training data: 0.938 f1-score on test data: 0.372\n",
      "          - 80trees. f1-score on training data: 0.938 f1-score on test data: 0.344\n",
      "          - 85trees. f1-score on training data: 0.938 f1-score on test data: 0.344\n",
      "          - 90trees. f1-score on training data: 0.938 f1-score on test data: 0.345\n",
      "          - 95trees. f1-score on training data: 0.938 f1-score on test data: 0.344\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.938 f1-score on test data: 0.405\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.938 f1-score on test data: 0.399\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.938 f1-score on test data: 0.306\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.940 f1-score on test data: 0.348\n",
      "- Using 9 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.618, Test set f1-score: 0.537\n",
      "          - saga_L1, Training set f1-score:0.504, Test set f1-score: 0.485\n",
      "          - newton-cg_L2, Training set f1-score:0.618, Test set f1-score: 0.537\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.600, Test set f1-score: 0.482\n",
      "          - saga_L1, Training set f1-score:0.504, Test set f1-score: 0.485\n",
      "          - newton-cg_L2, Training set f1-score:0.600, Test set f1-score: 0.482\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.628, Test set f1-score: 0.389\n",
      "          - saga_L1, Training set f1-score:0.590, Test set f1-score: 0.487\n",
      "          - newton-cg_L2, Training set f1-score:0.628, Test set f1-score: 0.389\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "          - saga_L1, Training set f1-score:0.628, Test set f1-score: 0.438\n",
      "          - newton-cg_L2, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "          - saga_L1, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "          - newton-cg_L2, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "          - saga_L1, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "          - newton-cg_L2, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "          - saga_L1, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "          - newton-cg_L2, Training set f1-score:0.636, Test set f1-score: 0.389\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.634 f1-score on test data: 0.489\n",
      "          - tree depth: 2.000. f1-score on training data: 0.625 f1-score on test data: 0.358\n",
      "          - tree depth: 3.000. f1-score on training data: 0.706 f1-score on test data: 0.416\n",
      "          - tree depth: 4.000. f1-score on training data: 0.781 f1-score on test data: 0.335\n",
      "          - tree depth: 5.000. f1-score on training data: 0.818 f1-score on test data: 0.282\n",
      "          - tree depth: 6.000. f1-score on training data: 0.867 f1-score on test data: 0.331\n",
      "          - tree depth: 7.000. f1-score on training data: 0.919 f1-score on test data: 0.320\n",
      "          - tree depth: 8.000. f1-score on training data: 0.939 f1-score on test data: 0.316\n",
      "          - tree depth: 9.000. f1-score on training data: 0.939 f1-score on test data: 0.325\n",
      "          - tree depth: 10.000. f1-score on training data: 0.939 f1-score on test data: 0.325\n",
      "          - tree depth: 11.000. f1-score on training data: 0.939 f1-score on test data: 0.316\n",
      "          - tree depth: 12.000. f1-score on training data: 0.939 f1-score on test data: 0.356\n",
      "          - tree depth: 13.000. f1-score on training data: 0.939 f1-score on test data: 0.356\n",
      "          - tree depth: 14.000. f1-score on training data: 0.939 f1-score on test data: 0.356\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.908 f1-score on test data: 0.468\n",
      "          - 10trees. f1-score on training data: 0.939 f1-score on test data: 0.408\n",
      "          - 15trees. f1-score on training data: 0.939 f1-score on test data: 0.422\n",
      "          - 20trees. f1-score on training data: 0.938 f1-score on test data: 0.459\n",
      "          - 25trees. f1-score on training data: 0.939 f1-score on test data: 0.317\n",
      "          - 30trees. f1-score on training data: 0.939 f1-score on test data: 0.317\n",
      "          - 35trees. f1-score on training data: 0.939 f1-score on test data: 0.317\n",
      "          - 40trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "          - 45trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "          - 50trees. f1-score on training data: 0.939 f1-score on test data: 0.360\n",
      "          - 55trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "          - 60trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "          - 65trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "          - 70trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "          - 75trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "          - 80trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "          - 85trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "          - 90trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "          - 95trees. f1-score on training data: 0.938 f1-score on test data: 0.360\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.938 f1-score on test data: 0.337\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.938 f1-score on test data: 0.318\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.938 f1-score on test data: 0.244\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.938 f1-score on test data: 0.260\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models(sleep_dxch_3g,drop_lst,'DXCHANGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c4fa8",
   "metadata": {},
   "source": [
    "normal data: random forest: 85trees. f1-score on training data: 0.969 f1-score on test data: 0.780\n",
    "normal data: random forest: 85trees. f1-score on training data: 0.969 f1-score on test data: 0.780\n",
    "\n",
    "pca:decision tree: tree depth: 6.000. f1-score on training data: 0.891 f1-score on test data: 0.810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "0071686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 123 ; Resampled dataset shape Counter({'MCI-AD': 41, 'MCI-CN': 41, 'MCI-MCI': 41})\n",
      "\n",
      "9 principle components are needed to explain 90% of the data\n",
      "\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.433\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.160\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.433\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.476\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.167\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.476\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.440\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.419\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.440\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.456\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.465\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.456\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.443\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.441\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.443\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.431\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.442\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.431\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.421\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.431\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.421\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.398\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.421\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.386\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.391\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.349\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.332\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.347\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.361\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.359\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.400\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.355\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.365\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.355\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.338\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.387\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.369\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.439\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.407\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.381\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.365\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.350\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.342\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.351\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.360\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.369\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.368\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.388\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.388\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.370\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.370\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.370\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.370\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.361\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.346\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.348\n",
      "          - hidden layer size[100, 100]. average weighted f1-score of 10-cross validation:0.366\n",
      "          - hidden layer size[50, 50, 50]. average weighted f1-score of 10-cross validation:0.328\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.406\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.174\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.406\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.428\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.167\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.428\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.430\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.452\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.430\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.463\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.450\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.463\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.440\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.442\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.431\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.431\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.432\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.421\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.398\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.421\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.386\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.391\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.349\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.332\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.347\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.361\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.359\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.400\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.355\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.365\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.355\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.338\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.389\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.369\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.439\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.407\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.368\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.355\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.345\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.331\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.340\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.349\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.358\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.357\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.369\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.369\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.358\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.358\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.359\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.359\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.350\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.283\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.360\n",
      "          - hidden layer size[100, 100]. average weighted f1-score of 10-cross validation:0.342\n",
      "          - hidden layer size[50, 50, 50]. average weighted f1-score of 10-cross validation:0.316\n",
      "- Using 9 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.432\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.167\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.432\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.438\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.174\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.438\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.435\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.462\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.435\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.439\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.463\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.439\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.437\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.445\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.437\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.437\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.437\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\weipi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.444\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.437\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.345\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.415\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.388\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.375\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.290\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.348\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.348\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.342\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.317\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.320\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.310\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.324\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.332\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.332\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.361\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.393\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.408\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.452\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.443\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.417\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.390\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.393\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.415\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.418\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.400\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.400\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.383\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.380\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.410\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.410\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.406\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.393\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.405\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.340\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.363\n",
      "          - hidden layer size[100, 100]. average weighted f1-score of 10-cross validation:0.357\n",
      "          - hidden layer size[50, 50, 50]. average weighted f1-score of 10-cross validation:0.338\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_models(sleep_dxch_3g,drop_lst,'DXCHANGE',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "a1f8203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 190 ; Resampled dataset shape Counter({'AD-AD': 38, 'CN-MCI': 38, 'MCI-AD': 38, 'MCI-CN': 38, 'MCI-MCI': 38})\n",
      "\n",
      "10 principle components are needed to explain 90% of the data\n",
      "\n",
      "Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.442, Test set f1-score: 0.242\n",
      "          - saga_L1, Training set f1-score:0.357, Test set f1-score: 0.233\n",
      "          - newton-cg_L2, Training set f1-score:0.442, Test set f1-score: 0.242\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.441, Test set f1-score: 0.249\n",
      "          - saga_L1, Training set f1-score:0.357, Test set f1-score: 0.233\n",
      "          - newton-cg_L2, Training set f1-score:0.441, Test set f1-score: 0.249\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.429, Test set f1-score: 0.230\n",
      "          - saga_L1, Training set f1-score:0.380, Test set f1-score: 0.248\n",
      "          - newton-cg_L2, Training set f1-score:0.429, Test set f1-score: 0.230\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.413, Test set f1-score: 0.255\n",
      "          - saga_L1, Training set f1-score:0.414, Test set f1-score: 0.267\n",
      "          - newton-cg_L2, Training set f1-score:0.413, Test set f1-score: 0.255\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "          - saga_L1, Training set f1-score:0.425, Test set f1-score: 0.274\n",
      "          - newton-cg_L2, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "          - saga_L1, Training set f1-score:0.426, Test set f1-score: 0.268\n",
      "          - newton-cg_L2, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "          - saga_L1, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "          - newton-cg_L2, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.442 f1-score on test data: 0.242\n",
      "          - tree depth: 2.000. f1-score on training data: 0.417 f1-score on test data: 0.216\n",
      "          - tree depth: 3.000. f1-score on training data: 0.415 f1-score on test data: 0.258\n",
      "          - tree depth: 4.000. f1-score on training data: 0.460 f1-score on test data: 0.314\n",
      "          - tree depth: 5.000. f1-score on training data: 0.550 f1-score on test data: 0.235\n",
      "          - tree depth: 6.000. f1-score on training data: 0.576 f1-score on test data: 0.223\n",
      "          - tree depth: 7.000. f1-score on training data: 0.644 f1-score on test data: 0.223\n",
      "          - tree depth: 8.000. f1-score on training data: 0.686 f1-score on test data: 0.268\n",
      "          - tree depth: 9.000. f1-score on training data: 0.747 f1-score on test data: 0.273\n",
      "          - tree depth: 10.000. f1-score on training data: 0.783 f1-score on test data: 0.269\n",
      "          - tree depth: 11.000. f1-score on training data: 0.811 f1-score on test data: 0.274\n",
      "          - tree depth: 12.000. f1-score on training data: 0.861 f1-score on test data: 0.253\n",
      "          - tree depth: 13.000. f1-score on training data: 0.883 f1-score on test data: 0.259\n",
      "          - tree depth: 14.000. f1-score on training data: 0.902 f1-score on test data: 0.221\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.855 f1-score on test data: 0.272\n",
      "          - 10trees. f1-score on training data: 0.894 f1-score on test data: 0.299\n",
      "          - 15trees. f1-score on training data: 0.902 f1-score on test data: 0.266\n",
      "          - 20trees. f1-score on training data: 0.914 f1-score on test data: 0.244\n",
      "          - 25trees. f1-score on training data: 0.914 f1-score on test data: 0.303\n",
      "          - 30trees. f1-score on training data: 0.914 f1-score on test data: 0.293\n",
      "          - 35trees. f1-score on training data: 0.914 f1-score on test data: 0.243\n",
      "          - 40trees. f1-score on training data: 0.914 f1-score on test data: 0.243\n",
      "          - 45trees. f1-score on training data: 0.914 f1-score on test data: 0.238\n",
      "          - 50trees. f1-score on training data: 0.914 f1-score on test data: 0.238\n",
      "          - 55trees. f1-score on training data: 0.914 f1-score on test data: 0.262\n",
      "          - 60trees. f1-score on training data: 0.914 f1-score on test data: 0.265\n",
      "          - 65trees. f1-score on training data: 0.914 f1-score on test data: 0.269\n",
      "          - 70trees. f1-score on training data: 0.914 f1-score on test data: 0.294\n",
      "          - 75trees. f1-score on training data: 0.914 f1-score on test data: 0.275\n",
      "          - 80trees. f1-score on training data: 0.915 f1-score on test data: 0.273\n",
      "          - 85trees. f1-score on training data: 0.914 f1-score on test data: 0.269\n",
      "          - 90trees. f1-score on training data: 0.914 f1-score on test data: 0.273\n",
      "          - 95trees. f1-score on training data: 0.914 f1-score on test data: 0.269\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.915 f1-score on test data: 0.165\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.914 f1-score on test data: 0.374\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.915 f1-score on test data: 0.250\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.915 f1-score on test data: 0.241\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.436, Test set f1-score: 0.282\n",
      "          - saga_L1, Training set f1-score:0.357, Test set f1-score: 0.233\n",
      "          - newton-cg_L2, Training set f1-score:0.436, Test set f1-score: 0.282\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.430, Test set f1-score: 0.216\n",
      "          - saga_L1, Training set f1-score:0.357, Test set f1-score: 0.233\n",
      "          - newton-cg_L2, Training set f1-score:0.430, Test set f1-score: 0.216\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.430, Test set f1-score: 0.251\n",
      "          - saga_L1, Training set f1-score:0.456, Test set f1-score: 0.316\n",
      "          - newton-cg_L2, Training set f1-score:0.430, Test set f1-score: 0.251\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.440, Test set f1-score: 0.268\n",
      "          - saga_L1, Training set f1-score:0.439, Test set f1-score: 0.272\n",
      "          - newton-cg_L2, Training set f1-score:0.440, Test set f1-score: 0.268\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.426, Test set f1-score: 0.268\n",
      "          - saga_L1, Training set f1-score:0.432, Test set f1-score: 0.274\n",
      "          - newton-cg_L2, Training set f1-score:0.426, Test set f1-score: 0.268\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "          - saga_L1, Training set f1-score:0.426, Test set f1-score: 0.268\n",
      "          - newton-cg_L2, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "          - saga_L1, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "          - newton-cg_L2, Training set f1-score:0.427, Test set f1-score: 0.268\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.442 f1-score on test data: 0.242\n",
      "          - tree depth: 2.000. f1-score on training data: 0.417 f1-score on test data: 0.216\n",
      "          - tree depth: 3.000. f1-score on training data: 0.415 f1-score on test data: 0.258\n",
      "          - tree depth: 4.000. f1-score on training data: 0.460 f1-score on test data: 0.314\n",
      "          - tree depth: 5.000. f1-score on training data: 0.550 f1-score on test data: 0.235\n",
      "          - tree depth: 6.000. f1-score on training data: 0.576 f1-score on test data: 0.223\n",
      "          - tree depth: 7.000. f1-score on training data: 0.644 f1-score on test data: 0.223\n",
      "          - tree depth: 8.000. f1-score on training data: 0.686 f1-score on test data: 0.268\n",
      "          - tree depth: 9.000. f1-score on training data: 0.747 f1-score on test data: 0.273\n",
      "          - tree depth: 10.000. f1-score on training data: 0.783 f1-score on test data: 0.269\n",
      "          - tree depth: 11.000. f1-score on training data: 0.811 f1-score on test data: 0.274\n",
      "          - tree depth: 12.000. f1-score on training data: 0.861 f1-score on test data: 0.253\n",
      "          - tree depth: 13.000. f1-score on training data: 0.883 f1-score on test data: 0.259\n",
      "          - tree depth: 14.000. f1-score on training data: 0.902 f1-score on test data: 0.221\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.855 f1-score on test data: 0.251\n",
      "          - 10trees. f1-score on training data: 0.894 f1-score on test data: 0.270\n",
      "          - 15trees. f1-score on training data: 0.902 f1-score on test data: 0.294\n",
      "          - 20trees. f1-score on training data: 0.914 f1-score on test data: 0.244\n",
      "          - 25trees. f1-score on training data: 0.914 f1-score on test data: 0.332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 30trees. f1-score on training data: 0.914 f1-score on test data: 0.294\n",
      "          - 35trees. f1-score on training data: 0.914 f1-score on test data: 0.239\n",
      "          - 40trees. f1-score on training data: 0.914 f1-score on test data: 0.271\n",
      "          - 45trees. f1-score on training data: 0.914 f1-score on test data: 0.237\n",
      "          - 50trees. f1-score on training data: 0.914 f1-score on test data: 0.237\n",
      "          - 55trees. f1-score on training data: 0.914 f1-score on test data: 0.271\n",
      "          - 60trees. f1-score on training data: 0.914 f1-score on test data: 0.265\n",
      "          - 65trees. f1-score on training data: 0.914 f1-score on test data: 0.270\n",
      "          - 70trees. f1-score on training data: 0.914 f1-score on test data: 0.271\n",
      "          - 75trees. f1-score on training data: 0.914 f1-score on test data: 0.271\n",
      "          - 80trees. f1-score on training data: 0.915 f1-score on test data: 0.270\n",
      "          - 85trees. f1-score on training data: 0.914 f1-score on test data: 0.270\n",
      "          - 90trees. f1-score on training data: 0.914 f1-score on test data: 0.270\n",
      "          - 95trees. f1-score on training data: 0.914 f1-score on test data: 0.266\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.914 f1-score on test data: 0.223\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.915 f1-score on test data: 0.382\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.915 f1-score on test data: 0.371\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.914 f1-score on test data: 0.225\n",
      "- Using 10 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.446, Test set f1-score: 0.282\n",
      "          - saga_L1, Training set f1-score:0.330, Test set f1-score: 0.348\n",
      "          - newton-cg_L2, Training set f1-score:0.446, Test set f1-score: 0.282\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.434, Test set f1-score: 0.225\n",
      "          - saga_L1, Training set f1-score:0.357, Test set f1-score: 0.233\n",
      "          - newton-cg_L2, Training set f1-score:0.434, Test set f1-score: 0.225\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.400, Test set f1-score: 0.255\n",
      "          - saga_L1, Training set f1-score:0.426, Test set f1-score: 0.215\n",
      "          - newton-cg_L2, Training set f1-score:0.400, Test set f1-score: 0.255\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.402, Test set f1-score: 0.283\n",
      "          - saga_L1, Training set f1-score:0.419, Test set f1-score: 0.255\n",
      "          - newton-cg_L2, Training set f1-score:0.402, Test set f1-score: 0.283\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.409, Test set f1-score: 0.283\n",
      "          - saga_L1, Training set f1-score:0.409, Test set f1-score: 0.283\n",
      "          - newton-cg_L2, Training set f1-score:0.409, Test set f1-score: 0.283\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.409, Test set f1-score: 0.283\n",
      "          - saga_L1, Training set f1-score:0.409, Test set f1-score: 0.283\n",
      "          - newton-cg_L2, Training set f1-score:0.409, Test set f1-score: 0.283\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.409, Test set f1-score: 0.283\n",
      "          - saga_L1, Training set f1-score:0.409, Test set f1-score: 0.283\n",
      "          - newton-cg_L2, Training set f1-score:0.409, Test set f1-score: 0.283\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.451 f1-score on test data: 0.242\n",
      "          - tree depth: 2.000. f1-score on training data: 0.468 f1-score on test data: 0.187\n",
      "          - tree depth: 3.000. f1-score on training data: 0.437 f1-score on test data: 0.160\n",
      "          - tree depth: 4.000. f1-score on training data: 0.507 f1-score on test data: 0.238\n",
      "          - tree depth: 5.000. f1-score on training data: 0.610 f1-score on test data: 0.291\n",
      "          - tree depth: 6.000. f1-score on training data: 0.649 f1-score on test data: 0.255\n",
      "          - tree depth: 7.000. f1-score on training data: 0.711 f1-score on test data: 0.302\n",
      "          - tree depth: 8.000. f1-score on training data: 0.785 f1-score on test data: 0.261\n",
      "          - tree depth: 9.000. f1-score on training data: 0.818 f1-score on test data: 0.333\n",
      "          - tree depth: 10.000. f1-score on training data: 0.839 f1-score on test data: 0.242\n",
      "          - tree depth: 11.000. f1-score on training data: 0.881 f1-score on test data: 0.227\n",
      "          - tree depth: 12.000. f1-score on training data: 0.901 f1-score on test data: 0.243\n",
      "          - tree depth: 13.000. f1-score on training data: 0.915 f1-score on test data: 0.350\n",
      "          - tree depth: 14.000. f1-score on training data: 0.915 f1-score on test data: 0.332\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.848 f1-score on test data: 0.322\n",
      "          - 10trees. f1-score on training data: 0.908 f1-score on test data: 0.236\n",
      "          - 15trees. f1-score on training data: 0.915 f1-score on test data: 0.248\n",
      "          - 20trees. f1-score on training data: 0.914 f1-score on test data: 0.240\n",
      "          - 25trees. f1-score on training data: 0.914 f1-score on test data: 0.233\n",
      "          - 30trees. f1-score on training data: 0.915 f1-score on test data: 0.252\n",
      "          - 35trees. f1-score on training data: 0.915 f1-score on test data: 0.240\n",
      "          - 40trees. f1-score on training data: 0.914 f1-score on test data: 0.240\n",
      "          - 45trees. f1-score on training data: 0.914 f1-score on test data: 0.240\n",
      "          - 50trees. f1-score on training data: 0.914 f1-score on test data: 0.270\n",
      "          - 55trees. f1-score on training data: 0.914 f1-score on test data: 0.270\n",
      "          - 60trees. f1-score on training data: 0.914 f1-score on test data: 0.266\n",
      "          - 65trees. f1-score on training data: 0.915 f1-score on test data: 0.266\n",
      "          - 70trees. f1-score on training data: 0.915 f1-score on test data: 0.266\n",
      "          - 75trees. f1-score on training data: 0.915 f1-score on test data: 0.266\n",
      "          - 80trees. f1-score on training data: 0.915 f1-score on test data: 0.289\n",
      "          - 85trees. f1-score on training data: 0.915 f1-score on test data: 0.287\n",
      "          - 90trees. f1-score on training data: 0.914 f1-score on test data: 0.287\n",
      "          - 95trees. f1-score on training data: 0.914 f1-score on test data: 0.287\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.915 f1-score on test data: 0.304\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.914 f1-score on test data: 0.285\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.914 f1-score on test data: 0.318\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.914 f1-score on test data: 0.233\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models(sleep_dxch_6g,drop_lst,'DXCHANGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "66c83805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 190 ; Resampled dataset shape Counter({'AD-AD': 38, 'CN-MCI': 38, 'MCI-AD': 38, 'MCI-CN': 38, 'MCI-MCI': 38})\n",
      "\n",
      "10 principle components are needed to explain 90% of the data\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-445-d0b01c2e9c4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleep_dxch_6g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DXCHANGE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-443-b756bd2ab768>\u001b[0m in \u001b[0;36mcv_models\u001b[1;34m(df, drop_lst, target, k)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfolds\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     '''\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0musampling_scale_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Output dataframes sequence: X_,X_scaled,X_pca,y_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-443-b756bd2ab768>\u001b[0m in \u001b[0;36musampling_scale_data\u001b[1;34m(df, drop_lst, target)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;31m#print('Output dataframes sequence: X_,X_scaled,X_pca,y_')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mX_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'original dataset'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'scaled dataset'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'%s pca-components'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mn_com\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_pca\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcv_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_models(sleep_dxch_6g,drop_lst,'DXCHANGE',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e387e3",
   "metadata": {},
   "source": [
    "## best score: newton-cg_L2,C:1, Training set score:0.711, Test set score: 0.688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc4bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0f51b52",
   "metadata": {},
   "source": [
    "## sleep vs final_diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "a670079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Phase</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>NPIK6</th>\n",
       "      <th>...</th>\n",
       "      <th>ratio_PTAU_bl</th>\n",
       "      <th>Ventricles_reduction_per_year</th>\n",
       "      <th>Hippocampus_reduction_per_year</th>\n",
       "      <th>wholebrain_reduction_per_year</th>\n",
       "      <th>Entorhinal_reduction_per_year</th>\n",
       "      <th>Fusiform_reduction_per_year</th>\n",
       "      <th>ICV_reduction_per_year</th>\n",
       "      <th>ABETA_reduction_per_year</th>\n",
       "      <th>TAU_reduction_per_year</th>\n",
       "      <th>PTAU_reduction_per_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>sc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m06</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m36</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m60</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m66</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24379</th>\n",
       "      <td>679</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m186</td>\n",
       "      <td>041_S_0679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.073308</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>-0.006792</td>\n",
       "      <td>-0.007425</td>\n",
       "      <td>0.004080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24380</th>\n",
       "      <td>4401</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>m120</td>\n",
       "      <td>014_S_4401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.055952</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>-0.013059</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24381</th>\n",
       "      <td>4644</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>m108</td>\n",
       "      <td>003_S_4644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.032266</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>-0.005400</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>-0.002509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24382</th>\n",
       "      <td>5100</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>m108</td>\n",
       "      <td>041_S_5100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.077513</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>-0.014872</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24383</th>\n",
       "      <td>5282</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>m84</td>\n",
       "      <td>082_S_5282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24384 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RID  Phase VISCODE        PTID  NPIK1  NPIK2  NPIK3  NPIK4  NPIK5  \\\n",
       "0         2  ADNI1      sc         NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1         2  ADNI1     m06  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "2         2  ADNI1     m36  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "3         2  ADNI1     m60  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "4         2  ADNI1     m66  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "...     ...    ...     ...         ...    ...    ...    ...    ...    ...   \n",
       "24379   679  ADNI1    m186  041_S_0679    NaN    NaN    NaN    NaN    NaN   \n",
       "24380  4401  ADNI2    m120  014_S_4401    NaN    NaN    NaN    NaN    NaN   \n",
       "24381  4644  ADNI2    m108  003_S_4644    NaN    NaN    NaN    NaN    NaN   \n",
       "24382  5100  ADNI2    m108  041_S_5100    NaN    NaN    NaN    NaN    NaN   \n",
       "24383  5282  ADNI2     m84  082_S_5282    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "       NPIK6  ...  ratio_PTAU_bl  Ventricles_reduction_per_year  \\\n",
       "0        NaN  ...            NaN                            NaN   \n",
       "1        NaN  ...            NaN                            NaN   \n",
       "2        NaN  ...            NaN                            NaN   \n",
       "3        NaN  ...            NaN                            NaN   \n",
       "4        NaN  ...            NaN                            NaN   \n",
       "...      ...  ...            ...                            ...   \n",
       "24379    NaN  ...            NaN                      -0.073308   \n",
       "24380    NaN  ...            NaN                      -0.055952   \n",
       "24381    NaN  ...            NaN                      -0.032266   \n",
       "24382    NaN  ...            NaN                      -0.077513   \n",
       "24383    NaN  ...            NaN                            NaN   \n",
       "\n",
       "       Hippocampus_reduction_per_year  wholebrain_reduction_per_year  \\\n",
       "0                                 NaN                            NaN   \n",
       "1                                 NaN                            NaN   \n",
       "2                                 NaN                            NaN   \n",
       "3                                 NaN                            NaN   \n",
       "4                                 NaN                            NaN   \n",
       "...                               ...                            ...   \n",
       "24379                        0.004114                       0.003316   \n",
       "24380                        0.006005                       0.007223   \n",
       "24381                        0.004784                       0.001844   \n",
       "24382                        0.010280                       0.007247   \n",
       "24383                        0.004950                            NaN   \n",
       "\n",
       "       Entorhinal_reduction_per_year  Fusiform_reduction_per_year  \\\n",
       "0                                NaN                          NaN   \n",
       "1                                NaN                          NaN   \n",
       "2                                NaN                          NaN   \n",
       "3                                NaN                          NaN   \n",
       "4                                NaN                          NaN   \n",
       "...                              ...                          ...   \n",
       "24379                      -0.006792                    -0.007425   \n",
       "24380                      -0.013059                     0.004802   \n",
       "24381                      -0.005400                     0.011367   \n",
       "24382                      -0.014872                     0.006389   \n",
       "24383                            NaN                          NaN   \n",
       "\n",
       "       ICV_reduction_per_year  ABETA_reduction_per_year  \\\n",
       "0                         NaN                       NaN   \n",
       "1                         NaN                       NaN   \n",
       "2                         NaN                       NaN   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "...                       ...                       ...   \n",
       "24379                0.004080                       NaN   \n",
       "24380                0.002871                       NaN   \n",
       "24381               -0.002509                       NaN   \n",
       "24382                0.005107                       NaN   \n",
       "24383                0.003868                       NaN   \n",
       "\n",
       "       TAU_reduction_per_year PTAU_reduction_per_year  \n",
       "0                         NaN                     NaN  \n",
       "1                         NaN                     NaN  \n",
       "2                         NaN                     NaN  \n",
       "3                         NaN                     NaN  \n",
       "4                         NaN                     NaN  \n",
       "...                       ...                     ...  \n",
       "24379                     NaN                     NaN  \n",
       "24380                     NaN                     NaN  \n",
       "24381                     NaN                     NaN  \n",
       "24382                     NaN                     NaN  \n",
       "24383                     NaN                     NaN  \n",
       "\n",
       "[24384 rows x 38 columns]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_brain_finaldxch = pd.read_csv('sleep_brain_finaldxch.csv',sep=',').iloc[:,1:]\n",
    "sleep_brain_finaldxch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "571078b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RID', 'Phase', 'VISCODE', 'PTID', 'NPIK1', 'NPIK2', 'NPIK3', 'NPIK4',\n",
       "       'NPIK5', 'NPIK6', 'NPIK7', 'NPIK8', 'NPIK9A', 'NPIK9B', 'NPIK9C',\n",
       "       'NPIKTOT', 'NPIKSEV', 'insomnia', 'OSA', 'final_dxch',\n",
       "       'ratio_Ventricles_bl', 'ratio_Hippocampus_bl', 'ratio_WholeBrain_bl',\n",
       "       'ratio_Entorhinal_bl', 'ratio_Fusiform_bl', 'ratio_ICV_bl',\n",
       "       'ratio_ABETA_bl', 'ratio_TAU_bl', 'ratio_PTAU_bl',\n",
       "       'Ventricles_reduction_per_year', 'Hippocampus_reduction_per_year',\n",
       "       'wholebrain_reduction_per_year', 'Entorhinal_reduction_per_year',\n",
       "       'Fusiform_reduction_per_year', 'ICV_reduction_per_year',\n",
       "       'ABETA_reduction_per_year', 'TAU_reduction_per_year',\n",
       "       'PTAU_reduction_per_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_brain_finaldxch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e9d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=54, ncols=3, figsize=(18,280))\n",
    "axes = axes.ravel()  # array to 1D\n",
    "for j in range(len(sleep_lst)):\n",
    "    for i in range(len(cat_lst)):\n",
    "        axes[i+j*9].scatter(sleep_brain_v[sleep_lst[j]], sleep_brain_v[cat_lst[i]])\n",
    "        axes[i+j*9].set(title=f'{sleep_lst[j]} vs {cat_lst[i]}', xlabel=None)   \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00f82b",
   "metadata": {},
   "source": [
    "NPIKSEV, insomnia, OSA have data available to correlate to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40f6dc",
   "metadata": {},
   "source": [
    "NPIKSEV vs brain volume ratio to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f470319",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_brain_v = sleep_brain_v.drop(['NPIK1', 'NPIK2', 'NPIK3', 'NPIK4',\n",
    "       'NPIK5', 'NPIK6', 'NPIK7', 'NPIK8', 'NPIK9A', 'NPIK9B', 'NPIK9C',\n",
    "       'NPIKTOT'],axis=1)\n",
    "sleep_brain_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f166fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_brain_red = sleep_brain_v.dropna(how='any',axis=0).reset_index().drop(['index'],axis=1)  # reduced\n",
    "sleep_brain_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_brain_red.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(sleep_brain_red[['NPIKSEV', 'insomnia', 'OSA',\n",
    "       'ratio_Ventricles_bl', 'ratio_Hippocampus_bl', 'ratio_WholeBrain_bl',\n",
    "       'ratio_Entorhinal_bl', 'ratio_Fusiform_bl', 'ratio_ICV_bl',\n",
    "       'ratio_ABETA_bl', 'ratio_TAU_bl', 'ratio_PTAU_bl']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_brain_red.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(sleep_brain_red,random_state=586,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffccbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7227c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9636cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['NPIKSEV', 'insomnia', 'OSA',\n",
    "       'ratio_ABETA_bl', 'ratio_TAU_bl', 'ratio_PTAU_bl']]\n",
    "X_test = test[['NPIKSEV', 'insomnia', 'OSA',\n",
    "       'ratio_ABETA_bl', 'ratio_TAU_bl', 'ratio_PTAU_bl']]\n",
    "y_lst = ['Ventricles_reduction_per_year', 'Hippocampus_reduction_per_year',\n",
    "       'wholebrain_reduction_per_year', 'Entorhinal_reduction_per_year',\n",
    "       'Fusiform_reduction_per_year', 'ICV_reduction_per_year',\n",
    "       'ABETA_reduction_per_year', 'TAU_reduction_per_year',\n",
    "       'PTAU_reduction_per_year','ratio_Ventricles_bl', 'ratio_Hippocampus_bl', 'ratio_WholeBrain_bl',\n",
    "       'ratio_Entorhinal_bl', 'ratio_Fusiform_bl', 'ratio_ICV_bl',\n",
    "       'ratio_ABETA_bl', 'ratio_TAU_bl', 'ratio_PTAU_bl']\n",
    "alpha_lst = [0.01,0.1,1,10]\n",
    "for i in range(len(y_lst)):\n",
    "    y_train = train[y_lst[i]]\n",
    "    y_test = test[y_lst[i]]\n",
    "    for j in range(len(alpha_lst)):\n",
    "        \n",
    "        ridge = Ridge(alpha = alpha_lst[j]).fit(X_train,y_train)\n",
    "        print('{}: target feature {}: alpha = {}; training set score: {:.3f}; test set score {:.3f}'.format('Ridge',y_lst[i],alpha_lst[j],lr.score(X_train,train_whole_brain),lr.score(X_test,test_whole_brain)))\n",
    "        lasso = Lasso(alpha = alpha_lst[j]).fit(X_train,y_train)\n",
    "        print('{}: target feature {}: alpha = {}; training set score: {:.3f}; test set score {:.3f}'.format('Lasso',y_lst[i],alpha_lst[j],lr.score(X_train,train_whole_brain),lr.score(X_test,test_whole_brain)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa934b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data to use PCA\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d796f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3) #keep the first two principal components of the data\n",
    "pca.fit(X_train_scaled) # fit PCA model to sleep-diagnosis_change data\n",
    "X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "plt.bar(range(1,len(pca.explained_variance_ratio_ )+1),pca.explained_variance_ratio_ )\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Components')\n",
    "plt.plot(range(1,len(pca.explained_variance_ )+1),\n",
    "             np.cumsum(pca.explained_variance_ratio_),\n",
    "             c='red',\n",
    "             label=\"Cumulative Explained Variance ratio\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# The component 1 can explain about 20% of the variance, conponent 2 can explain about 11.7%,... \n",
    "# It needs almost 10 principal components to explain at least 90% of the variance. \n",
    "pca = PCA(n_components=3) \n",
    "pca.fit(X_train_scaled) # fit PCA model to sleep-diagnosis_change data\n",
    "X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "print(\"Origianl shape: {}\".format(str(X_train_scaled.shape)))\n",
    "print(\"Reduced shape: {}\".format(str(X_pca.shape)))\n",
    "# plot\n",
    "plt.figure(figsize=(8,8))\n",
    "mglearn.discrete_scatter(X_pca[:,0],X_pca[:,1],train['wholebrain_reduction_per_year'])\n",
    "plt.legend(train['wholebrain_reduction_per_year'].unique(),loc='best')\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel('Seconde principal component')\n",
    "\n",
    "print('PCA components: \\n{}'.format(pca.components_))    # PCA components\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.matshow(pca.components_,cmap='viridis')\n",
    "plt.yticks([0,1,2,3],['First component','Seconde component','Third component','Fourth component'])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(X_train.columns)),X_train.columns,rotation=60,ha='left')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Principal components')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fc12b",
   "metadata": {},
   "source": [
    "## try to use all variables to predict diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c461ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file = pd.read_csv('main_file.csv',sep=',')\n",
    "main_2 = main_file[com_col + ['DX','DXCHANGE']]\n",
    "sleep_bio_brain_dx = sleep_brain_v.merge(main_2,how='left',on=com_col).dropna(axis=1, how='all')\n",
    "sleep_bio_brain_dx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1209347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sleep_bio_brain_dx = sleep_bio_brain_dx.dropna(how='any',axis=0)\n",
    "train,test = train_test_split(sleep_bio_brain_dx,random_state=586,test_size=0.25)\n",
    "X_train = train.drop(['Phase', 'RID', 'VISCODE','PTID','DX','DXCHANGE'],axis=1)\n",
    "X_test = test.drop(['Phase', 'RID', 'VISCODE','PTID','DX','DXCHANGE'],axis=1)\n",
    "y_lst = ['DX','DXCHANGE']\n",
    "for j in range(len(y_lst)):\n",
    "    y_train = train[y_lst[j]]\n",
    "    y_test = test[y_lst[j]]\n",
    "    print('target feature: {}'.format(y_lst[j]))\n",
    "    for i in range(1,14):\n",
    "        tree = DecisionTreeClassifier(random_state=5850,max_depth=i,criterion='gini')\n",
    "        tree.fit(X_train,y_train)\n",
    "        print('    Decision tree with unscaled data. tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(tree.predict(X_train),y_train,average='weighted'),f1_score(tree.predict(X_test),y_test,average='weighted')))\n",
    "        forest = RandomForestClassifier(n_estimators = 200, random_state = 560)\n",
    "        forest.fit(X_train,y_train)\n",
    "        print('    Random forest with unscaled data. n_estimators: 200. tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(forest.predict(X_train),y_train,average='weighted'),f1_score(forest.predict(X_test),y_test,average='weighted')))\n",
    "    # MLP with unscaled data\n",
    "    mlp = MLPClassifier(solver='lbfgs',random_state=785,hidden_layer_sizes = [100,100],max_iter=40000)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    print('    -MLP with uscaled data. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "      .format(f1_score(mlp.predict(X_train),y_train,average='weighted'),f1_score(mlp.predict(X_test),y_test,average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe89a0",
   "metadata": {},
   "source": [
    "For the target feature DXCHANGE tree depth of 3 works best. \n",
    "\n",
    "with f1-score on training data: 0.786 f1-score on test data: 0.606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled data \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "for j in range(len(y_lst)):\n",
    "    y_train = train[y_lst[j]]\n",
    "    y_test = test[y_lst[j]]\n",
    "    print('target feature: {}'.format(y_lst[j]))\n",
    "    for i in range(2,14):\n",
    "        tree = DecisionTreeClassifier(random_state=5850,max_depth=i,criterion='gini')\n",
    "        tree.fit(X_train_scaled,y_train)\n",
    "        print('    Decision tree with scaled data. tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(tree.predict(X_train_scaled),y_train,average='weighted'),f1_score(tree.predict(X_test_scaled),y_test,average='weighted')))\n",
    "        forest = RandomForestClassifier(n_estimators = 200, random_state = 560)\n",
    "        forest.fit(X_train_scaled,y_train)\n",
    "        print('    Random forest with scaled data. n_estimators: 200. tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(forest.predict(X_train_scaled),y_train,average='weighted'),f1_score(forest.predict(X_test_scaled),y_test,average='weighted')))\n",
    "    # MLP with scaled data\n",
    "    mlp = MLPClassifier(solver='lbfgs',random_state=785,hidden_layer_sizes = [200,200],max_iter=40000)\n",
    "    mlp.fit(X_train_scaled,y_train)\n",
    "    print('    -MLP with uscaled data. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "      .format(f1_score(mlp.predict(X_train_scaled),y_train,average='weighted'),f1_score(mlp.predict(X_test_scaled),y_test,average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6f377",
   "metadata": {},
   "source": [
    " DXCHANGE: Decision tree with scaled data. tree depth: 3.000. f1-score on training data: 0.786 f1-score on test data: 0.606\n",
    " \n",
    " DX: Decision tree with scaled data. tree depth: 2.000. f1-score on training data: 0.778 f1-score on test data: 0.487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f605dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=X_train_scaled.shape[1]) #keep the first two principal components of the data\n",
    "pca.fit(X_train_scaled) # fit PCA model to sleep-diagnosis_change data\n",
    "X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "plt.bar(range(1,len(pca.explained_variance_ratio_ )+1),pca.explained_variance_ratio_ )\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Components')\n",
    "plt.plot(range(1,len(pca.explained_variance_ )+1),\n",
    "             np.cumsum(pca.explained_variance_ratio_),\n",
    "             c='red',\n",
    "             label=\"Cumulative Explained Variance ratio\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8,8))\n",
    "mglearn.discrete_scatter(X_pca[:,0],X_pca[:,1],train['DXCHANGE'])\n",
    "plt.legend(train['DXCHANGE'].unique(),loc='best')\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel('Seconde principal component')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.matshow(pca.components_,cmap='viridis')\n",
    "plt.yticks([0,1,2,3,4,5,6],['First component','Seconde component','Third component','Fourth component','Fifth component','Sixth component','seventh component'])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(X_train.columns)),X_train.columns,rotation=60,ha='left')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Principal components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree with PCA data \n",
    "\n",
    "for k in range(len(y_lst)):\n",
    "    y_train = train[y_lst[k]]\n",
    "    y_test = test[y_lst[k]]\n",
    "    print('- target feature: {}'.format(y_lst[k]))\n",
    "    max_f1_test = 0\n",
    "    max_n_component = 0\n",
    "    tree_depth = 0\n",
    "    _f1_train = 0\n",
    "    for j in range(2,X_train_scaled.shape[1]):\n",
    "        pca = PCA(n_components=j) # keep the first j principal components of the data\n",
    "        pca.fit(X_train_scaled) # fit PCA model to sleep-diagnosis_change data\n",
    "        X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "        X_test_pca = pca.transform(X_test_scaled)\n",
    "        #print('   - Decision tree with PCA {} components:'.format(j))\n",
    "        \n",
    "        for i in range(1,14):\n",
    "            tree = DecisionTreeClassifier(random_state=580,max_depth=i,criterion='gini')\n",
    "            tree.fit(X_pca,y_train)\n",
    "            f1_score_test = f1_score(tree.predict(X_test_pca),y_test,average='weighted')\n",
    "            f1_score_train = f1_score(tree.predict(X_pca),y_train,average='weighted')\n",
    "            #print('        - tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'.format(i,f1_score_train,f1_score_test))\n",
    "            forest = RandomForestClassifier(n_estimators = 200, random_state = 560)\n",
    "        forest.fit(X_pca,y_train)\n",
    "        print('    Random forest with pca data. n_estimators: 200. tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(forest.predict(X_pca),y_train,average='weighted'),f1_score(forest.predict(X_test_pca),y_test,average='weighted')))\n",
    "        if f1_score_test >= max_f1_test:\n",
    "                max_f1_test = f1_score_test\n",
    "                _f1_train = f1_score_train\n",
    "                max_n_component = j\n",
    "                tree_depth = i\n",
    "    print('The decision tree model predicting target feature {} with {:.3f} components, tree-depth of {} has the best f1-score on test set {:.3f}, on train set{:.3f}.'\n",
    "          .format(y_lst[k],max_n_component,tree_depth, max_f1_test,_f1_train))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc3e2b",
   "metadata": {},
   "source": [
    "To predict feature DX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['DX']\n",
    "y_test = test['DX']\n",
    "clf = tree.DecisionTreeClassifier(random_state=580,max_depth=5,criterion='gini')\n",
    "clf = clf.fit(X_pca,y_train)\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "tree.plot_tree(clf,fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c199a",
   "metadata": {},
   "source": [
    "To predict feature DXCHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8d603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train,test = train_test_split(sleep_bio_brain_dx,random_state=586,test_size=0.25)\n",
    "X_train = train.drop(['Phase', 'RID', 'VISCODE','PTID','DX','DXCHANGE'],axis=1)\n",
    "X_test = test.drop(['Phase', 'RID', 'VISCODE','PTID','DX','DXCHANGE'],axis=1)\n",
    "feature_list = ['NPIKSEV', 'insomnia', 'OSA', 'ratio_Ventricles_bl',\n",
    "       'ratio_Hippocampus_bl', 'ratio_WholeBrain_bl', 'ratio_Entorhinal_bl',\n",
    "       'ratio_Fusiform_bl', 'ratio_ICV_bl', 'ratio_ABETA_bl', 'ratio_TAU_bl',\n",
    "       'ratio_PTAU_bl', 'Ventricles_reduction_per_year',\n",
    "       'Hippocampus_reduction_per_year', 'wholebrain_reduction_per_year',\n",
    "       'Entorhinal_reduction_per_year', 'Fusiform_reduction_per_year',\n",
    "       'ICV_reduction_per_year', 'ABETA_reduction_per_year',\n",
    "       'TAU_reduction_per_year', 'PTAU_reduction_per_year']\n",
    "y_train = train['DXCHANGE']\n",
    "y_test = test['DXCHANGE']\n",
    "clf = tree.DecisionTreeClassifier(random_state=5850,max_depth=3,criterion='gini')\n",
    "clf.fit(X_train,y_train)\n",
    "r = export_text(clf, feature_names=feature_list)\n",
    "print(r)\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "tree.plot_tree(clf,fontsize=15,feature_names=feature_list)\n",
    "\n",
    "print('    Decision tree with unscaled data. tree depth: 2. f1-score on training data: {0.786} f1-score on test data: {0.606}')        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
