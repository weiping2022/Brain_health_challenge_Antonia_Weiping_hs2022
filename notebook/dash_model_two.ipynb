{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f5d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import export_text\n",
    "import mglearn\n",
    "from dashboard_one import *\n",
    "from feature_selection import *\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8c0fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## models(df,drop_lst,target)\n",
    "\n",
    "def usampling_split_scale_data(df,drop_lst,target):\n",
    "    '''\n",
    "    undersampling data, split data (4:1),data scaling, pca components which could explains 90% data\n",
    "    ----------------------------------\n",
    "    df: the full dataframe\n",
    "    target: the target feature name\n",
    "    -----------\n",
    "    Outputs: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
    "       '''\n",
    "    # undersampling \n",
    "    X = df.copy()\n",
    "    y = X[target]\n",
    "    rus = RandomUnderSampler(random_state=432)\n",
    "    X_undersampled, y_unsampled = rus.fit_resample(X, y)\n",
    "    print('After undersampling data size is',len(X_undersampled),'; Resampled dataset shape %s' % Counter(y_unsampled))\n",
    "    # split data\n",
    "    train, test = train_test_split(X_undersampled,test_size=0.2)   \n",
    "    X_train = train.drop(drop_lst,axis=1)\n",
    "    y_train = train[target]\n",
    "    X_test = test.drop(drop_lst,axis=1)\n",
    "    y_test = test[target]\n",
    "    # data scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # pca components\n",
    "    pca = PCA(n_components=X_train_scaled.shape[1]) # keep all n principal components \n",
    "    pca.fit(X_train_scaled) # fit PCA model with scaled data\n",
    "    X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "    ex_ratio = pca.explained_variance_ratio_\n",
    "    cum_sum = 0\n",
    "    for i in range(len(ex_ratio)):\n",
    "        cum_sum += ex_ratio[i]\n",
    "        if cum_sum >= 0.9:  # if it could explain 90% of the data, then stop\n",
    "            break\n",
    "    n_com = i         \n",
    "         # PCA with first n_com components\n",
    "    pca = PCA(n_com)\n",
    "    pca.fit(X_train_scaled)\n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    '''   #plot \n",
    "    plt.bar(range(1,len(pca.explained_variance_ratio_ )+1),pca.explained_variance_ratio_ )\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Components')\n",
    "    plt.plot(range(1,len(pca.explained_variance_ )+1),\n",
    "             np.cumsum(pca.explained_variance_ratio_),\n",
    "             c='red',\n",
    "             label=\"Cumulative Explained Variance ratio\")\n",
    "    plt.legend(loc='upper left')'''\n",
    "    print('\\n{} principle components are needed to explain 90% of the data\\n'.format(n_com))  \n",
    "    print('Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test')\n",
    "    X_labels = ['original dataset','scaled dataset','%s pca-components'%n_com]\n",
    "    return X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test,X_labels  \n",
    "\n",
    "def models(df,drop_lst,target):\n",
    "    '''\n",
    "    for splitted data\n",
    "    '''\n",
    "    res = usampling_split_scale_data(df,drop_lst,target)\n",
    "    y_train = res[6]\n",
    "    y_test = res[7]\n",
    "    X_labels = res[8]\n",
    "    for i in range(len(X_labels)):\n",
    "        print('- Using {}:'.format(X_labels[i]))\n",
    "        X_train = res[2*i]\n",
    "        X_test = res[2*i+1]\n",
    "        # logistic regression\n",
    "        C_lst = [0.001,0.01,0.1,1,10,100,1000]\n",
    "        print('    - Logistic regression')\n",
    "        for i in range(len(C_lst)):\n",
    "            print('       - C = {}'.format(C_lst[i]))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='lbfgs',multi_class='auto',penalty='l2',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - lbfgs_L2, Training set f1-score:{:.3f}, Test set f1-score: {:.3f}'\n",
    "                  .format(f1_score(logreg.predict(X_train),y_train,average='weighted'),f1_score(logreg.predict(X_test),y_test,average='weighted')))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='saga',multi_class='auto',penalty='l1',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - saga_L1, Training set f1-score:{:.3f}, Test set f1-score: {:.3f}'\n",
    "                  .format(f1_score(logreg.predict(X_train),y_train,average='weighted'),f1_score(logreg.predict(X_test),y_test,average='weighted')))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='newton-cg',multi_class='auto',penalty='l2',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - newton-cg_L2, Training set f1-score:{:.3f}, Test set f1-score: {:.3f}'\n",
    "                  .format(f1_score(logreg.predict(X_train),y_train,average='weighted'),f1_score(logreg.predict(X_test),y_test,average='weighted')))\n",
    "\n",
    "        # decision tree\n",
    "        print('    - Decision tree')\n",
    "        for i in range(1,15):\n",
    "            dtree = DecisionTreeClassifier(random_state=0,max_depth=i,criterion='gini')\n",
    "            dtree.fit(X_train,y_train)\n",
    "            print('          - tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(dtree.predict(X_train),y_train,average='weighted'),f1_score(dtree.predict(X_test),y_test,average='weighted')))\n",
    "        # random forest\n",
    "        print('    - Random forest')\n",
    "        for i in range(1,20):   \n",
    "            m= 5*i\n",
    "            forest = RandomForestClassifier(n_estimators=m,random_state=5862)\n",
    "            forest.fit(X_train,y_train)\n",
    "            print('          - {}trees. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(m,f1_score(forest.predict(X_train),y_train,average='weighted'),f1_score(forest.predict(X_test),y_test,average='weighted')))\n",
    "        # MLP  \n",
    "        print('    - MLP')\n",
    "        hls = [[50,50],[20,20]] # hidden layer size  ,[100,100],[50,50,50]\n",
    "        for i in range(len(hls)):\n",
    "            mlp = MLPClassifier(solver='lbfgs',random_state=460,hidden_layer_sizes = hls[i],max_iter=20000)\n",
    "            mlp.fit(X_train,y_train)\n",
    "            print('          - hidden layer size{}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'.format(hls[i],f1_score(mlp.predict(X_train),y_train,average='weighted'),f1_score(mlp.predict(X_test),y_test,average='weighted'))) \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "##### cv_models(df,drop_lst,target,k)\n",
    "\n",
    "def usampling_scale_data(df,drop_lst,target):\n",
    "    '''\n",
    "    undersampling data, NOT SPLIT data (later use CROSS VALIDATION),data scaling, pca components which could explains 90% data\n",
    "    ----------------------------------\n",
    "    df: the full dataframe\n",
    "    drop_lst: drop the features which are not gonna be used in modeling, e.g. RID,...\n",
    "    target: the target feature name\n",
    "    -----------\n",
    "    Outputs: X,X_scaled,X_pca,y,X_labels\n",
    "       '''\n",
    "    # undersampling \n",
    "    y_output = df[target]\n",
    "    rus = RandomUnderSampler(random_state=432)\n",
    "    X_undersampled, y_unsampled = rus.fit_resample(df, y_output)\n",
    "    print('After undersampling data size is',len(X_undersampled),'; Resampled dataset shape %s' % Counter(y_unsampled)) \n",
    "    # feature list for the X \n",
    "    # normal input output data\n",
    "    X = X_undersampled.drop(drop_lst,axis=1)\n",
    "    y = X_undersampled[target]\n",
    "    # data scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    # pca components\n",
    "    pca = PCA(n_components=X_scaled.shape[1]) # keep all n principal components \n",
    "    pca.fit(X_scaled) # fit PCA model with scaled data\n",
    "    X_pca = pca.transform(X_scaled)  #transform data onto the first two principal components\n",
    "    ex_ratio = pca.explained_variance_ratio_\n",
    "    cum_sum = 0\n",
    "    for i in range(len(ex_ratio)):\n",
    "        cum_sum += ex_ratio[i]\n",
    "        if cum_sum >= 0.9:  # if it could explain 90% of the data, then stop\n",
    "            break\n",
    "    n_com = i         \n",
    "         # PCA with first n_com components\n",
    "    pca = PCA(n_com)\n",
    "    pca.fit(X_scaled)\n",
    "    X_pca = pca.transform(X_scaled)\n",
    "    '''   #plot \n",
    "    plt.bar(range(1,len(pca.explained_variance_ratio_ )+1),pca.explained_variance_ratio_ )\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Components')\n",
    "    plt.plot(range(1,len(pca.explained_variance_ )+1),\n",
    "             np.cumsum(pca.explained_variance_ratio_),\n",
    "             c='red',\n",
    "             label=\"Cumulative Explained Variance ratio\")\n",
    "    plt.legend(loc='upper left')'''\n",
    "    print('\\n{} principle components are needed to explain 90% of the data\\n'.format(n_com))  \n",
    "    #print('Output dataframes sequence: X_,X_scaled,X_pca,y_')\n",
    "    X_labels = ['original dataset','scaled dataset','%s pca-components'%n_com]\n",
    "    return X,X_scaled,X_pca,y,X_labels \n",
    "\n",
    "def cv_models(df,drop_lst,target,k):\n",
    "    '''\n",
    "    df: full dataframe.\n",
    "    drop_lst: drop the features which are not gonna be used in modeling, e.g. RID,...\n",
    "    target: the target feature name\n",
    "    k: folds of cross-validation\n",
    "    '''\n",
    "    res = usampling_scale_data(df,drop_lst,target) # Output dataframes sequence: X_,X_scaled,X_pca,y_\n",
    "    \n",
    "    y = res[3]\n",
    "    X_labels = res[4]\n",
    "    for i in range(3):\n",
    "        X = res[i]\n",
    "        print('- Using {}:'.format(X_labels[i]))\n",
    "        # logistic regression\n",
    "        C_lst = [0.001,0.01,0.1,1,10,100,1000]\n",
    "        print('    - Logistic regression')\n",
    "        for i in range(len(C_lst)):\n",
    "            print('       - C = {}'.format(C_lst[i]))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='lbfgs',multi_class='auto',penalty='l2',max_iter=10000).fit(X,y)\n",
    "            print('          - lbfgs_L2, average weighted f1-score of {}-cross validation:{:.3f}'.format(k,cross_val_score(logreg, X, y, cv = k,scoring='f1_weighted').mean()))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='saga',multi_class='auto',penalty='l1',max_iter=10000).fit(X,y)\n",
    "            print('          - saga_L1, average weighted f1-score of {}-cross validation:{:.3f}'.format(k,cross_val_score(logreg, X, y, cv = k,scoring='f1_weighted').mean()))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='newton-cg',multi_class='auto',penalty='l2',max_iter=10000).fit(X,y)\n",
    "            print('          - newton-cg_L2, average weighted f1-score of {}-cross validation:{:.3f}'.format(k,cross_val_score(logreg, X, y, cv = k,scoring='f1_weighted').mean()))\n",
    "\n",
    "        # decision tree\n",
    "        print('    - Decision tree')\n",
    "        for i in range(1,15):\n",
    "            dtree = DecisionTreeClassifier(random_state=0,max_depth=i,criterion='gini')\n",
    "            dtree.fit(X,y)\n",
    "            print('          - tree depth: {:.3f}. average weighted f1-score of {}-cross validation:{:.3f}'\n",
    "                .format(i,k,cross_val_score(dtree, X, y, cv = k,scoring='f1_weighted').mean()))\n",
    "\n",
    "        # random forest\n",
    "        print('    - Random forest')\n",
    "        for i in range(1,20):   \n",
    "            m= 5*i\n",
    "            forest = RandomForestClassifier(n_estimators=m,random_state=5862)\n",
    "            forest.fit(X,y)\n",
    "            print('          - {}trees. average weighted f1-score of {}-cross validation:{:.3f}'\n",
    "                .format(m,k,cross_val_score(forest, X, y, cv = k,scoring='f1_weighted').mean()))\n",
    "        # MLP \n",
    "        print('    - MLP')\n",
    "        hls = [[50,50],[20,20]] # hidden layer size  ,[100,100]\n",
    "        for i in range(len(hls)):\n",
    "            mlp = MLPClassifier(solver='lbfgs',random_state=460,hidden_layer_sizes = hls[i],max_iter=20000).fit(X,y)\n",
    "            mlp.fit(X,y)\n",
    "            print('          - hidden layer size{}. average weighted f1-score of {}-cross validation:{:.3f}'.format(hls[i],k,cross_val_score(mlp, X, y, cv = k,scoring='f1_weighted').mean()))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c055ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance check\n",
    "def feature_importance(X,y,clf,k,title_label):\n",
    "    '''\n",
    "    check the feature importance of the selected classification (decisiontree or random forest) model.\n",
    "    X: input data\n",
    "    y: output data\n",
    "    clf: classification model, e.g.clf = RandomForestClassifier(n_estimators = 90, random_state = 5862) \n",
    "    Return\n",
    "    ------\n",
    "    dataframe of raw importance info\n",
    "    boxplot of importance\n",
    "    '''\n",
    "    output = cross_validate(clf, X, y, cv=k, scoring = 'f1_weighted', return_estimator =True)\n",
    "    d = {}  # dictionary to collect all importance dataframes \n",
    "    print(\"Features sorted by their score for each estimator \")\n",
    "    for idx,estimator in enumerate(output['estimator']):   \n",
    "        feature_importances = pd.DataFrame(estimator.feature_importances_,\n",
    "                                           index = X.columns,\n",
    "                                            columns=[\"importance_%s\"% (idx+1)])\n",
    "        d[idx] = feature_importances  \n",
    "    df = d[0]  # dataframe to concat all dataframes in d\n",
    "    for i in range(1,len(d)):\n",
    "        df = pd.concat([df,d[i]],axis=1)\n",
    "    df['avg_importance'] = df.mean(axis=1)\n",
    "    df = df.sort_values(by = ['avg_importance'], ascending = [False])\n",
    "    # insert avg_importance column as first column\n",
    "    df.insert(0, 'avg_importance', df.pop('avg_importance'))\n",
    "    # preparation for plotting\n",
    "    dff = df.T.reset_index().iloc[1:,1:] \n",
    "    # plot feature importance\n",
    "    bp = dff.boxplot(rot=30,figsize=(12,6),fontsize=12)\n",
    "    bp.set_ylabel('Feature Importance',fontsize=12)\n",
    "    bp.set_title('Feature importance of %s'%(title_label),fontsize=18)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
