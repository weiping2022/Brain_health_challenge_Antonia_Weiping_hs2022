{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347dd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import export_text\n",
    "import mglearn\n",
    "from dashboard_one import *\n",
    "from feature_selection import *\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "## models_os(df,drop_lst,target)\n",
    "\n",
    "def oversampling_split_scale_data(df,drop_lst,target):\n",
    "    '''\n",
    "    oversampling data, split data (4:1),data scaling, pca components which could explains 90% data\n",
    "    ----------------------------------\n",
    "    df: the full dataframe\n",
    "    target: the target feature name\n",
    "    -----------\n",
    "    Outputs: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
    "       '''    \n",
    "    # split data\n",
    "    drop_lst_2 = drop_lst[0:-1]\n",
    "    train, test = train_test_split(df,test_size=0.2)  \n",
    "    X_train = train.drop(drop_lst,axis=1)\n",
    "    y_train = train[target]\n",
    "    X_test = test.drop(drop_lst,axis=1)\n",
    "    y_test = test[target]\n",
    "\n",
    "    # oversampling\n",
    "    ros = RandomOverSampler(random_state=432)\n",
    "    X_oversampled,y_oversampled = ros.fit_resample(X_train,y_train)\n",
    "    print('After oversampling, train data size is',len(X_oversampled),'; Resampled dataset shape %s' % Counter(y_oversampled))\n",
    "    \n",
    "    # data scaling\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_oversampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    # pca components\n",
    "    pca = PCA(n_components=X_train_scaled.shape[1]) # keep all n principal components \n",
    "    pca.fit(X_train_scaled) # fit PCA model with scaled data\n",
    "    X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "    ex_ratio = pca.explained_variance_ratio_\n",
    "    cum_sum = 0\n",
    "    for i in range(len(ex_ratio)):\n",
    "        cum_sum += ex_ratio[i]\n",
    "        if cum_sum >= 0.9:  # if it could explain 90% of the data, then stop\n",
    "            break\n",
    "    n_com = i         \n",
    "         # PCA with first n_com components\n",
    "    pca = PCA(n_com)\n",
    "    pca.fit(X_train_scaled)\n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    '''   #plot \n",
    "    plt.bar(range(1,len(pca.explained_variance_ratio_ )+1),pca.explained_variance_ratio_ )\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Components')\n",
    "    plt.plot(range(1,len(pca.explained_variance_ )+1),\n",
    "             np.cumsum(pca.explained_variance_ratio_),\n",
    "             c='red',\n",
    "             label=\"Cumulative Explained Variance ratio\")\n",
    "    plt.legend(loc='upper left')'''\n",
    "    print('\\n{} principle components are needed to explain 90% of the data\\n'.format(n_com))  \n",
    "    print('Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test')\n",
    "    X_labels = ['original dataset','scaled dataset','%s pca-components'%n_com]\n",
    "    return X_oversampled,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_oversampled,y_test,X_labels  \n",
    "\n",
    "def models_os(df,drop_lst,target):\n",
    "    '''\n",
    "    for splitted data\n",
    "    '''\n",
    "    res = oversampling_split_scale_data(df,drop_lst,target)\n",
    "    y_train = res[6]\n",
    "    y_test = res[7]\n",
    "    X_labels = res[8]\n",
    "    for i in range(len(X_labels)):\n",
    "        print('- Using {}:'.format(X_labels[i]))\n",
    "        X_train = res[2*i]\n",
    "        X_test = res[2*i+1]\n",
    "        # logistic regression\n",
    "        C_lst = [0.001,0.01,0.1,1,10,100,1000]\n",
    "        print('    - Logistic regression')\n",
    "        for i in range(len(C_lst)):\n",
    "            print('       - C = {}'.format(C_lst[i]))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='lbfgs',multi_class='auto',penalty='l2',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - lbfgs_L2, Training set f1-score:{:.3f}, Test set f1-score: {:.3f}'\n",
    "                  .format(f1_score(logreg.predict(X_train),y_train,average='weighted'),f1_score(logreg.predict(X_test),y_test,average='weighted')))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='saga',multi_class='auto',penalty='l1',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - saga_L1, Training set f1-score:{:.3f}, Test set f1-score: {:.3f}'\n",
    "                  .format(f1_score(logreg.predict(X_train),y_train,average='weighted'),f1_score(logreg.predict(X_test),y_test,average='weighted')))\n",
    "            logreg = LogisticRegression(C=C_lst[i],solver='newton-cg',multi_class='auto',penalty='l2',max_iter=10000).fit(X_train,y_train)\n",
    "            print('          - newton-cg_L2, Training set f1-score:{:.3f}, Test set f1-score: {:.3f}'\n",
    "                  .format(f1_score(logreg.predict(X_train),y_train,average='weighted'),f1_score(logreg.predict(X_test),y_test,average='weighted')))\n",
    "\n",
    "        # decision tree\n",
    "        print('    - Decision tree')\n",
    "        for i in range(1,15):\n",
    "            dtree = DecisionTreeClassifier(random_state=0,max_depth=i,criterion='gini')\n",
    "            dtree.fit(X_train,y_train)\n",
    "            print('          - tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(dtree.predict(X_train),y_train,average='weighted'),f1_score(dtree.predict(X_test),y_test,average='weighted')))\n",
    "        # random forest\n",
    "        print('    - Random forest')\n",
    "        for i in range(1,20):   \n",
    "            m= 5*i\n",
    "            forest = RandomForestClassifier(n_estimators=m,random_state=5862)\n",
    "            forest.fit(X_train,y_train)\n",
    "            print('          - {}trees. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(m,f1_score(forest.predict(X_train),y_train,average='weighted'),f1_score(forest.predict(X_test),y_test,average='weighted')))\n",
    "        # MLP  \n",
    "        print('    - MLP')\n",
    "        hls = [[50,50],[20,20]] # hidden layer size  ,[100,100],[50,50,50]\n",
    "        for i in range(len(hls)):\n",
    "            mlp = MLPClassifier(solver='lbfgs',random_state=460,hidden_layer_sizes = hls[i],max_iter=20000)\n",
    "            mlp.fit(X_train,y_train)\n",
    "            print('          - hidden layer size{}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'.format(hls[i],f1_score(mlp.predict(X_train),y_train,average='weighted'),f1_score(mlp.predict(X_test),y_test,average='weighted'))) \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c770f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
