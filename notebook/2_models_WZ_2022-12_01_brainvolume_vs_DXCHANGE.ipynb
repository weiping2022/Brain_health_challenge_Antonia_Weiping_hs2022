{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f5d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import export_text\n",
    "import mglearn\n",
    "from dashboard_one import *\n",
    "from dash_model_two import *\n",
    "from feature_selection import *\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b74b94",
   "metadata": {},
   "source": [
    "### brain_volume_ratio_to_baseline_____VS_____diagnosischanges from every visit\n",
    "\n",
    "\n",
    "#### sleep_brain_dxch.csv\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef6c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_col = ['Phase', 'RID', 'VISCODE','PTID']\n",
    "target = 'DXCHANGE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "769bd041",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Phase</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>NPIK6</th>\n",
       "      <th>...</th>\n",
       "      <th>ratio_PTAU_bl</th>\n",
       "      <th>Ventricles_reduction_per_year</th>\n",
       "      <th>Hippocampus_reduction_per_year</th>\n",
       "      <th>wholebrain_reduction_per_year</th>\n",
       "      <th>Entorhinal_reduction_per_year</th>\n",
       "      <th>Fusiform_reduction_per_year</th>\n",
       "      <th>ICV_reduction_per_year</th>\n",
       "      <th>ABETA_reduction_per_year</th>\n",
       "      <th>TAU_reduction_per_year</th>\n",
       "      <th>PTAU_reduction_per_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m06</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m36</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m60</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m72</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16095</th>\n",
       "      <td>7083</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>126_S_7083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16096</th>\n",
       "      <td>7085</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>941_S_7085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16097</th>\n",
       "      <td>7088</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>033_S_7088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16098</th>\n",
       "      <td>7092</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>021_S_7092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16099</th>\n",
       "      <td>7100</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>033_S_7100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16100 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RID  Phase VISCODE        PTID  NPIK1  NPIK2  NPIK3  NPIK4  NPIK5  \\\n",
       "0         2  ADNI1     m06  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "1         2  ADNI1     m36  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "2         2  ADNI1     m60  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "3         2  ADNI1     m72  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "4         2  ADNI1     NaN  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "...     ...    ...     ...         ...    ...    ...    ...    ...    ...   \n",
       "16095  7083  ADNI3      sc  126_S_7083    NaN    NaN    NaN    NaN    NaN   \n",
       "16096  7085  ADNI3      sc  941_S_7085    NaN    NaN    NaN    NaN    NaN   \n",
       "16097  7088  ADNI3      sc  033_S_7088    NaN    NaN    NaN    NaN    NaN   \n",
       "16098  7092  ADNI3      sc  021_S_7092    NaN    NaN    NaN    NaN    NaN   \n",
       "16099  7100  ADNI3      sc  033_S_7100    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "       NPIK6  ...  ratio_PTAU_bl  Ventricles_reduction_per_year  \\\n",
       "0        NaN  ...            NaN                            NaN   \n",
       "1        NaN  ...            NaN                            NaN   \n",
       "2        NaN  ...            NaN                            NaN   \n",
       "3        NaN  ...            NaN                            NaN   \n",
       "4        NaN  ...            NaN                            NaN   \n",
       "...      ...  ...            ...                            ...   \n",
       "16095    NaN  ...            NaN                            NaN   \n",
       "16096    NaN  ...            NaN                            NaN   \n",
       "16097    NaN  ...            NaN                            NaN   \n",
       "16098    NaN  ...            NaN                            NaN   \n",
       "16099    NaN  ...            NaN                            NaN   \n",
       "\n",
       "       Hippocampus_reduction_per_year  wholebrain_reduction_per_year  \\\n",
       "0                                 NaN                            NaN   \n",
       "1                                 NaN                            NaN   \n",
       "2                                 NaN                            NaN   \n",
       "3                                 NaN                            NaN   \n",
       "4                                 NaN                            NaN   \n",
       "...                               ...                            ...   \n",
       "16095                             NaN                            NaN   \n",
       "16096                             NaN                            NaN   \n",
       "16097                             NaN                            NaN   \n",
       "16098                             NaN                            NaN   \n",
       "16099                             NaN                            NaN   \n",
       "\n",
       "       Entorhinal_reduction_per_year  Fusiform_reduction_per_year  \\\n",
       "0                                NaN                          NaN   \n",
       "1                                NaN                          NaN   \n",
       "2                                NaN                          NaN   \n",
       "3                                NaN                          NaN   \n",
       "4                                NaN                          NaN   \n",
       "...                              ...                          ...   \n",
       "16095                            NaN                          NaN   \n",
       "16096                            NaN                          NaN   \n",
       "16097                            NaN                          NaN   \n",
       "16098                            NaN                          NaN   \n",
       "16099                            NaN                          NaN   \n",
       "\n",
       "       ICV_reduction_per_year  ABETA_reduction_per_year  \\\n",
       "0                         NaN                       NaN   \n",
       "1                         NaN                       NaN   \n",
       "2                         NaN                       NaN   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "...                       ...                       ...   \n",
       "16095                     NaN                       NaN   \n",
       "16096                     NaN                       NaN   \n",
       "16097                     NaN                       NaN   \n",
       "16098                     NaN                       NaN   \n",
       "16099                     NaN                       NaN   \n",
       "\n",
       "      TAU_reduction_per_year  PTAU_reduction_per_year  \n",
       "0                        NaN                      NaN  \n",
       "1                        NaN                      NaN  \n",
       "2                        NaN                      NaN  \n",
       "3                        NaN                      NaN  \n",
       "4                        NaN                      NaN  \n",
       "...                      ...                      ...  \n",
       "16095                    NaN                      NaN  \n",
       "16096                    NaN                      NaN  \n",
       "16097                    NaN                      NaN  \n",
       "16098                    NaN                      NaN  \n",
       "16099                    NaN                      NaN  \n",
       "\n",
       "[16100 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_brain_dxch = pd.read_csv('sleep_brain_dxch.csv').iloc[:,1:].drop(['NPIKSEV'],axis=1)\n",
    "sleep_brain_dxch = sleep_brain_dxch[sleep_brain_dxch[target].notna()].reset_index().drop(['index'],axis=1)   # keep the rows where DXCHANGE is not nan\n",
    "sleep_brain_dxch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ce40a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16100 entries, 0 to 16099\n",
      "Data columns (total 37 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   RID                             16100 non-null  int64  \n",
      " 1   Phase                           16100 non-null  object \n",
      " 2   VISCODE                         15004 non-null  object \n",
      " 3   PTID                            16100 non-null  object \n",
      " 4   NPIK1                           915 non-null    float64\n",
      " 5   NPIK2                           914 non-null    float64\n",
      " 6   NPIK3                           913 non-null    float64\n",
      " 7   NPIK4                           913 non-null    float64\n",
      " 8   NPIK5                           913 non-null    float64\n",
      " 9   NPIK6                           914 non-null    float64\n",
      " 10  NPIK7                           914 non-null    float64\n",
      " 11  NPIK8                           912 non-null    float64\n",
      " 12  NPIK9A                          916 non-null    float64\n",
      " 13  NPIK9B                          915 non-null    float64\n",
      " 14  NPIK9C                          916 non-null    float64\n",
      " 15  NPIKTOT                         915 non-null    float64\n",
      " 16  insomnia                        10370 non-null  float64\n",
      " 17  OSA                             16098 non-null  float64\n",
      " 18  DXCHANGE                        16100 non-null  object \n",
      " 19  ratio_Ventricles_bl             5698 non-null   float64\n",
      " 20  ratio_Hippocampus_bl            4936 non-null   float64\n",
      " 21  ratio_WholeBrain_bl             5930 non-null   float64\n",
      " 22  ratio_Entorhinal_bl             4652 non-null   float64\n",
      " 23  ratio_Fusiform_bl               4652 non-null   float64\n",
      " 24  ratio_ICV_bl                    6249 non-null   float64\n",
      " 25  ratio_ABETA_bl                  838 non-null    float64\n",
      " 26  ratio_TAU_bl                    1029 non-null   float64\n",
      " 27  ratio_PTAU_bl                   717 non-null    float64\n",
      " 28  Ventricles_reduction_per_year   5698 non-null   float64\n",
      " 29  Hippocampus_reduction_per_year  4936 non-null   float64\n",
      " 30  wholebrain_reduction_per_year   5930 non-null   float64\n",
      " 31  Entorhinal_reduction_per_year   4652 non-null   float64\n",
      " 32  Fusiform_reduction_per_year     4652 non-null   float64\n",
      " 33  ICV_reduction_per_year          6249 non-null   float64\n",
      " 34  ABETA_reduction_per_year        838 non-null    float64\n",
      " 35  TAU_reduction_per_year          1029 non-null   float64\n",
      " 36  PTAU_reduction_per_year         717 non-null    float64\n",
      "dtypes: float64(32), int64(1), object(4)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "sleep_brain_dxch.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81696451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RID', 'Phase', 'VISCODE', 'PTID', 'NPIK1', 'NPIK2', 'NPIK3', 'NPIK4',\n",
       "       'NPIK5', 'NPIK6', 'NPIK7', 'NPIK8', 'NPIK9A', 'NPIK9B', 'NPIK9C',\n",
       "       'NPIKTOT', 'insomnia', 'OSA', 'DXCHANGE', 'ratio_Ventricles_bl',\n",
       "       'ratio_Hippocampus_bl', 'ratio_WholeBrain_bl', 'ratio_Entorhinal_bl',\n",
       "       'ratio_Fusiform_bl', 'ratio_ICV_bl', 'ratio_ABETA_bl', 'ratio_TAU_bl',\n",
       "       'ratio_PTAU_bl', 'Ventricles_reduction_per_year',\n",
       "       'Hippocampus_reduction_per_year', 'wholebrain_reduction_per_year',\n",
       "       'Entorhinal_reduction_per_year', 'Fusiform_reduction_per_year',\n",
       "       'ICV_reduction_per_year', 'ABETA_reduction_per_year',\n",
       "       'TAU_reduction_per_year', 'PTAU_reduction_per_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_brain_dxch.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b7f3e",
   "metadata": {},
   "source": [
    "### brain_biomarker______VS______DXCHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b829bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col_lst = [ 'DXCHANGE','ratio_Ventricles_bl',\n",
    "       'ratio_Hippocampus_bl', 'ratio_WholeBrain_bl', 'ratio_Entorhinal_bl',\n",
    "       'ratio_Fusiform_bl', 'Ventricles_reduction_per_year',\n",
    "       'Hippocampus_reduction_per_year', 'wholebrain_reduction_per_year',\n",
    "       'Entorhinal_reduction_per_year', 'Fusiform_reduction_per_year',\n",
    "       'ICV_reduction_per_year']\n",
    "bio_lst = [ 'ratio_ABETA_bl', 'ratio_TAU_bl','ratio_PTAU_bl']\n",
    "brain_dxch = sleep_brain_dxch[com_col + col_lst].set_index(['Phase', 'RID', 'VISCODE','PTID']).dropna(how='any',axis=0).reset_index()\n",
    "#biomarkers to dxch\n",
    "bio_dxch = sleep_brain_dxch[com_col + bio_lst].set_index(['Phase', 'RID', 'VISCODE','PTID']).dropna(how='any',axis=0).reset_index()\n",
    "df = brain_dxch.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75224ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phase                             0\n",
       "RID                               0\n",
       "VISCODE                           0\n",
       "PTID                              0\n",
       "DXCHANGE                          0\n",
       "ratio_Ventricles_bl               0\n",
       "ratio_Hippocampus_bl              0\n",
       "ratio_WholeBrain_bl               0\n",
       "ratio_Entorhinal_bl               0\n",
       "ratio_Fusiform_bl                 0\n",
       "Ventricles_reduction_per_year     0\n",
       "Hippocampus_reduction_per_year    0\n",
       "wholebrain_reduction_per_year     0\n",
       "Entorhinal_reduction_per_year     0\n",
       "Fusiform_reduction_per_year       0\n",
       "ICV_reduction_per_year            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df.isna())   # check nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af84a781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase</th>\n",
       "      <th>RID</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>ratio_Ventricles_bl</th>\n",
       "      <th>ratio_Hippocampus_bl</th>\n",
       "      <th>ratio_WholeBrain_bl</th>\n",
       "      <th>ratio_Entorhinal_bl</th>\n",
       "      <th>ratio_Fusiform_bl</th>\n",
       "      <th>Ventricles_reduction_per_year</th>\n",
       "      <th>Hippocampus_reduction_per_year</th>\n",
       "      <th>wholebrain_reduction_per_year</th>\n",
       "      <th>Entorhinal_reduction_per_year</th>\n",
       "      <th>Fusiform_reduction_per_year</th>\n",
       "      <th>ICV_reduction_per_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DXCHANGE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AD-AD</th>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "      <td>404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AD-MCI</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-AD</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-CN</th>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-MCI</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-AD</th>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-CN</th>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-MCI</th>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "      <td>1735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Phase   RID  VISCODE  PTID  ratio_Ventricles_bl  \\\n",
       "DXCHANGE                                                    \n",
       "AD-AD       404   404      404   404                  404   \n",
       "AD-MCI        4     4        4     4                    4   \n",
       "CN-AD         6     6        6     6                    6   \n",
       "CN-CN      1478  1478     1478  1478                 1478   \n",
       "CN-MCI       97    97       97    97                   97   \n",
       "MCI-AD      456   456      456   456                  456   \n",
       "MCI-CN      114   114      114   114                  114   \n",
       "MCI-MCI    1735  1735     1735  1735                 1735   \n",
       "\n",
       "          ratio_Hippocampus_bl  ratio_WholeBrain_bl  ratio_Entorhinal_bl  \\\n",
       "DXCHANGE                                                                   \n",
       "AD-AD                      404                  404                  404   \n",
       "AD-MCI                       4                    4                    4   \n",
       "CN-AD                        6                    6                    6   \n",
       "CN-CN                     1478                 1478                 1478   \n",
       "CN-MCI                      97                   97                   97   \n",
       "MCI-AD                     456                  456                  456   \n",
       "MCI-CN                     114                  114                  114   \n",
       "MCI-MCI                   1735                 1735                 1735   \n",
       "\n",
       "          ratio_Fusiform_bl  Ventricles_reduction_per_year  \\\n",
       "DXCHANGE                                                     \n",
       "AD-AD                   404                            404   \n",
       "AD-MCI                    4                              4   \n",
       "CN-AD                     6                              6   \n",
       "CN-CN                  1478                           1478   \n",
       "CN-MCI                   97                             97   \n",
       "MCI-AD                  456                            456   \n",
       "MCI-CN                  114                            114   \n",
       "MCI-MCI                1735                           1735   \n",
       "\n",
       "          Hippocampus_reduction_per_year  wholebrain_reduction_per_year  \\\n",
       "DXCHANGE                                                                  \n",
       "AD-AD                                404                            404   \n",
       "AD-MCI                                 4                              4   \n",
       "CN-AD                                  6                              6   \n",
       "CN-CN                               1478                           1478   \n",
       "CN-MCI                                97                             97   \n",
       "MCI-AD                               456                            456   \n",
       "MCI-CN                               114                            114   \n",
       "MCI-MCI                             1735                           1735   \n",
       "\n",
       "          Entorhinal_reduction_per_year  Fusiform_reduction_per_year  \\\n",
       "DXCHANGE                                                               \n",
       "AD-AD                               404                          404   \n",
       "AD-MCI                                4                            4   \n",
       "CN-AD                                 6                            6   \n",
       "CN-CN                              1478                         1478   \n",
       "CN-MCI                               97                           97   \n",
       "MCI-AD                              456                          456   \n",
       "MCI-CN                              114                          114   \n",
       "MCI-MCI                            1735                         1735   \n",
       "\n",
       "          ICV_reduction_per_year  \n",
       "DXCHANGE                          \n",
       "AD-AD                        404  \n",
       "AD-MCI                         4  \n",
       "CN-AD                          6  \n",
       "CN-CN                       1478  \n",
       "CN-MCI                        97  \n",
       "MCI-AD                       456  \n",
       "MCI-CN                       114  \n",
       "MCI-MCI                     1735  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(target).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae317f9",
   "metadata": {},
   "source": [
    "- try different combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b631153b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2191, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to distinguish MCI to AD or stay in MCI   --> As the previous challenge group used\n",
    "df_2g = df.loc[df[target].isin(['MCI-AD','MCI-MCI'])].reset_index().drop(['index'],axis=1)\n",
    "df_2g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cf4b57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2305, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to distinguish MCI to AD or stay in MCI or get back to CN\n",
    "df_3g = df.loc[df[target].isin([ 'MCI-MCI', 'MCI-AD', 'MCI-CN',])].reset_index().drop(['index'],axis=1)\n",
    "df_3g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70429218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1575, 16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to distinguish CN to MCI or stay in CN\n",
    "df_2gg = df.loc[df[target].isin(['CN-MCI', 'CN-CN'])].reset_index().drop(['index'],axis=1)\n",
    "df_2gg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19827eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3880, 16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combination bl of CN or MCI, 5 groups\n",
    "df_5g = df.loc[brain_dxch[target].isin(['CN-MCI', 'MCI-MCI', 'MCI-AD', 'MCI-CN','CN-CN'])].reset_index().drop(['index'],axis=1)\n",
    "df_5g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23461b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4284, 16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 labels\n",
    "df_6g = df.loc[brain_dxch[target].isin(['CN-MCI', 'AD-AD', 'MCI-MCI', 'MCI-AD', 'MCI-CN','CN-CN'])].reset_index().drop(['index'],axis=1)\n",
    "df_6g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39783bf",
   "metadata": {},
   "source": [
    "### undersampling and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd62ca",
   "metadata": {},
   "source": [
    "- functions\n",
    "    - models(df,drop_lst,target) : under sampling, split, scale, pca, models\n",
    "    - cv_models(df,drop_lst,target,k): under sampling, NOT SPLIT, scale, pca, models with cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c356c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_lst = ['Phase', 'RID', 'VISCODE', 'PTID',target]\n",
    "\n",
    "k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916eb3f2",
   "metadata": {},
   "source": [
    "# - 'MCI-AD': 471, 'MCI-MCI': 471"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895d717",
   "metadata": {},
   "source": [
    "-  MCI-AD': 471, 'MCI-MCI': 471\n",
    "- original dataset: random forest 90trees. f1-score on training data: 1.000 f1-score on test data: 0.783, 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc70e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 912 ; Resampled dataset shape Counter({'MCI-AD': 456, 'MCI-MCI': 456})\n",
      "\n",
      "6 principle components are needed to explain 90% of the data\n",
      "\n",
      "Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.682, Test set f1-score: 0.603\n",
      "          - saga_L1, Training set f1-score:0.682, Test set f1-score: 0.603\n",
      "          - newton-cg_L2, Training set f1-score:0.682, Test set f1-score: 0.603\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.716, Test set f1-score: 0.714\n",
      "          - saga_L1, Training set f1-score:0.682, Test set f1-score: 0.603\n",
      "          - newton-cg_L2, Training set f1-score:0.716, Test set f1-score: 0.714\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.761, Test set f1-score: 0.771\n",
      "          - saga_L1, Training set f1-score:0.734, Test set f1-score: 0.755\n",
      "          - newton-cg_L2, Training set f1-score:0.761, Test set f1-score: 0.771\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.761, Test set f1-score: 0.778\n",
      "          - saga_L1, Training set f1-score:0.771, Test set f1-score: 0.784\n",
      "          - newton-cg_L2, Training set f1-score:0.761, Test set f1-score: 0.778\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.759, Test set f1-score: 0.778\n",
      "          - saga_L1, Training set f1-score:0.763, Test set f1-score: 0.778\n",
      "          - newton-cg_L2, Training set f1-score:0.759, Test set f1-score: 0.778\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.767, Test set f1-score: 0.778\n",
      "          - saga_L1, Training set f1-score:0.763, Test set f1-score: 0.778\n",
      "          - newton-cg_L2, Training set f1-score:0.767, Test set f1-score: 0.778\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.762, Test set f1-score: 0.778\n",
      "          - saga_L1, Training set f1-score:0.762, Test set f1-score: 0.778\n",
      "          - newton-cg_L2, Training set f1-score:0.762, Test set f1-score: 0.778\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.751 f1-score on test data: 0.743\n",
      "          - tree depth: 2.000. f1-score on training data: 0.764 f1-score on test data: 0.748\n",
      "          - tree depth: 3.000. f1-score on training data: 0.787 f1-score on test data: 0.788\n",
      "          - tree depth: 4.000. f1-score on training data: 0.814 f1-score on test data: 0.780\n",
      "          - tree depth: 5.000. f1-score on training data: 0.845 f1-score on test data: 0.781\n",
      "          - tree depth: 6.000. f1-score on training data: 0.866 f1-score on test data: 0.759\n",
      "          - tree depth: 7.000. f1-score on training data: 0.899 f1-score on test data: 0.736\n",
      "          - tree depth: 8.000. f1-score on training data: 0.929 f1-score on test data: 0.715\n",
      "          - tree depth: 9.000. f1-score on training data: 0.966 f1-score on test data: 0.722\n",
      "          - tree depth: 10.000. f1-score on training data: 0.978 f1-score on test data: 0.748\n",
      "          - tree depth: 11.000. f1-score on training data: 0.982 f1-score on test data: 0.737\n",
      "          - tree depth: 12.000. f1-score on training data: 0.995 f1-score on test data: 0.715\n",
      "          - tree depth: 13.000. f1-score on training data: 1.000 f1-score on test data: 0.731\n",
      "          - tree depth: 14.000. f1-score on training data: 1.000 f1-score on test data: 0.731\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.970 f1-score on test data: 0.758\n",
      "          - 10trees. f1-score on training data: 0.986 f1-score on test data: 0.781\n",
      "          - 15trees. f1-score on training data: 0.996 f1-score on test data: 0.809\n",
      "          - 20trees. f1-score on training data: 0.996 f1-score on test data: 0.797\n",
      "          - 25trees. f1-score on training data: 0.999 f1-score on test data: 0.786\n",
      "          - 30trees. f1-score on training data: 1.000 f1-score on test data: 0.769\n",
      "          - 35trees. f1-score on training data: 1.000 f1-score on test data: 0.781\n",
      "          - 40trees. f1-score on training data: 1.000 f1-score on test data: 0.786\n",
      "          - 45trees. f1-score on training data: 1.000 f1-score on test data: 0.791\n",
      "          - 50trees. f1-score on training data: 1.000 f1-score on test data: 0.786\n",
      "          - 55trees. f1-score on training data: 1.000 f1-score on test data: 0.792\n",
      "          - 60trees. f1-score on training data: 1.000 f1-score on test data: 0.786\n",
      "          - 65trees. f1-score on training data: 1.000 f1-score on test data: 0.786\n",
      "          - 70trees. f1-score on training data: 1.000 f1-score on test data: 0.797\n",
      "          - 75trees. f1-score on training data: 1.000 f1-score on test data: 0.791\n",
      "          - 80trees. f1-score on training data: 1.000 f1-score on test data: 0.791\n",
      "          - 85trees. f1-score on training data: 1.000 f1-score on test data: 0.781\n",
      "          - 90trees. f1-score on training data: 1.000 f1-score on test data: 0.781\n",
      "          - 95trees. f1-score on training data: 1.000 f1-score on test data: 0.775\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.861 f1-score on test data: 0.760\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.863 f1-score on test data: 0.776\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.760, Test set f1-score: 0.793\n",
      "          - saga_L1, Training set f1-score:0.682, Test set f1-score: 0.603\n",
      "          - newton-cg_L2, Training set f1-score:0.760, Test set f1-score: 0.793\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.752, Test set f1-score: 0.773\n",
      "          - saga_L1, Training set f1-score:0.748, Test set f1-score: 0.772\n",
      "          - newton-cg_L2, Training set f1-score:0.752, Test set f1-score: 0.773\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.767, Test set f1-score: 0.784\n",
      "          - saga_L1, Training set f1-score:0.766, Test set f1-score: 0.779\n",
      "          - newton-cg_L2, Training set f1-score:0.767, Test set f1-score: 0.784\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.763, Test set f1-score: 0.778\n",
      "          - saga_L1, Training set f1-score:0.766, Test set f1-score: 0.778\n",
      "          - newton-cg_L2, Training set f1-score:0.763, Test set f1-score: 0.778\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.762, Test set f1-score: 0.778\n",
      "          - saga_L1, Training set f1-score:0.763, Test set f1-score: 0.778\n",
      "          - newton-cg_L2, Training set f1-score:0.762, Test set f1-score: 0.778\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.760, Test set f1-score: 0.778\n",
      "          - saga_L1, Training set f1-score:0.763, Test set f1-score: 0.778\n",
      "          - newton-cg_L2, Training set f1-score:0.760, Test set f1-score: 0.778\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.760, Test set f1-score: 0.778\n",
      "          - saga_L1, Training set f1-score:0.762, Test set f1-score: 0.778\n",
      "          - newton-cg_L2, Training set f1-score:0.760, Test set f1-score: 0.778\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.751 f1-score on test data: 0.743\n",
      "          - tree depth: 2.000. f1-score on training data: 0.764 f1-score on test data: 0.748\n",
      "          - tree depth: 3.000. f1-score on training data: 0.787 f1-score on test data: 0.788\n",
      "          - tree depth: 4.000. f1-score on training data: 0.814 f1-score on test data: 0.780\n",
      "          - tree depth: 5.000. f1-score on training data: 0.845 f1-score on test data: 0.781\n",
      "          - tree depth: 6.000. f1-score on training data: 0.866 f1-score on test data: 0.759\n",
      "          - tree depth: 7.000. f1-score on training data: 0.899 f1-score on test data: 0.736\n",
      "          - tree depth: 8.000. f1-score on training data: 0.929 f1-score on test data: 0.715\n",
      "          - tree depth: 9.000. f1-score on training data: 0.966 f1-score on test data: 0.722\n",
      "          - tree depth: 10.000. f1-score on training data: 0.978 f1-score on test data: 0.748\n",
      "          - tree depth: 11.000. f1-score on training data: 0.982 f1-score on test data: 0.737\n",
      "          - tree depth: 12.000. f1-score on training data: 0.995 f1-score on test data: 0.715\n",
      "          - tree depth: 13.000. f1-score on training data: 1.000 f1-score on test data: 0.731\n",
      "          - tree depth: 14.000. f1-score on training data: 1.000 f1-score on test data: 0.731\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.970 f1-score on test data: 0.758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 10trees. f1-score on training data: 0.986 f1-score on test data: 0.781\n",
      "          - 15trees. f1-score on training data: 0.996 f1-score on test data: 0.809\n",
      "          - 20trees. f1-score on training data: 0.996 f1-score on test data: 0.797\n",
      "          - 25trees. f1-score on training data: 0.999 f1-score on test data: 0.786\n",
      "          - 30trees. f1-score on training data: 1.000 f1-score on test data: 0.769\n",
      "          - 35trees. f1-score on training data: 1.000 f1-score on test data: 0.781\n",
      "          - 40trees. f1-score on training data: 1.000 f1-score on test data: 0.786\n",
      "          - 45trees. f1-score on training data: 1.000 f1-score on test data: 0.791\n",
      "          - 50trees. f1-score on training data: 1.000 f1-score on test data: 0.786\n",
      "          - 55trees. f1-score on training data: 1.000 f1-score on test data: 0.792\n",
      "          - 60trees. f1-score on training data: 1.000 f1-score on test data: 0.786\n",
      "          - 65trees. f1-score on training data: 1.000 f1-score on test data: 0.786\n",
      "          - 70trees. f1-score on training data: 1.000 f1-score on test data: 0.797\n",
      "          - 75trees. f1-score on training data: 1.000 f1-score on test data: 0.791\n",
      "          - 80trees. f1-score on training data: 1.000 f1-score on test data: 0.791\n",
      "          - 85trees. f1-score on training data: 1.000 f1-score on test data: 0.781\n",
      "          - 90trees. f1-score on training data: 1.000 f1-score on test data: 0.781\n",
      "          - 95trees. f1-score on training data: 1.000 f1-score on test data: 0.775\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 1.000 f1-score on test data: 0.758\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 1.000 f1-score on test data: 0.704\n",
      "- Using 6 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.763, Test set f1-score: 0.793\n",
      "          - saga_L1, Training set f1-score:0.651, Test set f1-score: 0.725\n",
      "          - newton-cg_L2, Training set f1-score:0.763, Test set f1-score: 0.793\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.756, Test set f1-score: 0.773\n",
      "          - saga_L1, Training set f1-score:0.753, Test set f1-score: 0.799\n",
      "          - newton-cg_L2, Training set f1-score:0.756, Test set f1-score: 0.773\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.762, Test set f1-score: 0.772\n",
      "          - saga_L1, Training set f1-score:0.757, Test set f1-score: 0.767\n",
      "          - newton-cg_L2, Training set f1-score:0.762, Test set f1-score: 0.772\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "          - saga_L1, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "          - newton-cg_L2, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "          - saga_L1, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "          - newton-cg_L2, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "          - saga_L1, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "          - newton-cg_L2, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "          - saga_L1, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "          - newton-cg_L2, Training set f1-score:0.763, Test set f1-score: 0.767\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.761 f1-score on test data: 0.786\n",
      "          - tree depth: 2.000. f1-score on training data: 0.761 f1-score on test data: 0.786\n",
      "          - tree depth: 3.000. f1-score on training data: 0.779 f1-score on test data: 0.775\n",
      "          - tree depth: 4.000. f1-score on training data: 0.793 f1-score on test data: 0.747\n",
      "          - tree depth: 5.000. f1-score on training data: 0.800 f1-score on test data: 0.759\n",
      "          - tree depth: 6.000. f1-score on training data: 0.815 f1-score on test data: 0.769\n",
      "          - tree depth: 7.000. f1-score on training data: 0.849 f1-score on test data: 0.726\n",
      "          - tree depth: 8.000. f1-score on training data: 0.871 f1-score on test data: 0.693\n",
      "          - tree depth: 9.000. f1-score on training data: 0.907 f1-score on test data: 0.709\n",
      "          - tree depth: 10.000. f1-score on training data: 0.948 f1-score on test data: 0.720\n",
      "          - tree depth: 11.000. f1-score on training data: 0.971 f1-score on test data: 0.709\n",
      "          - tree depth: 12.000. f1-score on training data: 0.986 f1-score on test data: 0.704\n",
      "          - tree depth: 13.000. f1-score on training data: 0.997 f1-score on test data: 0.720\n",
      "          - tree depth: 14.000. f1-score on training data: 0.999 f1-score on test data: 0.698\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.975 f1-score on test data: 0.706\n",
      "          - 10trees. f1-score on training data: 0.979 f1-score on test data: 0.759\n",
      "          - 15trees. f1-score on training data: 0.993 f1-score on test data: 0.766\n",
      "          - 20trees. f1-score on training data: 0.995 f1-score on test data: 0.781\n",
      "          - 25trees. f1-score on training data: 0.999 f1-score on test data: 0.804\n",
      "          - 30trees. f1-score on training data: 0.999 f1-score on test data: 0.792\n",
      "          - 35trees. f1-score on training data: 1.000 f1-score on test data: 0.792\n",
      "          - 40trees. f1-score on training data: 1.000 f1-score on test data: 0.781\n",
      "          - 45trees. f1-score on training data: 0.999 f1-score on test data: 0.776\n",
      "          - 50trees. f1-score on training data: 1.000 f1-score on test data: 0.787\n",
      "          - 55trees. f1-score on training data: 1.000 f1-score on test data: 0.781\n",
      "          - 60trees. f1-score on training data: 1.000 f1-score on test data: 0.797\n",
      "          - 65trees. f1-score on training data: 1.000 f1-score on test data: 0.787\n",
      "          - 70trees. f1-score on training data: 1.000 f1-score on test data: 0.792\n",
      "          - 75trees. f1-score on training data: 1.000 f1-score on test data: 0.793\n",
      "          - 80trees. f1-score on training data: 1.000 f1-score on test data: 0.787\n",
      "          - 85trees. f1-score on training data: 1.000 f1-score on test data: 0.787\n",
      "          - 90trees. f1-score on training data: 1.000 f1-score on test data: 0.782\n",
      "          - 95trees. f1-score on training data: 1.000 f1-score on test data: 0.777\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 1.000 f1-score on test data: 0.776\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 1.000 f1-score on test data: 0.704\n"
     ]
    }
   ],
   "source": [
    "models(df_2g,drop_lst,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43e5725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 912 ; Resampled dataset shape Counter({'MCI-AD': 456, 'MCI-MCI': 456})\n",
      "\n",
      "6 principle components are needed to explain 90% of the data\n",
      "\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.738\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.332\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.738\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.744\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.331\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.744\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.752\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.725\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.752\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.761\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.770\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.761\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.761\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.758\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.761\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.756\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.755\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.756\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.754\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.755\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.754\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.715\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.723\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.746\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.752\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.731\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.719\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.702\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.706\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.699\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.689\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.694\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.681\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.678\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.685\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.751\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.765\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.771\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.760\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.762\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.766\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.772\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.775\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.773\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.767\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.773\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.769\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.769\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.772\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.769\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.771\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.770\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.769\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.770\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.759\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.740\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.748\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.333\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.748\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.751\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.731\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.751\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.755\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.758\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.755\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.752\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.755\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.752\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.754\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.752\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.754\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.754\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.754\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.754\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.754\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.754\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.754\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.715\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.723\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.746\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.752\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.731\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.719\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.702\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.706\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.699\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.689\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.694\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.681\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.678\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.685\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.751\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.765\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.771\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 25trees. average weighted f1-score of 10-cross validation:0.762\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.766\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.772\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.775\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.773\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.767\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.773\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.769\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.769\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.772\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.769\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.771\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.770\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.769\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.770\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.712\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.702\n",
      "- Using 6 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.748\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.335\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.748\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.755\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.752\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.755\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.758\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.754\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.758\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.759\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.761\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.759\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.759\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.759\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.759\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.759\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.759\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.759\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.759\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.759\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.759\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.753\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.752\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.750\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.731\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.726\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.701\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.704\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.689\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.686\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.692\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.679\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.675\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.674\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.685\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.713\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.729\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.740\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.747\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.744\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.756\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.756\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.758\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.747\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.749\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.755\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.755\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.754\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.753\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.750\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.750\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.756\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.756\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.754\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.712\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.680\n"
     ]
    }
   ],
   "source": [
    "cv_models(df_2g,drop_lst,target,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade44273",
   "metadata": {},
   "source": [
    "- Model selection:\n",
    "- original data, 10 fold-cv, randomforest: 90trees. average weighted f1-score of 10-cross validation:0.773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e0322c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 912 ; Resampled dataset shape Counter({'MCI-AD': 456, 'MCI-MCI': 456})\n",
      "\n",
      "6 principle components are needed to explain 90% of the data\n",
      "\n",
      "Features sorted by their score for each estimator \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_importance</th>\n",
       "      <th>importance_1</th>\n",
       "      <th>importance_2</th>\n",
       "      <th>importance_3</th>\n",
       "      <th>importance_4</th>\n",
       "      <th>importance_5</th>\n",
       "      <th>importance_6</th>\n",
       "      <th>importance_7</th>\n",
       "      <th>importance_8</th>\n",
       "      <th>importance_9</th>\n",
       "      <th>importance_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ratio_Ventricles_bl</th>\n",
       "      <td>0.169794</td>\n",
       "      <td>0.175124</td>\n",
       "      <td>0.165148</td>\n",
       "      <td>0.163583</td>\n",
       "      <td>0.169610</td>\n",
       "      <td>0.172999</td>\n",
       "      <td>0.173156</td>\n",
       "      <td>0.169377</td>\n",
       "      <td>0.168916</td>\n",
       "      <td>0.173408</td>\n",
       "      <td>0.166616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio_Hippocampus_bl</th>\n",
       "      <td>0.159510</td>\n",
       "      <td>0.160151</td>\n",
       "      <td>0.153166</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.170252</td>\n",
       "      <td>0.158334</td>\n",
       "      <td>0.154589</td>\n",
       "      <td>0.166782</td>\n",
       "      <td>0.161076</td>\n",
       "      <td>0.147845</td>\n",
       "      <td>0.159445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio_Fusiform_bl</th>\n",
       "      <td>0.106719</td>\n",
       "      <td>0.100405</td>\n",
       "      <td>0.097066</td>\n",
       "      <td>0.112787</td>\n",
       "      <td>0.103480</td>\n",
       "      <td>0.100547</td>\n",
       "      <td>0.117582</td>\n",
       "      <td>0.108392</td>\n",
       "      <td>0.105877</td>\n",
       "      <td>0.110352</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hippocampus_reduction_per_year</th>\n",
       "      <td>0.092454</td>\n",
       "      <td>0.094723</td>\n",
       "      <td>0.104333</td>\n",
       "      <td>0.099306</td>\n",
       "      <td>0.092149</td>\n",
       "      <td>0.088324</td>\n",
       "      <td>0.096741</td>\n",
       "      <td>0.091065</td>\n",
       "      <td>0.088695</td>\n",
       "      <td>0.088785</td>\n",
       "      <td>0.080419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio_Entorhinal_bl</th>\n",
       "      <td>0.091425</td>\n",
       "      <td>0.080244</td>\n",
       "      <td>0.080997</td>\n",
       "      <td>0.094688</td>\n",
       "      <td>0.084981</td>\n",
       "      <td>0.095195</td>\n",
       "      <td>0.092494</td>\n",
       "      <td>0.092371</td>\n",
       "      <td>0.101419</td>\n",
       "      <td>0.094431</td>\n",
       "      <td>0.097427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ventricles_reduction_per_year</th>\n",
       "      <td>0.081996</td>\n",
       "      <td>0.084199</td>\n",
       "      <td>0.095601</td>\n",
       "      <td>0.077286</td>\n",
       "      <td>0.081707</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>0.080650</td>\n",
       "      <td>0.077915</td>\n",
       "      <td>0.074805</td>\n",
       "      <td>0.081438</td>\n",
       "      <td>0.078641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio_WholeBrain_bl</th>\n",
       "      <td>0.075577</td>\n",
       "      <td>0.080319</td>\n",
       "      <td>0.080823</td>\n",
       "      <td>0.070350</td>\n",
       "      <td>0.074611</td>\n",
       "      <td>0.074426</td>\n",
       "      <td>0.067341</td>\n",
       "      <td>0.066725</td>\n",
       "      <td>0.074225</td>\n",
       "      <td>0.081364</td>\n",
       "      <td>0.085588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entorhinal_reduction_per_year</th>\n",
       "      <td>0.060006</td>\n",
       "      <td>0.062579</td>\n",
       "      <td>0.058789</td>\n",
       "      <td>0.059474</td>\n",
       "      <td>0.059884</td>\n",
       "      <td>0.059889</td>\n",
       "      <td>0.061618</td>\n",
       "      <td>0.058022</td>\n",
       "      <td>0.058758</td>\n",
       "      <td>0.058889</td>\n",
       "      <td>0.062155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fusiform_reduction_per_year</th>\n",
       "      <td>0.056578</td>\n",
       "      <td>0.055922</td>\n",
       "      <td>0.054298</td>\n",
       "      <td>0.055237</td>\n",
       "      <td>0.062864</td>\n",
       "      <td>0.056437</td>\n",
       "      <td>0.054849</td>\n",
       "      <td>0.061040</td>\n",
       "      <td>0.056682</td>\n",
       "      <td>0.054734</td>\n",
       "      <td>0.053712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wholebrain_reduction_per_year</th>\n",
       "      <td>0.053795</td>\n",
       "      <td>0.056558</td>\n",
       "      <td>0.055991</td>\n",
       "      <td>0.052462</td>\n",
       "      <td>0.049682</td>\n",
       "      <td>0.053337</td>\n",
       "      <td>0.050893</td>\n",
       "      <td>0.055144</td>\n",
       "      <td>0.053766</td>\n",
       "      <td>0.055417</td>\n",
       "      <td>0.054703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICV_reduction_per_year</th>\n",
       "      <td>0.052147</td>\n",
       "      <td>0.049775</td>\n",
       "      <td>0.053788</td>\n",
       "      <td>0.051366</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.052792</td>\n",
       "      <td>0.050087</td>\n",
       "      <td>0.053166</td>\n",
       "      <td>0.055782</td>\n",
       "      <td>0.053338</td>\n",
       "      <td>0.050593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                avg_importance  importance_1  importance_2  \\\n",
       "ratio_Ventricles_bl                   0.169794      0.175124      0.165148   \n",
       "ratio_Hippocampus_bl                  0.159510      0.160151      0.153166   \n",
       "ratio_Fusiform_bl                     0.106719      0.100405      0.097066   \n",
       "Hippocampus_reduction_per_year        0.092454      0.094723      0.104333   \n",
       "ratio_Entorhinal_bl                   0.091425      0.080244      0.080997   \n",
       "Ventricles_reduction_per_year         0.081996      0.084199      0.095601   \n",
       "ratio_WholeBrain_bl                   0.075577      0.080319      0.080823   \n",
       "Entorhinal_reduction_per_year         0.060006      0.062579      0.058789   \n",
       "Fusiform_reduction_per_year           0.056578      0.055922      0.054298   \n",
       "wholebrain_reduction_per_year         0.053795      0.056558      0.055991   \n",
       "ICV_reduction_per_year                0.052147      0.049775      0.053788   \n",
       "\n",
       "                                importance_3  importance_4  importance_5  \\\n",
       "ratio_Ventricles_bl                 0.163583      0.169610      0.172999   \n",
       "ratio_Hippocampus_bl                0.163462      0.170252      0.158334   \n",
       "ratio_Fusiform_bl                   0.112787      0.103480      0.100547   \n",
       "Hippocampus_reduction_per_year      0.099306      0.092149      0.088324   \n",
       "ratio_Entorhinal_bl                 0.094688      0.084981      0.095195   \n",
       "Ventricles_reduction_per_year       0.077286      0.081707      0.087719   \n",
       "ratio_WholeBrain_bl                 0.070350      0.074611      0.074426   \n",
       "Entorhinal_reduction_per_year       0.059474      0.059884      0.059889   \n",
       "Fusiform_reduction_per_year         0.055237      0.062864      0.056437   \n",
       "wholebrain_reduction_per_year       0.052462      0.049682      0.053337   \n",
       "ICV_reduction_per_year              0.051366      0.050781      0.052792   \n",
       "\n",
       "                                importance_6  importance_7  importance_8  \\\n",
       "ratio_Ventricles_bl                 0.173156      0.169377      0.168916   \n",
       "ratio_Hippocampus_bl                0.154589      0.166782      0.161076   \n",
       "ratio_Fusiform_bl                   0.117582      0.108392      0.105877   \n",
       "Hippocampus_reduction_per_year      0.096741      0.091065      0.088695   \n",
       "ratio_Entorhinal_bl                 0.092494      0.092371      0.101419   \n",
       "Ventricles_reduction_per_year       0.080650      0.077915      0.074805   \n",
       "ratio_WholeBrain_bl                 0.067341      0.066725      0.074225   \n",
       "Entorhinal_reduction_per_year       0.061618      0.058022      0.058758   \n",
       "Fusiform_reduction_per_year         0.054849      0.061040      0.056682   \n",
       "wholebrain_reduction_per_year       0.050893      0.055144      0.053766   \n",
       "ICV_reduction_per_year              0.050087      0.053166      0.055782   \n",
       "\n",
       "                                importance_9  importance_10  \n",
       "ratio_Ventricles_bl                 0.173408       0.166616  \n",
       "ratio_Hippocampus_bl                0.147845       0.159445  \n",
       "ratio_Fusiform_bl                   0.110352       0.110700  \n",
       "Hippocampus_reduction_per_year      0.088785       0.080419  \n",
       "ratio_Entorhinal_bl                 0.094431       0.097427  \n",
       "Ventricles_reduction_per_year       0.081438       0.078641  \n",
       "ratio_WholeBrain_bl                 0.081364       0.085588  \n",
       "Entorhinal_reduction_per_year       0.058889       0.062155  \n",
       "Fusiform_reduction_per_year         0.054734       0.053712  \n",
       "wholebrain_reduction_per_year       0.055417       0.054703  \n",
       "ICV_reduction_per_year              0.053338       0.050593  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = usampling_scale_data(df_2g,drop_lst,target)     \n",
    "X = res[0]\n",
    "y = res[3]\n",
    "clf = RandomForestClassifier(n_estimators =90, random_state = 5862)\n",
    "title_label = '10-fold crossvalidation of random forest with (90 trees)'\n",
    "feature_importance(X,y,clf,10,title_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d37f91",
   "metadata": {},
   "source": [
    "- 'MCI-AD': 132, 'MCI-CN': 132, 'MCI-MCI': 132}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f26e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 342 ; Resampled dataset shape Counter({'MCI-AD': 114, 'MCI-CN': 114, 'MCI-MCI': 114})\n",
      "\n",
      "5 principle components are needed to explain 90% of the data\n",
      "\n",
      "Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.524, Test set f1-score: 0.395\n",
      "          - saga_L1, Training set f1-score:0.496, Test set f1-score: 0.516\n",
      "          - newton-cg_L2, Training set f1-score:0.524, Test set f1-score: 0.395\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.511, Test set f1-score: 0.395\n",
      "          - saga_L1, Training set f1-score:0.524, Test set f1-score: 0.395\n",
      "          - newton-cg_L2, Training set f1-score:0.511, Test set f1-score: 0.395\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.630, Test set f1-score: 0.560\n",
      "          - saga_L1, Training set f1-score:0.616, Test set f1-score: 0.519\n",
      "          - newton-cg_L2, Training set f1-score:0.630, Test set f1-score: 0.560\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.622, Test set f1-score: 0.568\n",
      "          - saga_L1, Training set f1-score:0.639, Test set f1-score: 0.623\n",
      "          - newton-cg_L2, Training set f1-score:0.622, Test set f1-score: 0.568\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.645, Test set f1-score: 0.581\n",
      "          - saga_L1, Training set f1-score:0.630, Test set f1-score: 0.652\n",
      "          - newton-cg_L2, Training set f1-score:0.645, Test set f1-score: 0.581\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.633, Test set f1-score: 0.652\n",
      "          - saga_L1, Training set f1-score:0.622, Test set f1-score: 0.638\n",
      "          - newton-cg_L2, Training set f1-score:0.633, Test set f1-score: 0.652\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.644, Test set f1-score: 0.610\n",
      "          - saga_L1, Training set f1-score:0.633, Test set f1-score: 0.638\n",
      "          - newton-cg_L2, Training set f1-score:0.641, Test set f1-score: 0.610\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.568 f1-score on test data: 0.542\n",
      "          - tree depth: 2.000. f1-score on training data: 0.581 f1-score on test data: 0.595\n",
      "          - tree depth: 3.000. f1-score on training data: 0.671 f1-score on test data: 0.626\n",
      "          - tree depth: 4.000. f1-score on training data: 0.745 f1-score on test data: 0.622\n",
      "          - tree depth: 5.000. f1-score on training data: 0.796 f1-score on test data: 0.615\n",
      "          - tree depth: 6.000. f1-score on training data: 0.845 f1-score on test data: 0.557\n",
      "          - tree depth: 7.000. f1-score on training data: 0.904 f1-score on test data: 0.579\n",
      "          - tree depth: 8.000. f1-score on training data: 0.949 f1-score on test data: 0.551\n",
      "          - tree depth: 9.000. f1-score on training data: 0.978 f1-score on test data: 0.577\n",
      "          - tree depth: 10.000. f1-score on training data: 0.993 f1-score on test data: 0.578\n",
      "          - tree depth: 11.000. f1-score on training data: 1.000 f1-score on test data: 0.594\n",
      "          - tree depth: 12.000. f1-score on training data: 1.000 f1-score on test data: 0.594\n",
      "          - tree depth: 13.000. f1-score on training data: 1.000 f1-score on test data: 0.594\n",
      "          - tree depth: 14.000. f1-score on training data: 1.000 f1-score on test data: 0.594\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.938 f1-score on test data: 0.648\n",
      "          - 10trees. f1-score on training data: 0.989 f1-score on test data: 0.631\n",
      "          - 15trees. f1-score on training data: 0.996 f1-score on test data: 0.637\n",
      "          - 20trees. f1-score on training data: 1.000 f1-score on test data: 0.564\n",
      "          - 25trees. f1-score on training data: 0.996 f1-score on test data: 0.633\n",
      "          - 30trees. f1-score on training data: 1.000 f1-score on test data: 0.615\n",
      "          - 35trees. f1-score on training data: 1.000 f1-score on test data: 0.587\n",
      "          - 40trees. f1-score on training data: 1.000 f1-score on test data: 0.653\n",
      "          - 45trees. f1-score on training data: 1.000 f1-score on test data: 0.639\n",
      "          - 50trees. f1-score on training data: 1.000 f1-score on test data: 0.641\n",
      "          - 55trees. f1-score on training data: 1.000 f1-score on test data: 0.628\n",
      "          - 60trees. f1-score on training data: 1.000 f1-score on test data: 0.628\n",
      "          - 65trees. f1-score on training data: 1.000 f1-score on test data: 0.628\n",
      "          - 70trees. f1-score on training data: 1.000 f1-score on test data: 0.626\n",
      "          - 75trees. f1-score on training data: 1.000 f1-score on test data: 0.641\n",
      "          - 80trees. f1-score on training data: 1.000 f1-score on test data: 0.641\n",
      "          - 85trees. f1-score on training data: 1.000 f1-score on test data: 0.641\n",
      "          - 90trees. f1-score on training data: 1.000 f1-score on test data: 0.628\n",
      "          - 95trees. f1-score on training data: 1.000 f1-score on test data: 0.641\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.838 f1-score on test data: 0.601\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.713 f1-score on test data: 0.660\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.623, Test set f1-score: 0.529\n",
      "          - saga_L1, Training set f1-score:0.496, Test set f1-score: 0.516\n",
      "          - newton-cg_L2, Training set f1-score:0.623, Test set f1-score: 0.529\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.613, Test set f1-score: 0.610\n",
      "          - saga_L1, Training set f1-score:0.496, Test set f1-score: 0.516\n",
      "          - newton-cg_L2, Training set f1-score:0.613, Test set f1-score: 0.610\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.633, Test set f1-score: 0.626\n",
      "          - saga_L1, Training set f1-score:0.615, Test set f1-score: 0.598\n",
      "          - newton-cg_L2, Training set f1-score:0.633, Test set f1-score: 0.626\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.629, Test set f1-score: 0.642\n",
      "          - saga_L1, Training set f1-score:0.636, Test set f1-score: 0.626\n",
      "          - newton-cg_L2, Training set f1-score:0.629, Test set f1-score: 0.642\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.644, Test set f1-score: 0.626\n",
      "          - saga_L1, Training set f1-score:0.641, Test set f1-score: 0.626\n",
      "          - newton-cg_L2, Training set f1-score:0.644, Test set f1-score: 0.626\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.634, Test set f1-score: 0.626\n",
      "          - saga_L1, Training set f1-score:0.626, Test set f1-score: 0.626\n",
      "          - newton-cg_L2, Training set f1-score:0.634, Test set f1-score: 0.626\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.634, Test set f1-score: 0.626\n",
      "          - saga_L1, Training set f1-score:0.626, Test set f1-score: 0.626\n",
      "          - newton-cg_L2, Training set f1-score:0.634, Test set f1-score: 0.626\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.568 f1-score on test data: 0.542\n",
      "          - tree depth: 2.000. f1-score on training data: 0.581 f1-score on test data: 0.595\n",
      "          - tree depth: 3.000. f1-score on training data: 0.671 f1-score on test data: 0.626\n",
      "          - tree depth: 4.000. f1-score on training data: 0.745 f1-score on test data: 0.622\n",
      "          - tree depth: 5.000. f1-score on training data: 0.796 f1-score on test data: 0.615\n",
      "          - tree depth: 6.000. f1-score on training data: 0.845 f1-score on test data: 0.557\n",
      "          - tree depth: 7.000. f1-score on training data: 0.904 f1-score on test data: 0.579\n",
      "          - tree depth: 8.000. f1-score on training data: 0.949 f1-score on test data: 0.551\n",
      "          - tree depth: 9.000. f1-score on training data: 0.978 f1-score on test data: 0.577\n",
      "          - tree depth: 10.000. f1-score on training data: 0.993 f1-score on test data: 0.578\n",
      "          - tree depth: 11.000. f1-score on training data: 1.000 f1-score on test data: 0.594\n",
      "          - tree depth: 12.000. f1-score on training data: 1.000 f1-score on test data: 0.594\n",
      "          - tree depth: 13.000. f1-score on training data: 1.000 f1-score on test data: 0.594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - tree depth: 14.000. f1-score on training data: 1.000 f1-score on test data: 0.594\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.938 f1-score on test data: 0.648\n",
      "          - 10trees. f1-score on training data: 0.989 f1-score on test data: 0.631\n",
      "          - 15trees. f1-score on training data: 0.996 f1-score on test data: 0.637\n",
      "          - 20trees. f1-score on training data: 1.000 f1-score on test data: 0.564\n",
      "          - 25trees. f1-score on training data: 0.996 f1-score on test data: 0.633\n",
      "          - 30trees. f1-score on training data: 1.000 f1-score on test data: 0.615\n",
      "          - 35trees. f1-score on training data: 1.000 f1-score on test data: 0.587\n",
      "          - 40trees. f1-score on training data: 1.000 f1-score on test data: 0.653\n",
      "          - 45trees. f1-score on training data: 1.000 f1-score on test data: 0.639\n",
      "          - 50trees. f1-score on training data: 1.000 f1-score on test data: 0.641\n",
      "          - 55trees. f1-score on training data: 1.000 f1-score on test data: 0.628\n",
      "          - 60trees. f1-score on training data: 1.000 f1-score on test data: 0.628\n",
      "          - 65trees. f1-score on training data: 1.000 f1-score on test data: 0.628\n",
      "          - 70trees. f1-score on training data: 1.000 f1-score on test data: 0.626\n",
      "          - 75trees. f1-score on training data: 1.000 f1-score on test data: 0.641\n",
      "          - 80trees. f1-score on training data: 1.000 f1-score on test data: 0.641\n",
      "          - 85trees. f1-score on training data: 1.000 f1-score on test data: 0.641\n",
      "          - 90trees. f1-score on training data: 1.000 f1-score on test data: 0.628\n",
      "          - 95trees. f1-score on training data: 1.000 f1-score on test data: 0.641\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 1.000 f1-score on test data: 0.569\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 1.000 f1-score on test data: 0.563\n",
      "- Using 5 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.614, Test set f1-score: 0.529\n",
      "          - saga_L1, Training set f1-score:0.524, Test set f1-score: 0.395\n",
      "          - newton-cg_L2, Training set f1-score:0.614, Test set f1-score: 0.529\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.580, Test set f1-score: 0.622\n",
      "          - saga_L1, Training set f1-score:0.588, Test set f1-score: 0.463\n",
      "          - newton-cg_L2, Training set f1-score:0.580, Test set f1-score: 0.622\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.598, Test set f1-score: 0.624\n",
      "          - saga_L1, Training set f1-score:0.580, Test set f1-score: 0.626\n",
      "          - newton-cg_L2, Training set f1-score:0.598, Test set f1-score: 0.624\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.611, Test set f1-score: 0.632\n",
      "          - saga_L1, Training set f1-score:0.611, Test set f1-score: 0.628\n",
      "          - newton-cg_L2, Training set f1-score:0.611, Test set f1-score: 0.632\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.618, Test set f1-score: 0.606\n",
      "          - saga_L1, Training set f1-score:0.626, Test set f1-score: 0.617\n",
      "          - newton-cg_L2, Training set f1-score:0.618, Test set f1-score: 0.606\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.618, Test set f1-score: 0.606\n",
      "          - saga_L1, Training set f1-score:0.618, Test set f1-score: 0.606\n",
      "          - newton-cg_L2, Training set f1-score:0.618, Test set f1-score: 0.606\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.618, Test set f1-score: 0.606\n",
      "          - saga_L1, Training set f1-score:0.618, Test set f1-score: 0.606\n",
      "          - newton-cg_L2, Training set f1-score:0.618, Test set f1-score: 0.606\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.600 f1-score on test data: 0.651\n",
      "          - tree depth: 2.000. f1-score on training data: 0.593 f1-score on test data: 0.621\n",
      "          - tree depth: 3.000. f1-score on training data: 0.622 f1-score on test data: 0.644\n",
      "          - tree depth: 4.000. f1-score on training data: 0.676 f1-score on test data: 0.551\n",
      "          - tree depth: 5.000. f1-score on training data: 0.738 f1-score on test data: 0.596\n",
      "          - tree depth: 6.000. f1-score on training data: 0.797 f1-score on test data: 0.585\n",
      "          - tree depth: 7.000. f1-score on training data: 0.816 f1-score on test data: 0.580\n",
      "          - tree depth: 8.000. f1-score on training data: 0.859 f1-score on test data: 0.566\n",
      "          - tree depth: 9.000. f1-score on training data: 0.901 f1-score on test data: 0.668\n",
      "          - tree depth: 10.000. f1-score on training data: 0.927 f1-score on test data: 0.614\n",
      "          - tree depth: 11.000. f1-score on training data: 0.956 f1-score on test data: 0.594\n",
      "          - tree depth: 12.000. f1-score on training data: 0.974 f1-score on test data: 0.596\n",
      "          - tree depth: 13.000. f1-score on training data: 0.989 f1-score on test data: 0.596\n",
      "          - tree depth: 14.000. f1-score on training data: 1.000 f1-score on test data: 0.581\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.935 f1-score on test data: 0.574\n",
      "          - 10trees. f1-score on training data: 0.989 f1-score on test data: 0.629\n",
      "          - 15trees. f1-score on training data: 0.996 f1-score on test data: 0.606\n",
      "          - 20trees. f1-score on training data: 1.000 f1-score on test data: 0.656\n",
      "          - 25trees. f1-score on training data: 1.000 f1-score on test data: 0.638\n",
      "          - 30trees. f1-score on training data: 1.000 f1-score on test data: 0.667\n",
      "          - 35trees. f1-score on training data: 1.000 f1-score on test data: 0.650\n",
      "          - 40trees. f1-score on training data: 1.000 f1-score on test data: 0.665\n",
      "          - 45trees. f1-score on training data: 1.000 f1-score on test data: 0.665\n",
      "          - 50trees. f1-score on training data: 1.000 f1-score on test data: 0.665\n",
      "          - 55trees. f1-score on training data: 1.000 f1-score on test data: 0.682\n",
      "          - 60trees. f1-score on training data: 1.000 f1-score on test data: 0.665\n",
      "          - 65trees. f1-score on training data: 1.000 f1-score on test data: 0.665\n",
      "          - 70trees. f1-score on training data: 1.000 f1-score on test data: 0.665\n",
      "          - 75trees. f1-score on training data: 1.000 f1-score on test data: 0.649\n",
      "          - 80trees. f1-score on training data: 1.000 f1-score on test data: 0.649\n",
      "          - 85trees. f1-score on training data: 1.000 f1-score on test data: 0.682\n",
      "          - 90trees. f1-score on training data: 1.000 f1-score on test data: 0.698\n",
      "          - 95trees. f1-score on training data: 1.000 f1-score on test data: 0.698\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 1.000 f1-score on test data: 0.593\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 1.000 f1-score on test data: 0.529\n"
     ]
    }
   ],
   "source": [
    "models(df_3g,drop_lst,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de15359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 342 ; Resampled dataset shape Counter({'MCI-AD': 114, 'MCI-CN': 114, 'MCI-MCI': 114})\n",
      "\n",
      "5 principle components are needed to explain 90% of the data\n",
      "\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.353\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.169\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.353\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.486\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.162\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.486\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.529\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.357\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.529\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.600\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.617\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.600\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.614\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.603\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.612\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.611\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.612\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.608\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.609\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.606\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.612\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.373\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.528\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.584\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.580\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.630\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.585\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.543\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.551\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.529\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.551\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.531\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.546\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.524\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.532\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.585\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.603\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.592\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.583\n",
      "          - 25trees. average weighted f1-score of 10-cross validation:0.592\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.590\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.594\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.600\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.606\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.600\n",
      "          - 55trees. average weighted f1-score of 10-cross validation:0.606\n",
      "          - 60trees. average weighted f1-score of 10-cross validation:0.597\n",
      "          - 65trees. average weighted f1-score of 10-cross validation:0.596\n",
      "          - 70trees. average weighted f1-score of 10-cross validation:0.600\n",
      "          - 75trees. average weighted f1-score of 10-cross validation:0.600\n",
      "          - 80trees. average weighted f1-score of 10-cross validation:0.607\n",
      "          - 85trees. average weighted f1-score of 10-cross validation:0.591\n",
      "          - 90trees. average weighted f1-score of 10-cross validation:0.594\n",
      "          - 95trees. average weighted f1-score of 10-cross validation:0.596\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 10-cross validation:0.576\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 10-cross validation:0.582\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.548\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.164\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.548\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.573\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.164\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.573\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.589\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.597\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.589\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.605\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.611\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.605\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.597\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.607\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.597\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.598\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.598\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.598\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 10-cross validation:0.598\n",
      "          - saga_L1, average weighted f1-score of 10-cross validation:0.598\n",
      "          - newton-cg_L2, average weighted f1-score of 10-cross validation:0.598\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 10-cross validation:0.373\n",
      "          - tree depth: 2.000. average weighted f1-score of 10-cross validation:0.528\n",
      "          - tree depth: 3.000. average weighted f1-score of 10-cross validation:0.584\n",
      "          - tree depth: 4.000. average weighted f1-score of 10-cross validation:0.580\n",
      "          - tree depth: 5.000. average weighted f1-score of 10-cross validation:0.630\n",
      "          - tree depth: 6.000. average weighted f1-score of 10-cross validation:0.585\n",
      "          - tree depth: 7.000. average weighted f1-score of 10-cross validation:0.543\n",
      "          - tree depth: 8.000. average weighted f1-score of 10-cross validation:0.551\n",
      "          - tree depth: 9.000. average weighted f1-score of 10-cross validation:0.529\n",
      "          - tree depth: 10.000. average weighted f1-score of 10-cross validation:0.551\n",
      "          - tree depth: 11.000. average weighted f1-score of 10-cross validation:0.531\n",
      "          - tree depth: 12.000. average weighted f1-score of 10-cross validation:0.546\n",
      "          - tree depth: 13.000. average weighted f1-score of 10-cross validation:0.524\n",
      "          - tree depth: 14.000. average weighted f1-score of 10-cross validation:0.532\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 10-cross validation:0.585\n",
      "          - 10trees. average weighted f1-score of 10-cross validation:0.603\n",
      "          - 15trees. average weighted f1-score of 10-cross validation:0.592\n",
      "          - 20trees. average weighted f1-score of 10-cross validation:0.583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 25trees. average weighted f1-score of 10-cross validation:0.592\n",
      "          - 30trees. average weighted f1-score of 10-cross validation:0.590\n",
      "          - 35trees. average weighted f1-score of 10-cross validation:0.594\n",
      "          - 40trees. average weighted f1-score of 10-cross validation:0.600\n",
      "          - 45trees. average weighted f1-score of 10-cross validation:0.606\n",
      "          - 50trees. average weighted f1-score of 10-cross validation:0.600\n"
     ]
    }
   ],
   "source": [
    "cv_models(df_3g,drop_lst,target,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6502c",
   "metadata": {},
   "source": [
    "- original or scaled data: random forest 30trees. average weighted f1-score of 10-cross validation:0.627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa40ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = usampling_scale_data(df_3g,drop_lst,target)     \n",
    "X = res[0]\n",
    "y = res[3]\n",
    "clf = RandomForestClassifier(n_estimators = 30, random_state = 5862)\n",
    "title_label = '10-fold crossvalidation of random forest with (30 trees)'\n",
    "feature_importance(X,y,clf,k,title_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd218d",
   "metadata": {},
   "source": [
    "- 'CN-CN': 103, 'CN-MCI': 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05248ca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models(df_2gg,drop_lst,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad548232",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_models(df_2gg,drop_lst,target,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2cc015",
   "metadata": {},
   "source": [
    "- pca data, random forest 95trees. average weighted f1-score of 10-cross validation:0.623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a5ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = usampling_scale_data(df_2gg,drop_lst,target)     \n",
    "X = res[2]\n",
    "y = res[3]\n",
    "clf = RandomForestClassifier(n_estimators = 95, random_state = 5862)\n",
    "title_label = '10-fold crossvalidation of random forest with (95 trees)'\n",
    "feature_importance(X,y,clf,k,title_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2af5416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc3c2702",
   "metadata": {},
   "source": [
    "- 'CN-CN': 103, 'CN-MCI': 103, 'MCI-AD': 103, 'MCI-CN': 103, 'MCI-MCI': 103\n",
    "- all not so good. No selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "models(df_5g,drop_lst,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b16e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_models(df_5g,drop_lst,target,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af37cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f8461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "464d6d89",
   "metadata": {},
   "source": [
    "- 'AD-AD': 103, 'CN-CN': 103, 'CN-MCI': 103, 'MCI-AD': 103, 'MCI-CN': 103, 'MCI-MCI': 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c397f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "models(df_6g,drop_lst,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa378901",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_models(df_6g,drop_lst,target,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f0d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c8fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b05ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feature importance check\n",
    "def feature_importance(X,y,clf,k,title_label):\n",
    "    '''\n",
    "    check the feature importance of the selected classification (decisiontree or random forest) model.\n",
    "    X: input data\n",
    "    y: output data\n",
    "    clf: classification model, e.g.clf = RandomForestClassifier(n_estimators = 90, random_state = 5862) \n",
    "    Return\n",
    "    ------\n",
    "    dataframe of raw importance info\n",
    "    boxplot of importance\n",
    "    '''\n",
    "    output = cross_validate(clf, X, y, cv=k, scoring = 'f1_weighted', return_estimator =True)\n",
    "    d = {}  # dictionary to collect all importance dataframes \n",
    "    print(\"Features sorted by their score for each estimator \")\n",
    "    for idx,estimator in enumerate(output['estimator']):   \n",
    "        feature_importances = pd.DataFrame(estimator.feature_importances_,\n",
    "                                           index = X.columns,\n",
    "                                            columns=[\"importance_%s\"% (idx+1)])\n",
    "        d[idx] = feature_importances  \n",
    "    df = d[0]  # dataframe to concat all dataframes in d\n",
    "    for i in range(1,len(d)):\n",
    "        df = pd.concat([df,d[i]],axis=1)\n",
    "    df['avg_importance'] = df.mean(axis=1)\n",
    "    df = df.sort_values(by = ['avg_importance'], ascending = [False])\n",
    "    # insert avg_importance column as first column\n",
    "    df.insert(0, 'avg_importance', df.pop('avg_importance'))\n",
    "    # preparation for plotting\n",
    "    dff = df.T.reset_index().iloc[1:,1:] \n",
    "    # plot feature importance\n",
    "    bp = dff.boxplot(rot=30,figsize=(12,6),fontsize=12)\n",
    "    bp.set_ylabel('Feature Importance',fontsize=12)\n",
    "    bp.set_title('Feature importance of %s'%(title_label),fontsize=18)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
