{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "20f5d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import export_text\n",
    "import mglearn\n",
    "from dashboard_one import *\n",
    "from dash_model_two import *\n",
    "from feature_selection import *\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b74b94",
   "metadata": {},
   "source": [
    "### sleep, brain_volume_ratio_to_baseline_____VS_____diagnosischanges from every visit\n",
    "\n",
    "\n",
    "#### sleep_brain_dxch.csv\n",
    "#### drop column 'NPIKSEV', otherwise we get no samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "769bd041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Phase</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>NPIK6</th>\n",
       "      <th>...</th>\n",
       "      <th>ratio_PTAU_bl</th>\n",
       "      <th>Ventricles_reduction_per_year</th>\n",
       "      <th>Hippocampus_reduction_per_year</th>\n",
       "      <th>wholebrain_reduction_per_year</th>\n",
       "      <th>Entorhinal_reduction_per_year</th>\n",
       "      <th>Fusiform_reduction_per_year</th>\n",
       "      <th>ICV_reduction_per_year</th>\n",
       "      <th>ABETA_reduction_per_year</th>\n",
       "      <th>TAU_reduction_per_year</th>\n",
       "      <th>PTAU_reduction_per_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m06</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m36</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m60</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m72</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17155</th>\n",
       "      <td>7083</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>126_S_7083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17156</th>\n",
       "      <td>7085</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>941_S_7085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17157</th>\n",
       "      <td>7088</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>033_S_7088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17158</th>\n",
       "      <td>7092</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>021_S_7092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17159</th>\n",
       "      <td>7100</td>\n",
       "      <td>ADNI3</td>\n",
       "      <td>sc</td>\n",
       "      <td>033_S_7100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17160 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RID  Phase VISCODE        PTID  NPIK1  NPIK2  NPIK3  NPIK4  NPIK5  \\\n",
       "0         2  ADNI1     m06  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "1         2  ADNI1     m36  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "2         2  ADNI1     m60  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "3         2  ADNI1     m72  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "4         2  ADNI1     NaN  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "...     ...    ...     ...         ...    ...    ...    ...    ...    ...   \n",
       "17155  7083  ADNI3      sc  126_S_7083    NaN    NaN    NaN    NaN    NaN   \n",
       "17156  7085  ADNI3      sc  941_S_7085    NaN    NaN    NaN    NaN    NaN   \n",
       "17157  7088  ADNI3      sc  033_S_7088    NaN    NaN    NaN    NaN    NaN   \n",
       "17158  7092  ADNI3      sc  021_S_7092    NaN    NaN    NaN    NaN    NaN   \n",
       "17159  7100  ADNI3      sc  033_S_7100    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "       NPIK6  ...  ratio_PTAU_bl  Ventricles_reduction_per_year  \\\n",
       "0        NaN  ...            NaN                            NaN   \n",
       "1        NaN  ...            NaN                            NaN   \n",
       "2        NaN  ...            NaN                            NaN   \n",
       "3        NaN  ...            NaN                            NaN   \n",
       "4        NaN  ...            NaN                            NaN   \n",
       "...      ...  ...            ...                            ...   \n",
       "17155    NaN  ...            NaN                            NaN   \n",
       "17156    NaN  ...            NaN                            NaN   \n",
       "17157    NaN  ...            NaN                            NaN   \n",
       "17158    NaN  ...            NaN                            NaN   \n",
       "17159    NaN  ...            NaN                            NaN   \n",
       "\n",
       "       Hippocampus_reduction_per_year  wholebrain_reduction_per_year  \\\n",
       "0                                 NaN                            NaN   \n",
       "1                                 NaN                            NaN   \n",
       "2                                 NaN                            NaN   \n",
       "3                                 NaN                            NaN   \n",
       "4                                 NaN                            NaN   \n",
       "...                               ...                            ...   \n",
       "17155                             NaN                            NaN   \n",
       "17156                             NaN                            NaN   \n",
       "17157                             NaN                            NaN   \n",
       "17158                             NaN                            NaN   \n",
       "17159                             NaN                            NaN   \n",
       "\n",
       "       Entorhinal_reduction_per_year  Fusiform_reduction_per_year  \\\n",
       "0                                NaN                          NaN   \n",
       "1                                NaN                          NaN   \n",
       "2                                NaN                          NaN   \n",
       "3                                NaN                          NaN   \n",
       "4                                NaN                          NaN   \n",
       "...                              ...                          ...   \n",
       "17155                            NaN                          NaN   \n",
       "17156                            NaN                          NaN   \n",
       "17157                            NaN                          NaN   \n",
       "17158                            NaN                          NaN   \n",
       "17159                            NaN                          NaN   \n",
       "\n",
       "       ICV_reduction_per_year  ABETA_reduction_per_year  \\\n",
       "0                         NaN                       NaN   \n",
       "1                         NaN                       NaN   \n",
       "2                         NaN                       NaN   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "...                       ...                       ...   \n",
       "17155                     NaN                       NaN   \n",
       "17156                     NaN                       NaN   \n",
       "17157                     NaN                       NaN   \n",
       "17158                     NaN                       NaN   \n",
       "17159                     NaN                       NaN   \n",
       "\n",
       "      TAU_reduction_per_year  PTAU_reduction_per_year  \n",
       "0                        NaN                      NaN  \n",
       "1                        NaN                      NaN  \n",
       "2                        NaN                      NaN  \n",
       "3                        NaN                      NaN  \n",
       "4                        NaN                      NaN  \n",
       "...                      ...                      ...  \n",
       "17155                    NaN                      NaN  \n",
       "17156                    NaN                      NaN  \n",
       "17157                    NaN                      NaN  \n",
       "17158                    NaN                      NaN  \n",
       "17159                    NaN                      NaN  \n",
       "\n",
       "[17160 rows x 37 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_brain_dxch = pd.read_csv('sleep_brain_dxch.csv').iloc[:,1:].drop(['NPIKSEV'],axis=1)\n",
    "sleep_brain_dxch = sleep_brain_dxch[sleep_brain_dxch['DXCHANGE'].notna()].reset_index().drop(['index'],axis=1)   # keep the rows where DXCHANGE is not nan\n",
    "sleep_brain_dxch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1b7f3e",
   "metadata": {},
   "source": [
    "### sleep______VS______DXCHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b9b829bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase</th>\n",
       "      <th>RID</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>DXCHANGE</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>NPIK6</th>\n",
       "      <th>NPIK7</th>\n",
       "      <th>NPIK8</th>\n",
       "      <th>NPIK9A</th>\n",
       "      <th>NPIK9B</th>\n",
       "      <th>NPIK9C</th>\n",
       "      <th>NPIKTOT</th>\n",
       "      <th>insomnia</th>\n",
       "      <th>OSA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>8</td>\n",
       "      <td>v06</td>\n",
       "      <td>011_S_0008</td>\n",
       "      <td>CN-CN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>8</td>\n",
       "      <td>v41</td>\n",
       "      <td>011_S_0008</td>\n",
       "      <td>CN-MCI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>31</td>\n",
       "      <td>v06</td>\n",
       "      <td>023_S_0031</td>\n",
       "      <td>CN-CN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>31</td>\n",
       "      <td>v11</td>\n",
       "      <td>023_S_0031</td>\n",
       "      <td>CN-CN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADNI2</td>\n",
       "      <td>31</td>\n",
       "      <td>v21</td>\n",
       "      <td>023_S_0031</td>\n",
       "      <td>CN-CN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>ADNI3</td>\n",
       "      <td>6890</td>\n",
       "      <td>y1</td>\n",
       "      <td>021_S_6890</td>\n",
       "      <td>MCI-MCI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>ADNI3</td>\n",
       "      <td>6891</td>\n",
       "      <td>y1</td>\n",
       "      <td>123_S_6891</td>\n",
       "      <td>AD-AD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>ADNI3</td>\n",
       "      <td>6897</td>\n",
       "      <td>y1</td>\n",
       "      <td>036_S_6897</td>\n",
       "      <td>MCI-MCI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>ADNI3</td>\n",
       "      <td>6947</td>\n",
       "      <td>y1</td>\n",
       "      <td>035_S_6947</td>\n",
       "      <td>MCI-MCI</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>ADNI3</td>\n",
       "      <td>6962</td>\n",
       "      <td>y1</td>\n",
       "      <td>941_S_6962</td>\n",
       "      <td>AD-AD</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1131 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Phase   RID VISCODE        PTID DXCHANGE  NPIK1  NPIK2  NPIK3  NPIK4  \\\n",
       "0     ADNI2     8     v06  011_S_0008    CN-CN    1.0    0.0    0.0    0.0   \n",
       "1     ADNI2     8     v41  011_S_0008   CN-MCI    0.0    0.0    0.0    0.0   \n",
       "2     ADNI2    31     v06  023_S_0031    CN-CN    0.0    1.0    0.0    0.0   \n",
       "3     ADNI2    31     v11  023_S_0031    CN-CN    0.0    1.0    0.0    0.0   \n",
       "4     ADNI2    31     v21  023_S_0031    CN-CN    0.0    0.0    0.0    1.0   \n",
       "...     ...   ...     ...         ...      ...    ...    ...    ...    ...   \n",
       "1126  ADNI3  6890      y1  021_S_6890  MCI-MCI    0.0    1.0    0.0    0.0   \n",
       "1127  ADNI3  6891      y1  123_S_6891    AD-AD    1.0    1.0    0.0    1.0   \n",
       "1128  ADNI3  6897      y1  036_S_6897  MCI-MCI    0.0    0.0    0.0    1.0   \n",
       "1129  ADNI3  6947      y1  035_S_6947  MCI-MCI    0.0    1.0    0.0    0.0   \n",
       "1130  ADNI3  6962      y1  941_S_6962    AD-AD    0.0    1.0    0.0    0.0   \n",
       "\n",
       "      NPIK5  NPIK6  NPIK7  NPIK8  NPIK9A  NPIK9B  NPIK9C  NPIKTOT  insomnia  \\\n",
       "0       0.0    0.0    0.0    0.0     3.0     2.0     0.0      6.0       1.0   \n",
       "1       0.0    0.0    1.0    0.0     4.0     2.0     1.0      8.0       1.0   \n",
       "2       0.0    1.0    0.0    0.0     2.0     1.0     0.0      2.0       1.0   \n",
       "3       0.0    1.0    0.0    0.0     2.0     1.0     0.0      2.0       1.0   \n",
       "4       0.0    0.0    0.0    0.0     3.0     1.0     1.0      3.0       1.0   \n",
       "...     ...    ...    ...    ...     ...     ...     ...      ...       ...   \n",
       "1126    0.0    0.0    0.0    0.0     4.0     1.0     1.0      4.0       1.0   \n",
       "1127    0.0    0.0    1.0    0.0     4.0     2.0     3.0      8.0       1.0   \n",
       "1128    0.0    0.0    0.0    1.0     1.0     1.0     1.0      1.0       1.0   \n",
       "1129    0.0    0.0    1.0    0.0     1.0     1.0     1.0      1.0       1.0   \n",
       "1130    0.0    0.0    0.0    1.0     3.0     2.0     2.0      6.0       1.0   \n",
       "\n",
       "      OSA  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "4     0.0  \n",
       "...   ...  \n",
       "1126  1.0  \n",
       "1127  0.0  \n",
       "1128  0.0  \n",
       "1129  0.0  \n",
       "1130  0.0  \n",
       "\n",
       "[1131 rows x 19 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_lst = [ 'DXCHANGE','NPIK1', 'NPIK2', 'NPIK3', 'NPIK4', 'NPIK5', 'NPIK6', 'NPIK7', 'NPIK8',\n",
    "       'NPIK9A', 'NPIK9B', 'NPIK9C', 'NPIKTOT',  'insomnia','OSA']\n",
    "sleep_dxch = sleep_brain_dxch[['Phase', 'RID', 'VISCODE','PTID'] + col_lst].set_index(['Phase', 'RID', 'VISCODE','PTID']).dropna(how='any',axis=0).reset_index()\n",
    "sleep_dxch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c75224ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phase       0\n",
       "RID         0\n",
       "VISCODE     0\n",
       "PTID        0\n",
       "DXCHANGE    0\n",
       "NPIK1       0\n",
       "NPIK2       0\n",
       "NPIK3       0\n",
       "NPIK4       0\n",
       "NPIK5       0\n",
       "NPIK6       0\n",
       "NPIK7       0\n",
       "NPIK8       0\n",
       "NPIK9A      0\n",
       "NPIK9B      0\n",
       "NPIK9C      0\n",
       "NPIKTOT     0\n",
       "insomnia    0\n",
       "OSA         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sleep_dxch.isna())   # check nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "af84a781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase</th>\n",
       "      <th>RID</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>NPIK6</th>\n",
       "      <th>NPIK7</th>\n",
       "      <th>NPIK8</th>\n",
       "      <th>NPIK9A</th>\n",
       "      <th>NPIK9B</th>\n",
       "      <th>NPIK9C</th>\n",
       "      <th>NPIKTOT</th>\n",
       "      <th>insomnia</th>\n",
       "      <th>OSA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DXCHANGE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AD-AD</th>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AD-MCI</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-AD</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-CN</th>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN-MCI</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-AD</th>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-CN</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCI-MCI</th>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Phase  RID  VISCODE  PTID  NPIK1  NPIK2  NPIK3  NPIK4  NPIK5  NPIK6  \\\n",
       "DXCHANGE                                                                        \n",
       "AD-AD       159  159      159   159    159    159    159    159    159    159   \n",
       "AD-MCI        2    2        2     2      2      2      2      2      2      2   \n",
       "CN-AD         6    6        6     6      6      6      6      6      6      6   \n",
       "CN-CN       291  291      291   291    291    291    291    291    291    291   \n",
       "CN-MCI       38   38       38    38     38     38     38     38     38     38   \n",
       "MCI-AD       80   80       80    80     80     80     80     80     80     80   \n",
       "MCI-CN       39   39       39    39     39     39     39     39     39     39   \n",
       "MCI-MCI     516  516      516   516    516    516    516    516    516    516   \n",
       "\n",
       "          NPIK7  NPIK8  NPIK9A  NPIK9B  NPIK9C  NPIKTOT  insomnia  OSA  \n",
       "DXCHANGE                                                                \n",
       "AD-AD       159    159     159     159     159      159       159  159  \n",
       "AD-MCI        2      2       2       2       2        2         2    2  \n",
       "CN-AD         6      6       6       6       6        6         6    6  \n",
       "CN-CN       291    291     291     291     291      291       291  291  \n",
       "CN-MCI       38     38      38      38      38       38        38   38  \n",
       "MCI-AD       80     80      80      80      80       80        80   80  \n",
       "MCI-CN       39     39      39      39      39       39        39   39  \n",
       "MCI-MCI     516    516     516     516     516      516       516  516  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_dxch.groupby('DXCHANGE').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae317f9",
   "metadata": {},
   "source": [
    "- select only the MCI-AD, MCI-MCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "40b9cf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(596, 19)"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_dxch_2g = sleep_dxch.loc[sleep_dxch['DXCHANGE'].isin(['MCI-AD','MCI-MCI'])].reset_index().drop(['index'],axis=1)\n",
    "sleep_dxch_2g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522d6ff",
   "metadata": {},
   "source": [
    "- drop DXCHANGE labels 'AD-MCI','CN-AD','CN-CN' then undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "23461b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832, 19)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_dxch_6g = sleep_dxch[(sleep_dxch['DXCHANGE'].isin(['CN-MCI', 'AD-AD', 'MCI-MCI', 'MCI-AD', 'MCI-CN']))].reset_index().drop(['index'],axis=1)\n",
    "sleep_dxch_6g.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39783bf",
   "metadata": {},
   "source": [
    "### oversampling and undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdd62ca",
   "metadata": {},
   "source": [
    "- functions\n",
    "    - models(df,drop_lst,target) : under sampling, split, scale, pca, models\n",
    "    - cv_models(df,drop_lst,target,k): under sampling, NOT SPLIT, scale, pca, models with cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "80eed256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 160 ; Resampled dataset shape Counter({'MCI-AD': 80, 'MCI-MCI': 80})\n",
      "\n",
      "10 principle components are needed to explain 90% of the data\n",
      "\n",
      "Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.693, Test set f1-score: 0.425\n",
      "          - saga_L1, Training set f1-score:0.707, Test set f1-score: 0.476\n",
      "          - newton-cg_L2, Training set f1-score:0.693, Test set f1-score: 0.425\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.644, Test set f1-score: 0.519\n",
      "          - saga_L1, Training set f1-score:0.707, Test set f1-score: 0.476\n",
      "          - newton-cg_L2, Training set f1-score:0.644, Test set f1-score: 0.519\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.700, Test set f1-score: 0.589\n",
      "          - saga_L1, Training set f1-score:0.639, Test set f1-score: 0.484\n",
      "          - newton-cg_L2, Training set f1-score:0.700, Test set f1-score: 0.589\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.743, Test set f1-score: 0.625\n",
      "          - saga_L1, Training set f1-score:0.704, Test set f1-score: 0.526\n",
      "          - newton-cg_L2, Training set f1-score:0.743, Test set f1-score: 0.625\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "          - saga_L1, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "          - newton-cg_L2, Training set f1-score:0.742, Test set f1-score: 0.618\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "          - saga_L1, Training set f1-score:0.726, Test set f1-score: 0.618\n",
      "          - newton-cg_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "          - saga_L1, Training set f1-score:0.726, Test set f1-score: 0.618\n",
      "          - newton-cg_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.641 f1-score on test data: 0.583\n",
      "          - tree depth: 2.000. f1-score on training data: 0.641 f1-score on test data: 0.583\n",
      "          - tree depth: 3.000. f1-score on training data: 0.687 f1-score on test data: 0.646\n",
      "          - tree depth: 4.000. f1-score on training data: 0.758 f1-score on test data: 0.455\n",
      "          - tree depth: 5.000. f1-score on training data: 0.792 f1-score on test data: 0.463\n",
      "          - tree depth: 6.000. f1-score on training data: 0.826 f1-score on test data: 0.490\n",
      "          - tree depth: 7.000. f1-score on training data: 0.859 f1-score on test data: 0.611\n",
      "          - tree depth: 8.000. f1-score on training data: 0.930 f1-score on test data: 0.583\n",
      "          - tree depth: 9.000. f1-score on training data: 0.945 f1-score on test data: 0.549\n",
      "          - tree depth: 10.000. f1-score on training data: 0.953 f1-score on test data: 0.580\n",
      "          - tree depth: 11.000. f1-score on training data: 0.961 f1-score on test data: 0.613\n",
      "          - tree depth: 12.000. f1-score on training data: 0.977 f1-score on test data: 0.583\n",
      "          - tree depth: 13.000. f1-score on training data: 0.984 f1-score on test data: 0.547\n",
      "          - tree depth: 14.000. f1-score on training data: 0.984 f1-score on test data: 0.579\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.938 f1-score on test data: 0.678\n",
      "          - 10trees. f1-score on training data: 0.969 f1-score on test data: 0.554\n",
      "          - 15trees. f1-score on training data: 0.977 f1-score on test data: 0.589\n",
      "          - 20trees. f1-score on training data: 0.977 f1-score on test data: 0.554\n",
      "          - 25trees. f1-score on training data: 0.977 f1-score on test data: 0.589\n",
      "          - 30trees. f1-score on training data: 0.984 f1-score on test data: 0.647\n",
      "          - 35trees. f1-score on training data: 0.984 f1-score on test data: 0.563\n",
      "          - 40trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 45trees. f1-score on training data: 0.984 f1-score on test data: 0.647\n",
      "          - 50trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 55trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 60trees. f1-score on training data: 0.984 f1-score on test data: 0.647\n",
      "          - 65trees. f1-score on training data: 0.984 f1-score on test data: 0.647\n",
      "          - 70trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 75trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 80trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 85trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 90trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 95trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.984 f1-score on test data: 0.580\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.984 f1-score on test data: 0.613\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.984 f1-score on test data: 0.515\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.984 f1-score on test data: 0.583\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.707, Test set f1-score: 0.476\n",
      "          - saga_L1, Training set f1-score:0.707, Test set f1-score: 0.476\n",
      "          - newton-cg_L2, Training set f1-score:0.707, Test set f1-score: 0.476\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.690, Test set f1-score: 0.625\n",
      "          - saga_L1, Training set f1-score:0.707, Test set f1-score: 0.476\n",
      "          - newton-cg_L2, Training set f1-score:0.690, Test set f1-score: 0.625\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.743, Test set f1-score: 0.625\n",
      "          - saga_L1, Training set f1-score:0.683, Test set f1-score: 0.625\n",
      "          - newton-cg_L2, Training set f1-score:0.743, Test set f1-score: 0.625\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.750, Test set f1-score: 0.653\n",
      "          - saga_L1, Training set f1-score:0.726, Test set f1-score: 0.618\n",
      "          - newton-cg_L2, Training set f1-score:0.750, Test set f1-score: 0.653\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "          - saga_L1, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "          - newton-cg_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "          - saga_L1, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "          - newton-cg_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "          - saga_L1, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "          - newton-cg_L2, Training set f1-score:0.734, Test set f1-score: 0.618\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.641 f1-score on test data: 0.583\n",
      "          - tree depth: 2.000. f1-score on training data: 0.641 f1-score on test data: 0.583\n",
      "          - tree depth: 3.000. f1-score on training data: 0.687 f1-score on test data: 0.646\n",
      "          - tree depth: 4.000. f1-score on training data: 0.758 f1-score on test data: 0.455\n",
      "          - tree depth: 5.000. f1-score on training data: 0.792 f1-score on test data: 0.463\n",
      "          - tree depth: 6.000. f1-score on training data: 0.826 f1-score on test data: 0.490\n",
      "          - tree depth: 7.000. f1-score on training data: 0.859 f1-score on test data: 0.611\n",
      "          - tree depth: 8.000. f1-score on training data: 0.930 f1-score on test data: 0.583\n",
      "          - tree depth: 9.000. f1-score on training data: 0.945 f1-score on test data: 0.549\n",
      "          - tree depth: 10.000. f1-score on training data: 0.953 f1-score on test data: 0.580\n",
      "          - tree depth: 11.000. f1-score on training data: 0.961 f1-score on test data: 0.613\n",
      "          - tree depth: 12.000. f1-score on training data: 0.977 f1-score on test data: 0.583\n",
      "          - tree depth: 13.000. f1-score on training data: 0.984 f1-score on test data: 0.547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - tree depth: 14.000. f1-score on training data: 0.984 f1-score on test data: 0.579\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.938 f1-score on test data: 0.678\n",
      "          - 10trees. f1-score on training data: 0.969 f1-score on test data: 0.554\n",
      "          - 15trees. f1-score on training data: 0.977 f1-score on test data: 0.589\n",
      "          - 20trees. f1-score on training data: 0.977 f1-score on test data: 0.554\n",
      "          - 25trees. f1-score on training data: 0.977 f1-score on test data: 0.589\n",
      "          - 30trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 35trees. f1-score on training data: 0.984 f1-score on test data: 0.563\n",
      "          - 40trees. f1-score on training data: 0.984 f1-score on test data: 0.647\n",
      "          - 45trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 50trees. f1-score on training data: 0.984 f1-score on test data: 0.647\n",
      "          - 55trees. f1-score on training data: 0.984 f1-score on test data: 0.647\n",
      "          - 60trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 65trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 70trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 75trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 80trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 85trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 90trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "          - 95trees. f1-score on training data: 0.984 f1-score on test data: 0.618\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.984 f1-score on test data: 0.589\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.984 f1-score on test data: 0.563\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.984 f1-score on test data: 0.647\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.984 f1-score on test data: 0.613\n",
      "- Using 10 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.707, Test set f1-score: 0.476\n",
      "          - saga_L1, Training set f1-score:0.624, Test set f1-score: 0.815\n",
      "          - newton-cg_L2, Training set f1-score:0.707, Test set f1-score: 0.476\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.686, Test set f1-score: 0.599\n",
      "          - saga_L1, Training set f1-score:0.707, Test set f1-score: 0.476\n",
      "          - newton-cg_L2, Training set f1-score:0.686, Test set f1-score: 0.599\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.712, Test set f1-score: 0.647\n",
      "          - saga_L1, Training set f1-score:0.681, Test set f1-score: 0.599\n",
      "          - newton-cg_L2, Training set f1-score:0.712, Test set f1-score: 0.647\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.734, Test set f1-score: 0.678\n",
      "          - saga_L1, Training set f1-score:0.718, Test set f1-score: 0.647\n",
      "          - newton-cg_L2, Training set f1-score:0.734, Test set f1-score: 0.678\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.726, Test set f1-score: 0.709\n",
      "          - saga_L1, Training set f1-score:0.734, Test set f1-score: 0.678\n",
      "          - newton-cg_L2, Training set f1-score:0.726, Test set f1-score: 0.709\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.734, Test set f1-score: 0.709\n",
      "          - saga_L1, Training set f1-score:0.734, Test set f1-score: 0.709\n",
      "          - newton-cg_L2, Training set f1-score:0.734, Test set f1-score: 0.709\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.734, Test set f1-score: 0.709\n",
      "          - saga_L1, Training set f1-score:0.734, Test set f1-score: 0.709\n",
      "          - newton-cg_L2, Training set f1-score:0.734, Test set f1-score: 0.709\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.666 f1-score on test data: 0.547\n",
      "          - tree depth: 2.000. f1-score on training data: 0.703 f1-score on test data: 0.483\n",
      "          - tree depth: 3.000. f1-score on training data: 0.769 f1-score on test data: 0.554\n",
      "          - tree depth: 4.000. f1-score on training data: 0.863 f1-score on test data: 0.554\n",
      "          - tree depth: 5.000. f1-score on training data: 0.923 f1-score on test data: 0.484\n",
      "          - tree depth: 6.000. f1-score on training data: 0.945 f1-score on test data: 0.613\n",
      "          - tree depth: 7.000. f1-score on training data: 0.969 f1-score on test data: 0.579\n",
      "          - tree depth: 8.000. f1-score on training data: 0.977 f1-score on test data: 0.677\n",
      "          - tree depth: 9.000. f1-score on training data: 0.984 f1-score on test data: 0.579\n",
      "          - tree depth: 10.000. f1-score on training data: 0.984 f1-score on test data: 0.677\n",
      "          - tree depth: 11.000. f1-score on training data: 0.984 f1-score on test data: 0.582\n",
      "          - tree depth: 12.000. f1-score on training data: 0.984 f1-score on test data: 0.582\n",
      "          - tree depth: 13.000. f1-score on training data: 0.984 f1-score on test data: 0.582\n",
      "          - tree depth: 14.000. f1-score on training data: 0.984 f1-score on test data: 0.582\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.938 f1-score on test data: 0.618\n",
      "          - 10trees. f1-score on training data: 0.969 f1-score on test data: 0.678\n",
      "          - 15trees. f1-score on training data: 0.977 f1-score on test data: 0.678\n",
      "          - 20trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 25trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 30trees. f1-score on training data: 0.984 f1-score on test data: 0.741\n",
      "          - 35trees. f1-score on training data: 0.984 f1-score on test data: 0.709\n",
      "          - 40trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 45trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 50trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 55trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 60trees. f1-score on training data: 0.984 f1-score on test data: 0.709\n",
      "          - 65trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 70trees. f1-score on training data: 0.984 f1-score on test data: 0.709\n",
      "          - 75trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 80trees. f1-score on training data: 0.984 f1-score on test data: 0.709\n",
      "          - 85trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 90trees. f1-score on training data: 0.984 f1-score on test data: 0.678\n",
      "          - 95trees. f1-score on training data: 0.984 f1-score on test data: 0.647\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.984 f1-score on test data: 0.548\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.984 f1-score on test data: 0.676\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.984 f1-score on test data: 0.580\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.984 f1-score on test data: 0.515\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models(sleep_dxch_2g,drop_lst,'DXCHANGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c4fa8",
   "metadata": {},
   "source": [
    "normal data: random forest: 85trees. f1-score on training data: 0.969 f1-score on test data: 0.780\n",
    "normal data: random forest: 85trees. f1-score on training data: 0.969 f1-score on test data: 0.780\n",
    "\n",
    "pca:decision tree: tree depth: 6.000. f1-score on training data: 0.891 f1-score on test data: 0.810"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "0071686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 160 ; Resampled dataset shape Counter({'MCI-AD': 80, 'MCI-MCI': 80})\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-423-07f9faa0dac6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleep_dxch_2g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DXCHANGE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\Brain_health_challenge_tobereviewed\\notebook\\dash_model_two.py\u001b[0m in \u001b[0;36mcv_models\u001b[1;34m(df, drop_lst, target, k)\u001b[0m\n\u001b[0;32m    213\u001b[0m     '''\n\u001b[0;32m    214\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0musampling_scale_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Output dataframes sequence: X_,X_scaled,X_pca,y_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0mX_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Brain_health_challenge_tobereviewed\\notebook\\dash_model_two.py\u001b[0m in \u001b[0;36musampling_scale_data\u001b[1;34m(df, drop_lst, target)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m     \u001b[0mX_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;31m# pca components\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# keep all n principal components\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "cv_models(sleep_dxch_2g,drop_lst,'DXCHANGE',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "a1f8203f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 190 ; Resampled dataset shape Counter({'AD-AD': 38, 'CN-MCI': 38, 'MCI-AD': 38, 'MCI-CN': 38, 'MCI-MCI': 38})\n",
      "\n",
      "9 principle components are needed to explain 90% of the data\n",
      "\n",
      "Output dataframes sequence: X_train,X_test,X_train_scaled,X_test_scaled,X_train_pca,X_test_pca,y_train,y_test\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.359, Test set f1-score: 0.309\n",
      "          - saga_L1, Training set f1-score:0.339, Test set f1-score: 0.311\n",
      "          - newton-cg_L2, Training set f1-score:0.359, Test set f1-score: 0.309\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.393, Test set f1-score: 0.313\n",
      "          - saga_L1, Training set f1-score:0.348, Test set f1-score: 0.273\n",
      "          - newton-cg_L2, Training set f1-score:0.393, Test set f1-score: 0.313\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.399, Test set f1-score: 0.236\n",
      "          - saga_L1, Training set f1-score:0.382, Test set f1-score: 0.352\n",
      "          - newton-cg_L2, Training set f1-score:0.399, Test set f1-score: 0.236\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.452, Test set f1-score: 0.239\n",
      "          - saga_L1, Training set f1-score:0.432, Test set f1-score: 0.293\n",
      "          - newton-cg_L2, Training set f1-score:0.452, Test set f1-score: 0.239\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.442, Test set f1-score: 0.209\n",
      "          - saga_L1, Training set f1-score:0.454, Test set f1-score: 0.233\n",
      "          - newton-cg_L2, Training set f1-score:0.442, Test set f1-score: 0.209\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.436, Test set f1-score: 0.209\n",
      "          - saga_L1, Training set f1-score:0.436, Test set f1-score: 0.233\n",
      "          - newton-cg_L2, Training set f1-score:0.436, Test set f1-score: 0.209\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.436, Test set f1-score: 0.209\n",
      "          - saga_L1, Training set f1-score:0.436, Test set f1-score: 0.233\n",
      "          - newton-cg_L2, Training set f1-score:0.436, Test set f1-score: 0.209\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.429 f1-score on test data: 0.384\n",
      "          - tree depth: 2.000. f1-score on training data: 0.448 f1-score on test data: 0.346\n",
      "          - tree depth: 3.000. f1-score on training data: 0.410 f1-score on test data: 0.214\n",
      "          - tree depth: 4.000. f1-score on training data: 0.422 f1-score on test data: 0.267\n",
      "          - tree depth: 5.000. f1-score on training data: 0.480 f1-score on test data: 0.265\n",
      "          - tree depth: 6.000. f1-score on training data: 0.583 f1-score on test data: 0.300\n",
      "          - tree depth: 7.000. f1-score on training data: 0.661 f1-score on test data: 0.398\n",
      "          - tree depth: 8.000. f1-score on training data: 0.720 f1-score on test data: 0.389\n",
      "          - tree depth: 9.000. f1-score on training data: 0.785 f1-score on test data: 0.334\n",
      "          - tree depth: 10.000. f1-score on training data: 0.818 f1-score on test data: 0.360\n",
      "          - tree depth: 11.000. f1-score on training data: 0.830 f1-score on test data: 0.457\n",
      "          - tree depth: 12.000. f1-score on training data: 0.849 f1-score on test data: 0.392\n",
      "          - tree depth: 13.000. f1-score on training data: 0.875 f1-score on test data: 0.416\n",
      "          - tree depth: 14.000. f1-score on training data: 0.888 f1-score on test data: 0.389\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.830 f1-score on test data: 0.240\n",
      "          - 10trees. f1-score on training data: 0.875 f1-score on test data: 0.233\n",
      "          - 15trees. f1-score on training data: 0.888 f1-score on test data: 0.255\n",
      "          - 20trees. f1-score on training data: 0.888 f1-score on test data: 0.214\n",
      "          - 25trees. f1-score on training data: 0.888 f1-score on test data: 0.260\n",
      "          - 30trees. f1-score on training data: 0.888 f1-score on test data: 0.191\n",
      "          - 35trees. f1-score on training data: 0.888 f1-score on test data: 0.160\n",
      "          - 40trees. f1-score on training data: 0.888 f1-score on test data: 0.160\n",
      "          - 45trees. f1-score on training data: 0.888 f1-score on test data: 0.162\n",
      "          - 50trees. f1-score on training data: 0.888 f1-score on test data: 0.187\n",
      "          - 55trees. f1-score on training data: 0.888 f1-score on test data: 0.161\n",
      "          - 60trees. f1-score on training data: 0.888 f1-score on test data: 0.187\n",
      "          - 65trees. f1-score on training data: 0.888 f1-score on test data: 0.187\n",
      "          - 70trees. f1-score on training data: 0.888 f1-score on test data: 0.209\n",
      "          - 75trees. f1-score on training data: 0.888 f1-score on test data: 0.183\n",
      "          - 80trees. f1-score on training data: 0.888 f1-score on test data: 0.186\n",
      "          - 85trees. f1-score on training data: 0.888 f1-score on test data: 0.190\n",
      "          - 90trees. f1-score on training data: 0.888 f1-score on test data: 0.190\n",
      "          - 95trees. f1-score on training data: 0.888 f1-score on test data: 0.190\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.888 f1-score on test data: 0.295\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.888 f1-score on test data: 0.316\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.888 f1-score on test data: 0.212\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.888 f1-score on test data: 0.236\n",
      "- Using scaled dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.420, Test set f1-score: 0.174\n",
      "          - saga_L1, Training set f1-score:0.330, Test set f1-score: 0.348\n",
      "          - newton-cg_L2, Training set f1-score:0.420, Test set f1-score: 0.174\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.441, Test set f1-score: 0.316\n",
      "          - saga_L1, Training set f1-score:0.348, Test set f1-score: 0.273\n",
      "          - newton-cg_L2, Training set f1-score:0.441, Test set f1-score: 0.316\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.452, Test set f1-score: 0.261\n",
      "          - saga_L1, Training set f1-score:0.410, Test set f1-score: 0.370\n",
      "          - newton-cg_L2, Training set f1-score:0.452, Test set f1-score: 0.261\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.454, Test set f1-score: 0.261\n",
      "          - saga_L1, Training set f1-score:0.469, Test set f1-score: 0.263\n",
      "          - newton-cg_L2, Training set f1-score:0.454, Test set f1-score: 0.261\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.442, Test set f1-score: 0.233\n",
      "          - saga_L1, Training set f1-score:0.442, Test set f1-score: 0.233\n",
      "          - newton-cg_L2, Training set f1-score:0.442, Test set f1-score: 0.233\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.436, Test set f1-score: 0.209\n",
      "          - saga_L1, Training set f1-score:0.436, Test set f1-score: 0.209\n",
      "          - newton-cg_L2, Training set f1-score:0.436, Test set f1-score: 0.209\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.436, Test set f1-score: 0.209\n",
      "          - saga_L1, Training set f1-score:0.436, Test set f1-score: 0.209\n",
      "          - newton-cg_L2, Training set f1-score:0.436, Test set f1-score: 0.209\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.429 f1-score on test data: 0.384\n",
      "          - tree depth: 2.000. f1-score on training data: 0.448 f1-score on test data: 0.346\n",
      "          - tree depth: 3.000. f1-score on training data: 0.410 f1-score on test data: 0.214\n",
      "          - tree depth: 4.000. f1-score on training data: 0.422 f1-score on test data: 0.267\n",
      "          - tree depth: 5.000. f1-score on training data: 0.480 f1-score on test data: 0.265\n",
      "          - tree depth: 6.000. f1-score on training data: 0.583 f1-score on test data: 0.300\n",
      "          - tree depth: 7.000. f1-score on training data: 0.661 f1-score on test data: 0.398\n",
      "          - tree depth: 8.000. f1-score on training data: 0.720 f1-score on test data: 0.389\n",
      "          - tree depth: 9.000. f1-score on training data: 0.785 f1-score on test data: 0.334\n",
      "          - tree depth: 10.000. f1-score on training data: 0.818 f1-score on test data: 0.360\n",
      "          - tree depth: 11.000. f1-score on training data: 0.830 f1-score on test data: 0.457\n",
      "          - tree depth: 12.000. f1-score on training data: 0.849 f1-score on test data: 0.392\n",
      "          - tree depth: 13.000. f1-score on training data: 0.875 f1-score on test data: 0.416\n",
      "          - tree depth: 14.000. f1-score on training data: 0.888 f1-score on test data: 0.389\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.823 f1-score on test data: 0.238\n",
      "          - 10trees. f1-score on training data: 0.869 f1-score on test data: 0.259\n",
      "          - 15trees. f1-score on training data: 0.888 f1-score on test data: 0.282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          - 20trees. f1-score on training data: 0.888 f1-score on test data: 0.214\n",
      "          - 25trees. f1-score on training data: 0.888 f1-score on test data: 0.260\n",
      "          - 30trees. f1-score on training data: 0.888 f1-score on test data: 0.191\n",
      "          - 35trees. f1-score on training data: 0.888 f1-score on test data: 0.160\n",
      "          - 40trees. f1-score on training data: 0.888 f1-score on test data: 0.160\n",
      "          - 45trees. f1-score on training data: 0.888 f1-score on test data: 0.162\n",
      "          - 50trees. f1-score on training data: 0.888 f1-score on test data: 0.187\n",
      "          - 55trees. f1-score on training data: 0.888 f1-score on test data: 0.161\n",
      "          - 60trees. f1-score on training data: 0.888 f1-score on test data: 0.217\n",
      "          - 65trees. f1-score on training data: 0.888 f1-score on test data: 0.217\n",
      "          - 70trees. f1-score on training data: 0.888 f1-score on test data: 0.209\n",
      "          - 75trees. f1-score on training data: 0.888 f1-score on test data: 0.183\n",
      "          - 80trees. f1-score on training data: 0.888 f1-score on test data: 0.211\n",
      "          - 85trees. f1-score on training data: 0.888 f1-score on test data: 0.190\n",
      "          - 90trees. f1-score on training data: 0.888 f1-score on test data: 0.190\n",
      "          - 95trees. f1-score on training data: 0.888 f1-score on test data: 0.190\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.888 f1-score on test data: 0.212\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.888 f1-score on test data: 0.340\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.888 f1-score on test data: 0.164\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.888 f1-score on test data: 0.218\n",
      "- Using 9 pca-components:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, Training set f1-score:0.434, Test set f1-score: 0.245\n",
      "          - saga_L1, Training set f1-score:0.348, Test set f1-score: 0.273\n",
      "          - newton-cg_L2, Training set f1-score:0.434, Test set f1-score: 0.285\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, Training set f1-score:0.430, Test set f1-score: 0.236\n",
      "          - saga_L1, Training set f1-score:0.311, Test set f1-score: 0.417\n",
      "          - newton-cg_L2, Training set f1-score:0.430, Test set f1-score: 0.236\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, Training set f1-score:0.414, Test set f1-score: 0.223\n",
      "          - saga_L1, Training set f1-score:0.417, Test set f1-score: 0.306\n",
      "          - newton-cg_L2, Training set f1-score:0.414, Test set f1-score: 0.223\n",
      "       - C = 1\n",
      "          - lbfgs_L2, Training set f1-score:0.426, Test set f1-score: 0.240\n",
      "          - saga_L1, Training set f1-score:0.407, Test set f1-score: 0.240\n",
      "          - newton-cg_L2, Training set f1-score:0.426, Test set f1-score: 0.240\n",
      "       - C = 10\n",
      "          - lbfgs_L2, Training set f1-score:0.413, Test set f1-score: 0.239\n",
      "          - saga_L1, Training set f1-score:0.419, Test set f1-score: 0.240\n",
      "          - newton-cg_L2, Training set f1-score:0.413, Test set f1-score: 0.239\n",
      "       - C = 100\n",
      "          - lbfgs_L2, Training set f1-score:0.433, Test set f1-score: 0.239\n",
      "          - saga_L1, Training set f1-score:0.419, Test set f1-score: 0.239\n",
      "          - newton-cg_L2, Training set f1-score:0.433, Test set f1-score: 0.239\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, Training set f1-score:0.433, Test set f1-score: 0.239\n",
      "          - saga_L1, Training set f1-score:0.433, Test set f1-score: 0.239\n",
      "          - newton-cg_L2, Training set f1-score:0.433, Test set f1-score: 0.239\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. f1-score on training data: 0.365 f1-score on test data: 0.225\n",
      "          - tree depth: 2.000. f1-score on training data: 0.413 f1-score on test data: 0.177\n",
      "          - tree depth: 3.000. f1-score on training data: 0.456 f1-score on test data: 0.196\n",
      "          - tree depth: 4.000. f1-score on training data: 0.486 f1-score on test data: 0.235\n",
      "          - tree depth: 5.000. f1-score on training data: 0.547 f1-score on test data: 0.179\n",
      "          - tree depth: 6.000. f1-score on training data: 0.605 f1-score on test data: 0.303\n",
      "          - tree depth: 7.000. f1-score on training data: 0.663 f1-score on test data: 0.276\n",
      "          - tree depth: 8.000. f1-score on training data: 0.711 f1-score on test data: 0.279\n",
      "          - tree depth: 9.000. f1-score on training data: 0.792 f1-score on test data: 0.197\n",
      "          - tree depth: 10.000. f1-score on training data: 0.834 f1-score on test data: 0.200\n",
      "          - tree depth: 11.000. f1-score on training data: 0.868 f1-score on test data: 0.149\n",
      "          - tree depth: 12.000. f1-score on training data: 0.873 f1-score on test data: 0.152\n",
      "          - tree depth: 13.000. f1-score on training data: 0.888 f1-score on test data: 0.147\n",
      "          - tree depth: 14.000. f1-score on training data: 0.888 f1-score on test data: 0.145\n",
      "    - Random forest\n",
      "          - 5trees. f1-score on training data: 0.810 f1-score on test data: 0.171\n",
      "          - 10trees. f1-score on training data: 0.881 f1-score on test data: 0.225\n",
      "          - 15trees. f1-score on training data: 0.881 f1-score on test data: 0.253\n",
      "          - 20trees. f1-score on training data: 0.888 f1-score on test data: 0.219\n",
      "          - 25trees. f1-score on training data: 0.888 f1-score on test data: 0.177\n",
      "          - 30trees. f1-score on training data: 0.888 f1-score on test data: 0.211\n",
      "          - 35trees. f1-score on training data: 0.888 f1-score on test data: 0.160\n",
      "          - 40trees. f1-score on training data: 0.888 f1-score on test data: 0.187\n",
      "          - 45trees. f1-score on training data: 0.888 f1-score on test data: 0.154\n",
      "          - 50trees. f1-score on training data: 0.888 f1-score on test data: 0.182\n",
      "          - 55trees. f1-score on training data: 0.888 f1-score on test data: 0.188\n",
      "          - 60trees. f1-score on training data: 0.888 f1-score on test data: 0.153\n",
      "          - 65trees. f1-score on training data: 0.888 f1-score on test data: 0.153\n",
      "          - 70trees. f1-score on training data: 0.888 f1-score on test data: 0.207\n",
      "          - 75trees. f1-score on training data: 0.888 f1-score on test data: 0.186\n",
      "          - 80trees. f1-score on training data: 0.888 f1-score on test data: 0.186\n",
      "          - 85trees. f1-score on training data: 0.888 f1-score on test data: 0.186\n",
      "          - 90trees. f1-score on training data: 0.888 f1-score on test data: 0.183\n",
      "          - 95trees. f1-score on training data: 0.888 f1-score on test data: 0.210\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. f1-score on training data: 0.888 f1-score on test data: 0.310\n",
      "          - hidden layer size[20, 20]. f1-score on training data: 0.888 f1-score on test data: 0.311\n",
      "          - hidden layer size[100, 100]. f1-score on training data: 0.888 f1-score on test data: 0.209\n",
      "          - hidden layer size[50, 50, 50]. f1-score on training data: 0.888 f1-score on test data: 0.207\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models(sleep_dxch_6g,drop_lst,'DXCHANGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "66c83805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After undersampling data size is 190 ; Resampled dataset shape Counter({'AD-AD': 38, 'CN-MCI': 38, 'MCI-AD': 38, 'MCI-CN': 38, 'MCI-MCI': 38})\n",
      "\n",
      "10 principle components are needed to explain 90% of the data\n",
      "\n",
      "- Using original dataset:\n",
      "    - Logistic regression\n",
      "       - C = 0.001\n",
      "          - lbfgs_L2, average weighted f1-score of 5-cross validation:0.147\n",
      "          - saga_L1, average weighted f1-score of 5-cross validation:0.064\n",
      "          - newton-cg_L2, average weighted f1-score of 5-cross validation:0.147\n",
      "       - C = 0.01\n",
      "          - lbfgs_L2, average weighted f1-score of 5-cross validation:0.222\n",
      "          - saga_L1, average weighted f1-score of 5-cross validation:0.064\n",
      "          - newton-cg_L2, average weighted f1-score of 5-cross validation:0.222\n",
      "       - C = 0.1\n",
      "          - lbfgs_L2, average weighted f1-score of 5-cross validation:0.226\n",
      "          - saga_L1, average weighted f1-score of 5-cross validation:0.164\n",
      "          - newton-cg_L2, average weighted f1-score of 5-cross validation:0.226\n",
      "       - C = 1\n",
      "          - lbfgs_L2, average weighted f1-score of 5-cross validation:0.289\n",
      "          - saga_L1, average weighted f1-score of 5-cross validation:0.309\n",
      "          - newton-cg_L2, average weighted f1-score of 5-cross validation:0.289\n",
      "       - C = 10\n",
      "          - lbfgs_L2, average weighted f1-score of 5-cross validation:0.274\n",
      "          - saga_L1, average weighted f1-score of 5-cross validation:0.267\n",
      "          - newton-cg_L2, average weighted f1-score of 5-cross validation:0.274\n",
      "       - C = 100\n",
      "          - lbfgs_L2, average weighted f1-score of 5-cross validation:0.268\n",
      "          - saga_L1, average weighted f1-score of 5-cross validation:0.268\n",
      "          - newton-cg_L2, average weighted f1-score of 5-cross validation:0.268\n",
      "       - C = 1000\n",
      "          - lbfgs_L2, average weighted f1-score of 5-cross validation:0.268\n",
      "          - saga_L1, average weighted f1-score of 5-cross validation:0.263\n",
      "          - newton-cg_L2, average weighted f1-score of 5-cross validation:0.268\n",
      "    - Decision tree\n",
      "          - tree depth: 1.000. average weighted f1-score of 5-cross validation:0.155\n",
      "          - tree depth: 2.000. average weighted f1-score of 5-cross validation:0.213\n",
      "          - tree depth: 3.000. average weighted f1-score of 5-cross validation:0.240\n",
      "          - tree depth: 4.000. average weighted f1-score of 5-cross validation:0.230\n",
      "          - tree depth: 5.000. average weighted f1-score of 5-cross validation:0.283\n",
      "          - tree depth: 6.000. average weighted f1-score of 5-cross validation:0.230\n",
      "          - tree depth: 7.000. average weighted f1-score of 5-cross validation:0.293\n",
      "          - tree depth: 8.000. average weighted f1-score of 5-cross validation:0.272\n",
      "          - tree depth: 9.000. average weighted f1-score of 5-cross validation:0.266\n",
      "          - tree depth: 10.000. average weighted f1-score of 5-cross validation:0.268\n",
      "          - tree depth: 11.000. average weighted f1-score of 5-cross validation:0.243\n",
      "          - tree depth: 12.000. average weighted f1-score of 5-cross validation:0.252\n",
      "          - tree depth: 13.000. average weighted f1-score of 5-cross validation:0.247\n",
      "          - tree depth: 14.000. average weighted f1-score of 5-cross validation:0.261\n",
      "    - Random forest\n",
      "          - 5trees. average weighted f1-score of 5-cross validation:0.231\n",
      "          - 10trees. average weighted f1-score of 5-cross validation:0.213\n",
      "          - 15trees. average weighted f1-score of 5-cross validation:0.232\n",
      "          - 20trees. average weighted f1-score of 5-cross validation:0.256\n",
      "          - 25trees. average weighted f1-score of 5-cross validation:0.235\n",
      "          - 30trees. average weighted f1-score of 5-cross validation:0.228\n",
      "          - 35trees. average weighted f1-score of 5-cross validation:0.223\n",
      "          - 40trees. average weighted f1-score of 5-cross validation:0.212\n",
      "          - 45trees. average weighted f1-score of 5-cross validation:0.214\n",
      "          - 50trees. average weighted f1-score of 5-cross validation:0.215\n",
      "          - 55trees. average weighted f1-score of 5-cross validation:0.222\n",
      "          - 60trees. average weighted f1-score of 5-cross validation:0.210\n",
      "          - 65trees. average weighted f1-score of 5-cross validation:0.233\n",
      "          - 70trees. average weighted f1-score of 5-cross validation:0.231\n",
      "          - 75trees. average weighted f1-score of 5-cross validation:0.230\n",
      "          - 80trees. average weighted f1-score of 5-cross validation:0.237\n",
      "          - 85trees. average weighted f1-score of 5-cross validation:0.232\n",
      "          - 90trees. average weighted f1-score of 5-cross validation:0.227\n",
      "          - 95trees. average weighted f1-score of 5-cross validation:0.230\n",
      "    - MLP\n",
      "          - hidden layer size[50, 50]. average weighted f1-score of 5-cross validation:0.231\n",
      "          - hidden layer size[20, 20]. average weighted f1-score of 5-cross validation:0.233\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-419-d0b01c2e9c4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleep_dxch_6g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_lst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DXCHANGE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-418-c73a8188c81d>\u001b[0m in \u001b[0;36mcv_models\u001b[1;34m(df, drop_lst, target, k)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m460\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'          - hidden layer size{}. average weighted f1-score of {}-cross validation:{:.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m     cv_results = cross_validate(\n\u001b[0m\u001b[0;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;31m# independent, and that it is pickle-able.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    267\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         \"\"\"\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;31m# Run the LBFGS solver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"lbfgs\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m             self._fit_lbfgs(\n\u001b[0m\u001b[0;32m    442\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_lbfgs\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[0miprint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m         opt_res = scipy.optimize.minimize(\n\u001b[0m\u001b[0;32m    547\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loss_grad_lbfgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mpacked_coef_inter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    617\u001b[0m                                   **options)\n\u001b[0;32m    618\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[0;32m    620\u001b[0m                                 callback=callback, **options)\n\u001b[0;32m    621\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfun_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36marray_equal\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36marray_equal\u001b[1;34m(a1, a2, equal_nan)\u001b[0m\n\u001b[0;32m   2451\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2452\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mequal_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2453\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2454\u001b[0m     \u001b[1;31m# Handling NaN values if equal_nan is True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2455\u001b[0m     \u001b[0ma1nan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma2nan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_models(sleep_dxch_6g,drop_lst,'DXCHANGE',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e387e3",
   "metadata": {},
   "source": [
    "## best score: newton-cg_L2,C:1, Training set score:0.711, Test set score: 0.688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb7568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89137af9",
   "metadata": {},
   "source": [
    "## sleep vs final_diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "a670079b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>Phase</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>PTID</th>\n",
       "      <th>NPIK1</th>\n",
       "      <th>NPIK2</th>\n",
       "      <th>NPIK3</th>\n",
       "      <th>NPIK4</th>\n",
       "      <th>NPIK5</th>\n",
       "      <th>NPIK6</th>\n",
       "      <th>...</th>\n",
       "      <th>ratio_PTAU_bl</th>\n",
       "      <th>Ventricles_reduction_per_year</th>\n",
       "      <th>Hippocampus_reduction_per_year</th>\n",
       "      <th>wholebrain_reduction_per_year</th>\n",
       "      <th>Entorhinal_reduction_per_year</th>\n",
       "      <th>Fusiform_reduction_per_year</th>\n",
       "      <th>ICV_reduction_per_year</th>\n",
       "      <th>ABETA_reduction_per_year</th>\n",
       "      <th>TAU_reduction_per_year</th>\n",
       "      <th>PTAU_reduction_per_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>sc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m06</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m36</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m60</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m66</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24379</th>\n",
       "      <td>679</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>m186</td>\n",
       "      <td>041_S_0679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.073308</td>\n",
       "      <td>0.004114</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>-0.006792</td>\n",
       "      <td>-0.007425</td>\n",
       "      <td>0.004080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24380</th>\n",
       "      <td>4401</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>m120</td>\n",
       "      <td>014_S_4401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.055952</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>-0.013059</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24381</th>\n",
       "      <td>4644</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>m108</td>\n",
       "      <td>003_S_4644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.032266</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>-0.005400</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>-0.002509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24382</th>\n",
       "      <td>5100</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>m108</td>\n",
       "      <td>041_S_5100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.077513</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>-0.014872</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24383</th>\n",
       "      <td>5282</td>\n",
       "      <td>ADNI2</td>\n",
       "      <td>m84</td>\n",
       "      <td>082_S_5282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24384 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        RID  Phase VISCODE        PTID  NPIK1  NPIK2  NPIK3  NPIK4  NPIK5  \\\n",
       "0         2  ADNI1      sc         NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "1         2  ADNI1     m06  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "2         2  ADNI1     m36  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "3         2  ADNI1     m60  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "4         2  ADNI1     m66  011_S_0002    NaN    NaN    NaN    NaN    NaN   \n",
       "...     ...    ...     ...         ...    ...    ...    ...    ...    ...   \n",
       "24379   679  ADNI1    m186  041_S_0679    NaN    NaN    NaN    NaN    NaN   \n",
       "24380  4401  ADNI2    m120  014_S_4401    NaN    NaN    NaN    NaN    NaN   \n",
       "24381  4644  ADNI2    m108  003_S_4644    NaN    NaN    NaN    NaN    NaN   \n",
       "24382  5100  ADNI2    m108  041_S_5100    NaN    NaN    NaN    NaN    NaN   \n",
       "24383  5282  ADNI2     m84  082_S_5282    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "       NPIK6  ...  ratio_PTAU_bl  Ventricles_reduction_per_year  \\\n",
       "0        NaN  ...            NaN                            NaN   \n",
       "1        NaN  ...            NaN                            NaN   \n",
       "2        NaN  ...            NaN                            NaN   \n",
       "3        NaN  ...            NaN                            NaN   \n",
       "4        NaN  ...            NaN                            NaN   \n",
       "...      ...  ...            ...                            ...   \n",
       "24379    NaN  ...            NaN                      -0.073308   \n",
       "24380    NaN  ...            NaN                      -0.055952   \n",
       "24381    NaN  ...            NaN                      -0.032266   \n",
       "24382    NaN  ...            NaN                      -0.077513   \n",
       "24383    NaN  ...            NaN                            NaN   \n",
       "\n",
       "       Hippocampus_reduction_per_year  wholebrain_reduction_per_year  \\\n",
       "0                                 NaN                            NaN   \n",
       "1                                 NaN                            NaN   \n",
       "2                                 NaN                            NaN   \n",
       "3                                 NaN                            NaN   \n",
       "4                                 NaN                            NaN   \n",
       "...                               ...                            ...   \n",
       "24379                        0.004114                       0.003316   \n",
       "24380                        0.006005                       0.007223   \n",
       "24381                        0.004784                       0.001844   \n",
       "24382                        0.010280                       0.007247   \n",
       "24383                        0.004950                            NaN   \n",
       "\n",
       "       Entorhinal_reduction_per_year  Fusiform_reduction_per_year  \\\n",
       "0                                NaN                          NaN   \n",
       "1                                NaN                          NaN   \n",
       "2                                NaN                          NaN   \n",
       "3                                NaN                          NaN   \n",
       "4                                NaN                          NaN   \n",
       "...                              ...                          ...   \n",
       "24379                      -0.006792                    -0.007425   \n",
       "24380                      -0.013059                     0.004802   \n",
       "24381                      -0.005400                     0.011367   \n",
       "24382                      -0.014872                     0.006389   \n",
       "24383                            NaN                          NaN   \n",
       "\n",
       "       ICV_reduction_per_year  ABETA_reduction_per_year  \\\n",
       "0                         NaN                       NaN   \n",
       "1                         NaN                       NaN   \n",
       "2                         NaN                       NaN   \n",
       "3                         NaN                       NaN   \n",
       "4                         NaN                       NaN   \n",
       "...                       ...                       ...   \n",
       "24379                0.004080                       NaN   \n",
       "24380                0.002871                       NaN   \n",
       "24381               -0.002509                       NaN   \n",
       "24382                0.005107                       NaN   \n",
       "24383                0.003868                       NaN   \n",
       "\n",
       "       TAU_reduction_per_year PTAU_reduction_per_year  \n",
       "0                         NaN                     NaN  \n",
       "1                         NaN                     NaN  \n",
       "2                         NaN                     NaN  \n",
       "3                         NaN                     NaN  \n",
       "4                         NaN                     NaN  \n",
       "...                       ...                     ...  \n",
       "24379                     NaN                     NaN  \n",
       "24380                     NaN                     NaN  \n",
       "24381                     NaN                     NaN  \n",
       "24382                     NaN                     NaN  \n",
       "24383                     NaN                     NaN  \n",
       "\n",
       "[24384 rows x 38 columns]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sleep_brain_finaldxch = pd.read_csv('sleep_brain_finaldxch.csv',sep=',').iloc[:,1:]\n",
    "sleep_brain_finaldxch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54901a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3592c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d4466",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=54, ncols=3, figsize=(18,280))\n",
    "axes = axes.ravel()  # array to 1D\n",
    "for j in range(len(sleep_lst)):\n",
    "    for i in range(len(cat_lst)):\n",
    "        axes[i+j*9].scatter(sleep_brain_v[sleep_lst[j]], sleep_brain_v[cat_lst[i]])\n",
    "        axes[i+j*9].set(title=f'{sleep_lst[j]} vs {cat_lst[i]}', xlabel=None)   \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f00f82b",
   "metadata": {},
   "source": [
    "NPIKSEV, insomnia, OSA have data available to correlate to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40f6dc",
   "metadata": {},
   "source": [
    "NPIKSEV vs brain volume ratio to baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f470319",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_brain_v = sleep_brain_v.drop(['NPIK1', 'NPIK2', 'NPIK3', 'NPIK4',\n",
    "       'NPIK5', 'NPIK6', 'NPIK7', 'NPIK8', 'NPIK9A', 'NPIK9B', 'NPIK9C',\n",
    "       'NPIKTOT'],axis=1)\n",
    "sleep_brain_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f166fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_brain_red = sleep_brain_v.dropna(how='any',axis=0).reset_index().drop(['index'],axis=1)  # reduced\n",
    "sleep_brain_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_brain_red.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(sleep_brain_red[['NPIKSEV', 'insomnia', 'OSA',\n",
    "       'ratio_Ventricles_bl', 'ratio_Hippocampus_bl', 'ratio_WholeBrain_bl',\n",
    "       'ratio_Entorhinal_bl', 'ratio_Fusiform_bl', 'ratio_ICV_bl',\n",
    "       'ratio_ABETA_bl', 'ratio_TAU_bl', 'ratio_PTAU_bl']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_brain_red.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = train_test_split(sleep_brain_red,random_state=586,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffccbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7227c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9636cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['NPIKSEV', 'insomnia', 'OSA',\n",
    "       'ratio_ABETA_bl', 'ratio_TAU_bl', 'ratio_PTAU_bl']]\n",
    "X_test = test[['NPIKSEV', 'insomnia', 'OSA',\n",
    "       'ratio_ABETA_bl', 'ratio_TAU_bl', 'ratio_PTAU_bl']]\n",
    "y_lst = ['Ventricles_reduction_per_year', 'Hippocampus_reduction_per_year',\n",
    "       'wholebrain_reduction_per_year', 'Entorhinal_reduction_per_year',\n",
    "       'Fusiform_reduction_per_year', 'ICV_reduction_per_year',\n",
    "       'ABETA_reduction_per_year', 'TAU_reduction_per_year',\n",
    "       'PTAU_reduction_per_year','ratio_Ventricles_bl', 'ratio_Hippocampus_bl', 'ratio_WholeBrain_bl',\n",
    "       'ratio_Entorhinal_bl', 'ratio_Fusiform_bl', 'ratio_ICV_bl',\n",
    "       'ratio_ABETA_bl', 'ratio_TAU_bl', 'ratio_PTAU_bl']\n",
    "alpha_lst = [0.01,0.1,1,10]\n",
    "for i in range(len(y_lst)):\n",
    "    y_train = train[y_lst[i]]\n",
    "    y_test = test[y_lst[i]]\n",
    "    for j in range(len(alpha_lst)):\n",
    "        \n",
    "        ridge = Ridge(alpha = alpha_lst[j]).fit(X_train,y_train)\n",
    "        print('{}: target feature {}: alpha = {}; training set score: {:.3f}; test set score {:.3f}'.format('Ridge',y_lst[i],alpha_lst[j],lr.score(X_train,train_whole_brain),lr.score(X_test,test_whole_brain)))\n",
    "        lasso = Lasso(alpha = alpha_lst[j]).fit(X_train,y_train)\n",
    "        print('{}: target feature {}: alpha = {}; training set score: {:.3f}; test set score {:.3f}'.format('Lasso',y_lst[i],alpha_lst[j],lr.score(X_train,train_whole_brain),lr.score(X_test,test_whole_brain)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa934b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data to use PCA\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d796f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3) #keep the first two principal components of the data\n",
    "pca.fit(X_train_scaled) # fit PCA model to sleep-diagnosis_change data\n",
    "X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "plt.bar(range(1,len(pca.explained_variance_ratio_ )+1),pca.explained_variance_ratio_ )\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Components')\n",
    "plt.plot(range(1,len(pca.explained_variance_ )+1),\n",
    "             np.cumsum(pca.explained_variance_ratio_),\n",
    "             c='red',\n",
    "             label=\"Cumulative Explained Variance ratio\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# The component 1 can explain about 20% of the variance, conponent 2 can explain about 11.7%,... \n",
    "# It needs almost 10 principal components to explain at least 90% of the variance. \n",
    "pca = PCA(n_components=3) \n",
    "pca.fit(X_train_scaled) # fit PCA model to sleep-diagnosis_change data\n",
    "X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "print(\"Origianl shape: {}\".format(str(X_train_scaled.shape)))\n",
    "print(\"Reduced shape: {}\".format(str(X_pca.shape)))\n",
    "# plot\n",
    "plt.figure(figsize=(8,8))\n",
    "mglearn.discrete_scatter(X_pca[:,0],X_pca[:,1],train['wholebrain_reduction_per_year'])\n",
    "plt.legend(train['wholebrain_reduction_per_year'].unique(),loc='best')\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel('Seconde principal component')\n",
    "\n",
    "print('PCA components: \\n{}'.format(pca.components_))    # PCA components\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.matshow(pca.components_,cmap='viridis')\n",
    "plt.yticks([0,1,2,3],['First component','Seconde component','Third component','Fourth component'])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(X_train.columns)),X_train.columns,rotation=60,ha='left')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Principal components')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fc12b",
   "metadata": {},
   "source": [
    "## try to use all variables to predict diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c461ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file = pd.read_csv('main_file.csv',sep=',')\n",
    "main_2 = main_file[com_col + ['DX','DXCHANGE']]\n",
    "sleep_bio_brain_dx = sleep_brain_v.merge(main_2,how='left',on=com_col).dropna(axis=1, how='all')\n",
    "sleep_bio_brain_dx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1209347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sleep_bio_brain_dx = sleep_bio_brain_dx.dropna(how='any',axis=0)\n",
    "train,test = train_test_split(sleep_bio_brain_dx,random_state=586,test_size=0.25)\n",
    "X_train = train.drop(['Phase', 'RID', 'VISCODE','PTID','DX','DXCHANGE'],axis=1)\n",
    "X_test = test.drop(['Phase', 'RID', 'VISCODE','PTID','DX','DXCHANGE'],axis=1)\n",
    "y_lst = ['DX','DXCHANGE']\n",
    "for j in range(len(y_lst)):\n",
    "    y_train = train[y_lst[j]]\n",
    "    y_test = test[y_lst[j]]\n",
    "    print('target feature: {}'.format(y_lst[j]))\n",
    "    for i in range(1,14):\n",
    "        tree = DecisionTreeClassifier(random_state=5850,max_depth=i,criterion='gini')\n",
    "        tree.fit(X_train,y_train)\n",
    "        print('    Decision tree with unscaled data. tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(tree.predict(X_train),y_train,average='weighted'),f1_score(tree.predict(X_test),y_test,average='weighted')))\n",
    "        forest = RandomForestClassifier(n_estimators = 200, random_state = 560)\n",
    "        forest.fit(X_train,y_train)\n",
    "        print('    Random forest with unscaled data. n_estimators: 200. tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(forest.predict(X_train),y_train,average='weighted'),f1_score(forest.predict(X_test),y_test,average='weighted')))\n",
    "    # MLP with unscaled data\n",
    "    mlp = MLPClassifier(solver='lbfgs',random_state=785,hidden_layer_sizes = [100,100],max_iter=40000)\n",
    "    mlp.fit(X_train,y_train)\n",
    "    print('    -MLP with uscaled data. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "      .format(f1_score(mlp.predict(X_train),y_train,average='weighted'),f1_score(mlp.predict(X_test),y_test,average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe89a0",
   "metadata": {},
   "source": [
    "For the target feature DXCHANGE tree depth of 3 works best. \n",
    "\n",
    "with f1-score on training data: 0.786 f1-score on test data: 0.606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec8238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled data \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "for j in range(len(y_lst)):\n",
    "    y_train = train[y_lst[j]]\n",
    "    y_test = test[y_lst[j]]\n",
    "    print('target feature: {}'.format(y_lst[j]))\n",
    "    for i in range(2,14):\n",
    "        tree = DecisionTreeClassifier(random_state=5850,max_depth=i,criterion='gini')\n",
    "        tree.fit(X_train_scaled,y_train)\n",
    "        print('    Decision tree with scaled data. tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(tree.predict(X_train_scaled),y_train,average='weighted'),f1_score(tree.predict(X_test_scaled),y_test,average='weighted')))\n",
    "        forest = RandomForestClassifier(n_estimators = 200, random_state = 560)\n",
    "        forest.fit(X_train_scaled,y_train)\n",
    "        print('    Random forest with scaled data. n_estimators: 200. tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(forest.predict(X_train_scaled),y_train,average='weighted'),f1_score(forest.predict(X_test_scaled),y_test,average='weighted')))\n",
    "    # MLP with scaled data\n",
    "    mlp = MLPClassifier(solver='lbfgs',random_state=785,hidden_layer_sizes = [200,200],max_iter=40000)\n",
    "    mlp.fit(X_train_scaled,y_train)\n",
    "    print('    -MLP with uscaled data. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "      .format(f1_score(mlp.predict(X_train_scaled),y_train,average='weighted'),f1_score(mlp.predict(X_test_scaled),y_test,average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d6f377",
   "metadata": {},
   "source": [
    " DXCHANGE: Decision tree with scaled data. tree depth: 3.000. f1-score on training data: 0.786 f1-score on test data: 0.606\n",
    " \n",
    " DX: Decision tree with scaled data. tree depth: 2.000. f1-score on training data: 0.778 f1-score on test data: 0.487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f605dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=X_train_scaled.shape[1]) #keep the first two principal components of the data\n",
    "pca.fit(X_train_scaled) # fit PCA model to sleep-diagnosis_change data\n",
    "X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "plt.bar(range(1,len(pca.explained_variance_ratio_ )+1),pca.explained_variance_ratio_ )\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Components')\n",
    "plt.plot(range(1,len(pca.explained_variance_ )+1),\n",
    "             np.cumsum(pca.explained_variance_ratio_),\n",
    "             c='red',\n",
    "             label=\"Cumulative Explained Variance ratio\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8,8))\n",
    "mglearn.discrete_scatter(X_pca[:,0],X_pca[:,1],train['DXCHANGE'])\n",
    "plt.legend(train['DXCHANGE'].unique(),loc='best')\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel('Seconde principal component')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,18))\n",
    "plt.matshow(pca.components_,cmap='viridis')\n",
    "plt.yticks([0,1,2,3,4,5,6],['First component','Seconde component','Third component','Fourth component','Fifth component','Sixth component','seventh component'])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(X_train.columns)),X_train.columns,rotation=60,ha='left')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Principal components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree with PCA data \n",
    "\n",
    "for k in range(len(y_lst)):\n",
    "    y_train = train[y_lst[k]]\n",
    "    y_test = test[y_lst[k]]\n",
    "    print('- target feature: {}'.format(y_lst[k]))\n",
    "    max_f1_test = 0\n",
    "    max_n_component = 0\n",
    "    tree_depth = 0\n",
    "    _f1_train = 0\n",
    "    for j in range(2,X_train_scaled.shape[1]):\n",
    "        pca = PCA(n_components=j) # keep the first j principal components of the data\n",
    "        pca.fit(X_train_scaled) # fit PCA model to sleep-diagnosis_change data\n",
    "        X_pca = pca.transform(X_train_scaled)  #transform data onto the first two principal components\n",
    "        X_test_pca = pca.transform(X_test_scaled)\n",
    "        #print('   - Decision tree with PCA {} components:'.format(j))\n",
    "        \n",
    "        for i in range(1,14):\n",
    "            tree = DecisionTreeClassifier(random_state=580,max_depth=i,criterion='gini')\n",
    "            tree.fit(X_pca,y_train)\n",
    "            f1_score_test = f1_score(tree.predict(X_test_pca),y_test,average='weighted')\n",
    "            f1_score_train = f1_score(tree.predict(X_pca),y_train,average='weighted')\n",
    "            #print('        - tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'.format(i,f1_score_train,f1_score_test))\n",
    "            forest = RandomForestClassifier(n_estimators = 200, random_state = 560)\n",
    "        forest.fit(X_pca,y_train)\n",
    "        print('    Random forest with pca data. n_estimators: 200. tree depth: {:.3f}. f1-score on training data: {:.3f} f1-score on test data: {:.3f}'\n",
    "              .format(i,f1_score(forest.predict(X_pca),y_train,average='weighted'),f1_score(forest.predict(X_test_pca),y_test,average='weighted')))\n",
    "        if f1_score_test >= max_f1_test:\n",
    "                max_f1_test = f1_score_test\n",
    "                _f1_train = f1_score_train\n",
    "                max_n_component = j\n",
    "                tree_depth = i\n",
    "    print('The decision tree model predicting target feature {} with {:.3f} components, tree-depth of {} has the best f1-score on test set {:.3f}, on train set{:.3f}.'\n",
    "          .format(y_lst[k],max_n_component,tree_depth, max_f1_test,_f1_train))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc3e2b",
   "metadata": {},
   "source": [
    "To predict feature DX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['DX']\n",
    "y_test = test['DX']\n",
    "clf = tree.DecisionTreeClassifier(random_state=580,max_depth=5,criterion='gini')\n",
    "clf = clf.fit(X_pca,y_train)\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "tree.plot_tree(clf,fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c199a",
   "metadata": {},
   "source": [
    "To predict feature DXCHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8d603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train,test = train_test_split(sleep_bio_brain_dx,random_state=586,test_size=0.25)\n",
    "X_train = train.drop(['Phase', 'RID', 'VISCODE','PTID','DX','DXCHANGE'],axis=1)\n",
    "X_test = test.drop(['Phase', 'RID', 'VISCODE','PTID','DX','DXCHANGE'],axis=1)\n",
    "feature_list = ['NPIKSEV', 'insomnia', 'OSA', 'ratio_Ventricles_bl',\n",
    "       'ratio_Hippocampus_bl', 'ratio_WholeBrain_bl', 'ratio_Entorhinal_bl',\n",
    "       'ratio_Fusiform_bl', 'ratio_ICV_bl', 'ratio_ABETA_bl', 'ratio_TAU_bl',\n",
    "       'ratio_PTAU_bl', 'Ventricles_reduction_per_year',\n",
    "       'Hippocampus_reduction_per_year', 'wholebrain_reduction_per_year',\n",
    "       'Entorhinal_reduction_per_year', 'Fusiform_reduction_per_year',\n",
    "       'ICV_reduction_per_year', 'ABETA_reduction_per_year',\n",
    "       'TAU_reduction_per_year', 'PTAU_reduction_per_year']\n",
    "y_train = train['DXCHANGE']\n",
    "y_test = test['DXCHANGE']\n",
    "clf = tree.DecisionTreeClassifier(random_state=5850,max_depth=3,criterion='gini')\n",
    "clf.fit(X_train,y_train)\n",
    "r = export_text(clf, feature_names=feature_list)\n",
    "print(r)\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "tree.plot_tree(clf,fontsize=15,feature_names=feature_list)\n",
    "\n",
    "print('    Decision tree with unscaled data. tree depth: 2. f1-score on training data: {0.786} f1-score on test data: {0.606}')        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
